[{"title":"docker","url":"/2026/02/28/cloud-docker/","content":"原理\n虚拟机：通过 Hypervisor 在硬件上模拟出完整的计算机，每个 VM 都运行自己的 Guest OS（包括内核），彼此完全独立。\nDocker 容器：没有自己的内核！所有容器都直接运行在宿主机的 Linux 内核之上，只是被“隔离”成看起来像独立系统的进程集合。\n\n\n✅ 简单说：容器 &#x3D; 被隔离 + 受限的普通 Linux 进程。\n\nNamespaces（命名空间）Namespaces 是 Linux 内核提供的一种机制，用于隔离不同进程组看到的系统资源视图。Docker 利用多种 Namespace 类型，让每个容器“以为”自己独占一台机器。\n\n\n\nNamespace 类型\n隔离内容\n容器中的表现\n\n\n\nPID（进程 ID）\n进程编号空间\n容器内 PID&#x3D;1 的进程是它的主进程，看不到宿主机或其他容器的进程\n\n\nNET（网络）\n网络设备、IP、端口、路由表等\n容器拥有独立的网络栈（如 eth0、lo），可绑定 80 端口而不冲突\n\n\nMNT（挂载）\n文件系统挂载点\n容器有自己的根文件系统（/），由镜像层叠加而成\n\n\nUTS\n主机名和域名\n容器可设置自己的 hostname（如 my-container）\n\n\nIPC\n进程间通信（信号量、消息队列等）\n容器间无法直接通过 IPC 通信\n\n\nUSER\n用户和用户组 ID\n容器内 root（UID&#x3D;0）可映射为宿主机的非特权用户（提升安全性）\n\n\nCgroups（Control Groups）Cgroups 是 Linux 内核的另一项功能，用于限制、记录和隔离进程组使用的物理资源（CPU、内存、磁盘 I&#x2F;O、网络带宽等）。\nDocker 利用 Cgroups 实现：\n\n限制容器最多使用 512MB 内存\n限制 CPU 使用不超过 0.5 个核心\n限制磁盘写入速率为 10MB&#x2F;s\n统计容器实际资源消耗（用于监控或计费）\n\n\n✅ 效果：防止某个容器“吃光”宿主机资源，影响其他容器或系统稳定性，实现多租户安全共存。\n\n文件系统：UnionFS + OverlayFS虽然容器共享内核，但每个容器需要独立的文件系统视图（比如 &#x2F;bin, &#x2F;etc, &#x2F;usr）。Docker 通过以下方式实现：\n\n镜像分层：镜像是只读层的叠加（如基础 OS 层 + Python 层 + 应用层）。\n写时复制（Copy-on-Write）：容器启动时，在镜像层之上加一个可写层。只有当容器修改文件时，才将该文件从只读层复制到可写层进行修改。\nUnion Mount：通过 OverlayFS 等联合文件系统，将多层“合并”成一个统一的 &#x2F; 目录呈现给容器。\n\n✅ 效果：\n\n多个容器可共享同一镜像的只读层，节省磁盘空间。\n容器的修改互不影响，重启后可丢弃（除非使用 Volume 持久化）。\n\n镜像本质分层存储\n\nDocker 镜像由多个只读层（Layers） 叠加而成。\n每一层代表对文件系统的一次增量修改（如安装软件、复制文件、设置环境变量）。\n层与层之间通过 Union File System（如 OverlayFS、AUFS）合并成一个统一的文件系统视图。\n\n✅ 优势：\n\n高效复用：多个镜像可共享相同的基础层（如 alpine:3.18）。\n快速构建：修改 Dockerfile 后，仅重建变更层及之后的层（利用构建缓存）。\n节省存储：相同层在磁盘上只存一份。\n📌 示例：镜像 A 和 B 都基于 ubuntu:22.04，则它们共享该基础层，无需重复下载或存储。\n\n\n\n内容寻址\n\n每一层通过 SHA256 哈希值唯一标识（如 sha256:a1b2c3…）。\n镜像 ID 即其顶层 Layer 的哈希值。\n确保镜像内容不可篡改，提升安全性。\n\n组成要素\n\n\n组件\n说明\n\n\n\n基础操作系统层\n如 alpine、ubuntu、centos，提供基本命令和库（但无内核）\n\n\n运行时环境\n如 Python 3.11、Node.js 18、OpenJDK 17\n\n\n应用代码\n用户编写的程序（如 app.py、main.go）\n\n\n依赖库\n第三方包（如 requests、lodash）\n\n\n配置文件\n如 nginx.conf、.env、application.yml\n\n\n元数据（Metadata）\n由 Dockerfile 指令定义： - 启动命令（CMD &#x2F; ENTRYPOINT） - 暴露端口（EXPOSE） - 环境变量（ENV） - 工作目录（WORKDIR）\n\n\nDockerfile每条指令独立构建一层（除 ENV、LABEL、EXPOSE 等元数据指令外）。\nFROM —— 指定基础镜像（必需）\n必须是 Dockerfile 的第一条指令（注释除外）。\n支持多阶段构建（Multi-stage Build）：\n使用具体版本标签（如 python:3.11-slim），避免 latest。\n优先选择精简镜像（如 -alpine、-slim、distroless）。\n\nFROM &lt;image&gt; [AS &lt;name&gt;]\nRUN —— 执行命令并创建新层\n在构建时执行命令（如安装软件、编译代码）。\n\nRUN apt-get update &amp;&amp; apt-get install -y curl\nCOPY 和 ADD —— 复制文件到镜像\nCOPY：仅复制本地文件&#x2F;目录到镜像\nADD：支持 URL 下载、自动解压 tar 包\n\nCOPY --from=builder /app .\nWORKDIR —— 设置工作目录\n类似于 cd，后续的 RUN、CMD、COPY 等指令都在此目录下执行。\n如果目录不存在，会自动创建。\n可多次使用，路径可相对或绝对。\n\nWORKDIR  /app\nENV —— 设置环境变量\n构建时和运行时都可用。\n常用于配置应用参数、版本号等。\n\nENV KEY=123\nEXPOSE —— 声明容器监听端口\n仅起文档作用，不实际开放端口。\n实际端口映射需在 docker run -p host_port:container_port 中指定。\n\nEXPOSE 8080\nCMD —— 容器启动命令\n提供默认参数，可被 docker run 后的命令完全替换\n\nCMD [&quot;python&quot;, &quot;app.py&quot;]\n\n网关\nDocker 容器默认运行在隔离的网络命名空间中，拥有自己的虚拟网络接口（如 eth0）、IP 地址和路由表。\nDocker 会自动创建虚拟网络设备，并设置默认网关作为流量出口，让容器能：访问外网（如下载依赖、调用 API）被宿主机或其他容器访问\n\n网络模式bridge这是最常用的网络模式。运行 docker run 而不指定网络时，容器会连接到名为 bridge 的默认网桥。\n\nDocker 在宿主机上创建一个虚拟网桥：docker0\n每个容器通过 veth pair（虚拟以太网设备对）连接到 docker0\n容器内分配一个私有 IP（如 172.17.0.2&#x2F;16）\n默认网关 &#x3D; docker0 的 IP 地址\n使用 docker network create 创建的自定义网桥更灵活、支持 DNS 解析\n\n\n\nhost\n容器共享宿主机网络命名空间\n无独立 IP，无虚拟网卡，无 Docker 网关\n直接使用宿主机的路由表和网关\n\nnone\n容器无网络接口（只有 lo 回环）\n无 IP，无网关，完全隔离\n\n原理Docker 网关本身不直接转发流量到外网，而是依赖 Linux 内核的 NAT（网络地址转换） 机制。\n\n容器发包 → 目标 IP 为外网（如 8.8.8.8）\n路由匹配 default via 172.17.0.1\n包到达 docker0 网桥\niptables 的 MASQUERADE 规则将源 IP（容器 IP）替换为宿主机 IP\n包通过宿主机物理网卡发出\n\n","categories":["cloud"]},{"title":"etcd","url":"/2026/02/28/cloud-etcd/","content":"raftRaft 是一种用于管理复制日志（replicated log）的一致性算法，其设计目标是易于理解，同时保证与 Paxos 等经典一致性算法相当的正确性和性能。\n基本概念节点角色（Node Roles）\n每个节点在任意时刻只能处于以下三种状态之一：\n\nFollower（跟随者）：被动接收来自 Leader 或 Candidate 的请求。\nCandidate（候选人）：参与选举，试图成为 Leader。\nLeader（领导者）：处理所有客户端请求，并负责将日志复制到其他节点。\n正常情况下，集群中只有一个 Leader，其余都是 Follower。\n\n\n\n任期（Term）\n\nRaft 将时间划分为一个个任期（Term），每个 Term 以一次选举开始。\n每个 Term 最多有一个 Leader。\nTerm 是一个单调递增的整数，用于检测过期信息（如旧 Leader 的请求）。\n\n日志（Log）\n\n每个节点维护一个有序的日志条目序列。\n每个日志条目包含：命令（Command）、所属 Term日志索引（Index）\n一致性要求：如果两个节点在某个 Index 上的日志条目具有相同的 Term，则它们在此 Index 之前的所有日志都相同。\n\n\n\nLeader 选举触发条件\nFollower 在 选举&#x2F;任期 超时内未收到 Leader 的心跳（AppendEntries RPC），则变为 Candidate。\n选举流程\n\nCandidate 增加当前 Term，为自己投票。\n并行向其他节点发送 RequestVote RPC。\n若收到多数派（majority） 的投票，则成为 Leader。\n若收到另一个更高 Term 的心跳，则退为 Follower。\n若选举超时仍未选出 Leader，则开始新一轮选举（Term+1）。\n选举超时通常设置为 150–300ms 的随机值，避免多个 Follower 同时发起选举导致选票分裂。\n\n\n\n日志复制\n客户端向 Leader 发送命令。\nLeader 将命令作为新日志条目追加到本地日志。\nLeader 并行向所有 Follower 发送 AppendEntries RPC（包含新日志）。\nFollower 收到后：\n检查日志一致性（前一个日志的 Term 和 Index 是否匹配）。\n若不一致，拒绝并返回冲突信息；Leader 会逐步回退并重试。\n若一致，则追加日志并返回成功。\n\n\n一旦 Leader 收到多数派的成功响应，就认为该日志条目已提交（committed）。\nLeader 在后续的 AppendEntries 中通知 Follower 提交该日志。\n各节点将已提交的日志应用到状态机（state machine），并向客户端返回结果。\n\nFollower 长期离线\n如果某个 Follower 长期离线，其日志会严重落后，此时直接逐条同步效率低。\n实践中（如 etcd），会使用 快照（Snapshot） 机制：\nLeader 将状态机快照 + 最新日志元信息发送给落后节点。\n节点直接加载快照，跳过大量历史日志。\n\n\n\n安全性选举限制\n\nCandidate 必须包含所有已提交的日志才能当选。\n具体做法：在 RequestVote RPC 中携带自己最后一条日志的 (term, index)，Follower 只有在自己的日志“不比它更新”时才投票。\n\n提交规则\n\nLeader 不能直接提交当前 Term 之前的日志，即使已复制到多数节点。\n必须等到当前 Term 有日志被复制到多数节点后，才能一起提交之前的所有日志。\n\n状态机安全性\n\n如果一个服务器已将某日志条目应用到状态机，则其他服务器永远不会在同一索引位置应用不同的命令。\n\n成员变更（Membership Changes）Raft 支持动态集群配置变更（如增加&#x2F;删除节点），但需谨慎处理以避免脑裂，使用两阶段提交方式：\n\n先切换到联合共识（joint consensus）：新旧配置同时生效。\n再切换到新配置。\n\netcdetcd 是一个高可用、强一致、分布式键值存储系统，专为可靠地存储关键数据（如配置信息、服务发现元数据、集群状态等）而设计。它被广泛应用于云原生和分布式系统中，最著名的用途是作为 Kubernetes 的默认后端存储。\n核心特性\n强一致性：基于 Raft 共识算法，保证所有节点数据一致\n高可用：支持多节点集群（通常 3 或 5 节点），容忍少数节点故障（n 节点可容忍 ⌊(n-1)&#x2F;2⌋ 故障）\n持久化存储：数据写入磁盘（使用 WAL + snapshot），重启不丢失\nWatch 机制：客户端可监听 key 或目录的变化，实现实时通知\nLease（租约）：支持带 TTL 的 key，用于实现临时节点（如服务注册的健康检查）\n事务支持\t提供原子性的：compare-and-swap（CAS）和多操作事务（Txn）\n安全通信：支持 TLS 加密、客户端证书认证、RBAC 权限控制\n高性能：单集群可支撑数千 QPS，低延迟（毫秒级）\n\n架构与工作原理基于 Raft 的复制\n\netcd 集群由多个节点组成，通过 Raft 算法选举 Leader。\n所有写请求必须由 Leader 处理，Leader 将日志复制到 Follower。\n日志提交后，应用到状态机（即键值存储），并持久化。\n\n存储模型\n\n扁平的键值对：key 是字符串（支持 &#x2F; 分隔，模拟目录结构），value 是字节数组。\n有序存储：key 按字典序排序，支持范围查询（range scan）。\n版本控制：\n每个 key 有两个版本号：\ncreate_revision：创建时的全局修订版本\nmod_revision：最后一次修改的全局修订版本\n\n\n用于实现 Watch 和 CAS。\n\n\n\nWAL + Snapshot\n\nWAL（Write-Ahead Log）：所有变更先写日志，再应用到状态机，保证崩溃恢复。\nSnapshot（快照）：定期将内存状态保存为快照，避免 WAL 过长。旧 WAL 可被压缩删除。\n\n","categories":["cloud"]},{"title":"node","url":"/2026/02/28/cloud-node/","content":"node概念\nNode &#x3D; Worker Machine：可以是云上的 VM（如 AWS EC2）、本地物理服务器、或边缘设备。\n每个 Node 由 控制平面（Control Plane） 管理，自身不自主决策。\n用户通常不直接操作 Node，而是通过声明式 API 操作 Pod&#x2F;Service，由调度器（Scheduler）决定 Pod 落在哪一个 Node 上。\n\nNode 的两种类型\nWorker Node（工作节点）：运行用户应用 Pod 的节点（绝大多数 Node 属于此类）。\nControl Plane Node（控制平面节点）：运行 kube-apiserver、etcd、kube-scheduler 等组件的节点。在高可用集群中，这些节点通常也标记为不可调度（NoSchedule），避免混部应用。\n\nNode 的核心组件\nkubelet：\t节点代理，负责管理 Pod 和容器生命周期，与 API Server 通信。\nkube-proxy：维护网络规则，实现 Service 的流量转发和负载均衡。\nContainer Runtime：容器运行时（如 containerd、CRI-O），负责拉取镜像、运行容器。\n\n此外，Node 通常还需：\n\nCNI 插件（如 Calico、Flannel）：配置 Pod 网络。\nCSI 驱动（可选）：支持动态存储卷挂载。\n\nkube-proxy核心功能\n监听 Service 和 Endpoint 的变化：通过与 API Server 通信，实时获取 Service 及其关联的后端 Pod 列表。\n维护网络规则：根据 Service 的类型（如 ClusterIP、NodePort、LoadBalancer）和配置，设置本地节点上的网络规则，使得发往 Service IP 的流量能被正确路由。\n支持多种代理模式：kube-proxy 支持不同的实现方式（称为“代理模式”），以适应不同性能和功能需求。\n\n\n它只负责 Service 到 Pod 的流量转发，不负责 Pod 之间的网络通信\n\n工作模式iptables 模式（默认模式之一）\n利用 Linux 内核的 iptables 规则直接进行包转发。\nkube-proxy 不处理数据包，只负责动态更新 iptables 规则。\n优点：性能好、稳定。\n缺点：\n规则随 Service&#x2F;Endpoint 数量线性增长，大规模集群下 iptables 规则庞大，更新慢。\n负载均衡基于随机选择，不支持会话保持（session affinity 有限支持）。\n\n\n\nIPVS 模式（推荐用于大规模集群）\n基于 Linux 内核的 IP Virtual Server (IPVS) 技术，属于 L4 负载均衡。\nkube-proxy 通过调用 netlink 接口管理 IPVS 规则。\n优点：\n高性能，支持数万级 Service。\n支持多种负载均衡算法（轮询、最小连接、源地址哈希等）。\n更快的故障恢复和更高效的连接复用。\n\n\n要求节点内核启用 IPVS 模块（通常现代 Linux 发行版默认支持）。\n\n具体类型\nClusterIP：在集群内部通过虚拟 IP 提供服务，kube-proxy 设置规则使 Pod 可访问该 IP。\nNodePort：在每个节点上开放一个端口，kube-proxy 将该端口的流量转发到后端 Pod。\nLoadBalancer：通常由云控制器创建外部 LB，但流量到达节点后仍由 kube-proxy 转发到 Pod。\nExternalName：不涉及 kube-proxy，仅通过 DNS CNAME 实现。\n\nkubeletkubelet 是 Kubernetes 架构中运行在 每个工作节点（Node） 上的核心组件，是 节点代理（Node Agent），负责管理本机上 Pod 和容器的生命周期。\n关系+———————+|    Kubernetes       ||    Control Plane    || (API Server, etc.)  |+———-+———-+           |           | (Watch Pods assigned to this node)           v+———————+|      kubelet        | ← 运行在每个 Node 上+———-+———-+           |           | (via CRI)           v+———————+| Container Runtime   | ← 如 containerd, CRI-O+———-+———-+           |           v+———————+| Linux Kernel        || (cgroups, namespaces)|+———————+\n关键特性详解Pod 生命周期管理当 API Server 将一个 Pod 分配给该节点后，kubelet：\n\n下载镜像（若不存在）\n创建 pause 容器（infra container）\n挂载卷\n启动业务容器\n执行 initContainers（如有）\n持续监控容器状态\n\n资源管理与 QoS\n根据 Pod 的 requests&#x2F;limits 设置 cgroups 限制。\nQoS 等级决定 OOM（内存不足）时的驱逐优先级：\nGuaranteed：requests &#x3D;&#x3D; limits（最高优先级，最后被杀）\nBurstable：requests &lt; limits 或只设 requests\nBestEffort：未设置 requests&#x2F;limits（最先被杀）\n\n\n\n节点状态管理\nkubelet 定期更新 Node.Status，包括：\nReady condition\nMemoryPressure, DiskPressure, PIDPressure\n可分配资源（Allocatable）\n\n\n若 kubelet 停止上报，Node Controller 会在超时后将节点标记为 NotReady。\n\nEviction（驱逐）机制\n当节点资源紧张（如磁盘、内存），kubelet 会主动驱逐低优先级 Pod 以保护节点稳定性。\n可配置驱逐阈值（如 memory.available&lt;100Mi）。\n\n","categories":["cloud"]},{"title":"pod","url":"/2026/02/28/cloud-pod/","content":"Pod 是 Kubernetes 中最小的可部署、可调度的计算单元，它代表集群中正在运行的一个“应用实例”或“进程组”。\n一个 Pod 可以包含 一个或多个紧密耦合的容器，这些容器共享网络、存储和运行上下文。\n它是对 Linux 命名空间（Namespace） 和 控制组（Cgroups） 的封装。\n每个 Pod 在集群中拥有：\n\n唯一的 IP 地址（集群内可达）\n共享的网络命名空间\n共享的存储卷（Volume）\n统一的生命周期\n\n核心特性\n原子性调度：整个 Pod 被调度到同一个 Node 上，不可拆分\n共享网络：所有容器共享同一 IP 和端口空间，可通过 localhost 互访\n共享存储：通过 Volume 挂载实现容器间文件共享（如 EmptyDir、PVC）\n短暂性：Pod 一旦被删除，其 IP、数据（非持久化卷）将永久丢失\n唯一标识：每个 Pod 有唯一 UID 和名称（在命名空间内）\n生命周期统一：Pod 启动&#x2F;停止&#x2F;重启影响所有容器\n\n结构Pause 容器（Infra Container &#x2F; 基础容器）\n由 kubelet 自动创建，对用户透明。\n镜像通常为 registry.k8s.io&#x2F;pause 或国内镜像如 registry.cn-hangzhou.aliyuncs.com&#x2F;google-containers&#x2F;pause-amd64:3.0。\n作用：创建并持有 Pod 的网络命名空间（IP、端口）、创建 PID、IPC、UTS 等命名空间、作为所有业务容器的“根容器”，确保即使业务容器全部崩溃，Pod 的网络上下文仍存在\n该容器几乎不消耗资源，仅运行 &#x2F;pause 进程。\n\nInit Containers（初始化容器）（可选）\n在主容器启动前按顺序执行。\n成功退出（exit 0） 后，下一个 Init 容器或主容器才会启动。\n用途：初始化配置文件、等待依赖服务就绪（如数据库、API）、执行安全检查或权限设置\n失败时会触发 Pod 重启（取决于 restartPolicy）。\n\nContainers（主容器 &#x2F; 业务容器）\n实际运行业务逻辑的容器（如 Nginx、MySQL、Java 应用等）。\n可以有多个，彼此通过 localhost 通信。\n共享 Pause 容器的网络和存储命名空间。\n\nEphemeral Containers（临时容器，调试用）\n用于故障排查，动态注入到已运行的 Pod 中。\n不参与调度，无资源限制，不能指定端口。\n通过 kubectl debug 命令使用。\n\n多容器目的Kubernetes 引入多容器 Pod 是为了解决以下问题：\n\n主应用需要辅助功能（如日志、监控）：用 Sidecar 容器实现\n不同语言&#x2F;进程需共享本地状态：共享 Volume + localhost 通信\n需要统一管理一组协作进程：绑定生命周期，避免分散部署\n\n如果将这些功能拆成独立 Pod，会面临：\n\n网络延迟（跨 Pod 通信）\n调度不一致（可能分配到不同节点）\n生命周期不同步（主应用重启时 sidecar 未就绪）\n\n使用模式Sidecar 模式（最常见）\n为主容器提供辅助功能的“伴生”容器。\n典型用途：日志收集、监控指标采集、证书自动刷新、数据同步\n\napiVersion: v1kind: Podmetadata:  name: web-with-sidecarspec:  containers:  - name: nginx    image: nginx    volumeMounts:    - name: shared-logs      mountPath: /var/log/nginx  - name: log-agent    image: fluentd    volumeMounts:    - name: shared-logs      mountPath: /var/log/nginx  volumes:  - name: shared-logs    emptyDir: &#123;&#125;\nAdapter 模式\n标准化主容器的输出格式，便于外部系统消费。\n用途：将应用日志转换为 JSON 格式、汇总多个指标为统一接口\n\n✅ 示例：应用输出原始指标 → Adapter 转换为 Prometheus 格式\nAmbassador 模式\n作为主容器与外部服务之间的代理。\n用途：隐藏外部服务细节（如数据库连接池）、实现重试、超时、加密等网络策略\n\n✅ 示例：应用连接 localhost:3306 → Ambassador 容器转发到真实 MySQL 集群\n关键技术实现共享网络：localhost 通信\n\n所有容器共享同一个网络命名空间。\n主容器监听 8080，sidecar 可直接访问 http://localhost:8080/metrics。\n\n⚠️ 注意：端口不能冲突！每个容器可监听相同端口，但实际只能有一个绑定成功。\n共享存储：Volume 挂载\n\nemptyDir：Pod 生命周期内临时共享（推荐用于日志、缓存）\nconfigMap &#x2F; secret：共享配置或密钥\npersistentVolumeClaim：持久化共享数据（较少用于多容器，因通常属于单一应用）启动顺序控制：Init Containers\n若 sidecar 依赖主容器先写入配置文件，可用 Init Container 确保顺序。\n但 主容器之间无法控制启动顺序（K8s 不保证 containers 列表的启动顺序）。\n\n\n✅ 解决方案：在 sidecar 中加入重试逻辑（如等待 localhost:8080 就绪）。\n\n对比\n\n\n维度\n多容器 Pod\n独立 Pod\n\n\n\n调度粒度\n原子整体\n可分散\n\n\n通信方式\nlocalhost（低延迟）\nService &#x2F; ClusterIP（网络开销）\n\n\n资源共享\n共享 Volume、IP\n需通过 PVC、Service 共享\n\n\n生命周期\n完全同步\n独立管理\n\n\n适用场景\n紧耦合辅助任务\n松耦合微服务\n\n\n\n❌ 错误用法：把前端 + 后端放在一个 Pod → 应拆分为两个 Service。\n\n","categories":["cloud"]},{"title":"minikube","url":"/2026/02/28/cloud-minikube/","content":"核心原理在一个隔离的虚拟化环境（如 VM 或容器）中，启动一个包含所有 Kubernetes 控制平面组件和工作节点的“一体化”单节点集群。\n运行环境现代 Minikube 默认使用 Docker 驱动，即把整个 Kubernetes 节点封装在一个容器里（称为 “minikube node container”）。\n部署方式在 Minikube 创建的环境中，所有 Kubernetes 核心组件以 静态 Pod（Static Pods） 或 systemd 服务 的形式运行：\n\n控制平面组件（Control Plane）：kube-apiserver、etcd、kube-scheduler、kube-controller-manager\n工作节点组件（Node）：kubelet、containerd、kube-proxy\n\n网络与访问\n内部网络：Minikube 自动配置 CNI（默认使用 CNI bridge + host-local IPAM），为 Pod 和 Service 分配 IP。\n外部访问：\n通过 minikube ip 获取集群 IP；\n使用 minikube service  自动打开浏览器或打印可访问 URL；\n支持 Ingress（需启用插件）；\n通过 kubectl 与集群交互（自动配置 ～&#x2F;.kube&#x2F;config）。\n\n\n\n存储\n使用本地目录挂载（如 &#x2F;data）模拟持久卷；\n支持动态存储（通过内置的 hostpath-provisioner）；\n可启用插件支持更高级存储（如 CSI）。\n\n工作流程当你运行 minikube start 时，Minikube 会执行以下步骤：\n\n检查并下载依赖\n检查是否安装了 Docker&#x2F;VirtualBox 等驱动；\n下载指定版本的 Kubernetes 镜像（如 k8s.gcr.io&#x2F;kube-apiserver:v1.29.0）。\n\n\n创建运行环境\n若使用 Docker 驱动：docker run -d –name minikube … gcr.io&#x2F;k8s-minikube&#x2F;kicbase:…\n该容器基于 kicbase（Kubernetes in Container Base）镜像，预装了 systemd、containerd、crictl 等工具。\n\n\n在容器&#x2F;VM 中启动 Kubernetes\n启动 kubelet；\nkubelet 自动加载 &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F; 下的静态 Pod 清单，拉起 API Server、etcd 等；\n\n\n初始化网络插件（如 bridge CNI）；\n配置 DNS（CoreDNS）。\n配置 kubectl 上下文\n自动生成 kubeconfig 文件，指向 Minikube 集群；\n设置证书和 token 认证。\n\n\n启用插件（可选）\n如 dashboard、ingress、metrics-server 等可通过 minikube addons enable 开启。\n\n\n\n关键技术点KIC（Kubernetes in Container）\n\nMinikube 使用特殊的基础镜像 kicbase，它是一个轻量级 Linux 系统（基于 Ubuntu&#x2F;Debian），内置 systemd，可在容器中运行完整系统服务。\n这使得 Kubernetes 组件能像在 VM 中一样运行，但启动更快、资源占用更低。\n\n静态 Pod（Static Pods）\n\n控制平面组件不是由 API Server 管理的普通 Pod，而是由 kubelet 直接监控的静态 Pod。\n即使 API Server 挂掉，kubelet 仍能重启这些关键组件。\n\n资源隔离与限制\n\n可通过 –memory, –cpus 参数限制 Minikube 资源使用；\n避免占用过多本地机器资源。\n\n","categories":["cloud"]},{"title":"service","url":"/2026/02/28/cloud-service/","content":"ServiceService 是一个核心的网络抽象对象，用于定义一组 Pod 的访问策略，为动态变化的后端 Pod 提供稳定、统一的网络入口。它是实现微服务通信、负载均衡和服务发现的关键机制。\n来源在 Kubernetes 中，Pod 具有以下特性：\n\n生命周期短暂：Pod 可能因扩缩容、节点故障、滚动更新等原因被销毁和重建。\nIP 地址不固定：每次新建的 Pod 会分配新的 IP 地址。\n\n\n❌ 直接通过 Pod IP 访问服务极不稳定。Service 解决的问题：\n\n\n提供一个固定的虚拟 IP（ClusterIP）或 DNS 名称\n自动发现并负载均衡到所有匹配的后端 Pod\n支持从集群内部或外部访问服务\n\n核心原理\nService 通过 selector 字段匹配具有相同标签的 Pod。\n\nselector:  app: nginx\n\n自动维护 Endpoints（或 EndpointSlice）\n\n\n控制平面组件（Endpoint Controller）会监控匹配的 Pod。\n动态生成 Endpoints 对象，列出所有健康的 Pod IP + 端口。\nkube-proxy 或 CNI 插件据此配置转发规则。\n\n\n流量转发由 kube-proxy 或 CNI 实现\n\n\n默认通过 kube-proxy 在每个 Node 上配置 iptables&#x2F;IPVS 规则。\n流量发往 Service IP → 被内核网络子系统转发到某个后端 Pod。\n\n关键字段\nselector：标签选择器，用于匹配后端 Pod\nports.port：Service 暴露的端口（客户端连接此端口）\nports.targetPort：后端 Pod 实际监听的端口（可为数字或名称）\nports.protocol：协议（TCP&#x2F;UDP&#x2F;SCTP，默认 TCP）\nexternalTrafficPolicy：Cluster（默认）或 Local，影响源 IP 保留和跨节点转发\nsessionAffinity：是否启用会话亲和性（ClientIP 或 None）\n\n类型ClusterIP（默认类型）\n创建一个仅在集群内部可访问的虚拟 IP。\n适用于微服务之间的内部调用（如 frontend → backend）。\n\napiVersion: v1kind: Servicemetadata:  name: my-servicespec:  type: ClusterIP  selector:    app: my-app  ports:    - protocol: TCP      port: 80        # Service 端口      targetPort: 8080 # Pod 端口\nNodePort\n在每个 Node 的主机网络上开放一个固定端口（30000–32767）。\n外部可通过 &lt;任意NodeIP&gt;: 访问服务。\n底层仍基于 ClusterIP，NodePort 是其扩展。\n\nspec:  type: NodePort  ports:    - port: 80      targetPort: 8080      nodePort: 31000  # 可选，否则自动分配\nLoadBalancer\n专为云环境设计（AWS、GCP、Azure 等）。\n自动创建一个云厂商的外部负载均衡器（如 AWS ELB、GCP CLB）。\n负载均衡器将公网流量转发到各 Node 的 NodePort，再由 kube-proxy 转发到 Pod。\n\nspec:  type: LoadBalancer  ports:    - port: 80      targetPort: 8080\n\n☁️ 结果：获得一个公网 IP（.status.loadBalancer.ingress[0].ip），可直接对外提供服务。\n\nExternalName\n不关联任何 Pod。\n将 Service 映射到一个外部 DNS 名称。\n通过 CNAME 记录实现。\n\nspec:  type: ExternalName  externalName: my.database.example.com\n\n📌 集群内访问 my-service → 自动解析为 my.database.example.com。适用于对接外部数据库、API 等。\n\nHeadless Service（无头服务）\n不分配虚拟 IP，也不做负载均衡。\nDNS 直接返回所有后端 Pod 的 IP 列表。\n适用于：StatefulSet（如数据库主从、ZooKeeper），客户端需要直连特定 Pod（如 gRPC 客户端负载均衡）\n\nspec:  clusterIP: None  selector:    app: redis\n\n🔍 查询 DNS：nslookup redis-service → 返回多个 A 记录（Pod IPs）\n\nService 与 Ingress 的区别\n\n\n特性\nService\nIngress\n\n\n\n层级\nL4（传输层，基于 IP+Port）\nL7（应用层，基于 HTTP&#x2F;HTTPS Host&#x2F;Path）\n\n\n协议支持\nTCP&#x2F;UDP\n仅 HTTP&#x2F;HTTPS\n\n\n外部访问\nNodePort&#x2F;LoadBalancer\n需部署 Ingress Controller（如 Nginx、Traefik）\n\n\n功能\n负载均衡、服务发现\n路由、TLS 终止、重写、认证等\n\n\n资源类型\nService\nIngress（独立 API 对象）\n\n\n✅ 最佳实践\n内部服务 → ClusterIP Service\n外部 HTTP 服务 → Ingress + ClusterIP Service\n非 HTTP 服务（如 MySQL、gRPC）→ LoadBalancer 或 NodePort\n\n","categories":["cloud"]},{"title":"组件","url":"/2026/02/28/cloud-%E7%BB%84%E4%BB%B6/","content":"kube-apiserverkube-apiserver 是 Kubernetes 控制平面（Control Plane）的核心组件，是整个 Kubernetes 系统的“前端”入口。它负责暴露 Kubernetes API，处理所有 REST 请求，并为集群状态提供统一的访问接口。无论是用户通过 kubectl、控制器（Controller）、调度器（Scheduler），还是其他组件，都必须通过 kube-apiserver 与集群进行交互。\nkube-schedulerkube-scheduler 是 Kubernetes 控制平面（Control Plane）中的核心组件之一，负责将新创建的 Pod 分配到集群中合适的 节点（Node） 上运行。它是 Kubernetes 调度系统的核心，决定了工作负载在集群中的分布策略。\n调度流程当一个 Pod 被创建但尚未绑定到任何节点（即 spec.nodeName 为空，且状态为 Pending）时，kube-scheduler 会监听到该事件，并执行调度流程，最终选择一个最优节点并将 Pod 绑定到该节点上。\n阶段 1：Filtering（预选）\n\n目标：排除不满足 Pod 要求的节点。\n依据：资源是否足够（CPU、内存）、节点亲和性、Pod 亲和性&#x2F;反亲和性、污点与容忍、卷（Volume）拓扑要求、节点标签匹配、端口冲突等\n结果：生成一个“可行节点列表”（feasible nodes）\n\n阶段 2：Scoring（优选）\n\n目标：在可行节点中选出“最优”节点。\n方法：对每个可行节点打分（默认范围 0–100），分数越高越优先。\n常见评分维度：资源均衡（如 CPU&#x2F;内存剩余最多）、拓扑分布（如跨可用区分散 Pod）、镜像是否已存在（避免重复拉取）、亲和性偏好\n结果：选择得分最高的节点（若有多个同分，随机选一个）\n\n阶段 3：Binding（绑定）\n\n调用 kube-apiserver，将 Pod 的 spec.nodeName 设置为目标节点。\n此后 kubelet 会发现该 Pod 并启动容器。\n\n\n⚠️ 注意：调度决策是一次性的（除非使用动态调度器或重调度器），一旦绑定，不会自动迁移（除非 Pod 被删除重建）。\n\nkube-controller-managerkube-controller-manager 是 Kubernetes 自动化运维的大脑，它将用户的声明式意图转化为实际的集群状态。通过一系列协同工作的控制器，实现了副本管理、节点健康、存储绑定、自动扩缩容等关键功能。\n\n它是多个控制器的“打包运行体”\n基于“期望状态 vs 实际状态”的 reconciliation 模型\n无状态 + Leader Election 支持高可用\n云相关逻辑已移至 cloud-controller-manager\n是理解 Kubernetes 自愈、自适应能力的核心组件\n\ncloud-controller-manager cloud-controller-manager（简称 CCM）是 Kubernetes 控制平面中的一个可选组件，用于将 云厂商特定的控制逻辑 从核心 Kubernetes 组件（如 kube-controller-manager）中解耦出来，实现 云平台无关的核心控制平面 与 云平台相关的扩展逻辑 的分离。\n背景在 Kubernetes 早期版本中，所有与云平台交互的逻辑（如创建负载均衡器、获取节点公网 IP、管理路由等）都直接硬编码在 kube-controller-manager 中。这导致：\n\n核心代码臃肿，难以维护；\n每新增一个云厂商支持，都需要修改 Kubernetes 主干代码；\n裸金属或私有云用户也被迫加载无用的云逻辑。\n\n为解决这些问题，Kubernetes 从 v1.8 开始引入 cloud-controller-manager，采用 插件化架构，将云相关控制器独立出来。\n✅ 目标：Kubernetes 核心 &#x3D; 云无关 + 可移植；云能力 &#x3D; 通过 CCM 插件按需注入\n","categories":["cloud"]},{"title":"mvcc","url":"/2026/02/28/db-mvcc/","content":"mvccMVCC 的核心思想是：“用空间换时间” —— 保留数据的多个版本，让不同事务看到各自“应该看到”的数据快照，从而避免读写冲突。\n组件1. 隐藏字段\n\nDB_TRX_ID：\t最近一次修改该行的事务 ID（插入或更新）\nDB_ROLL_PTR：\t回滚指针，指向该行的上一个版本（存储在 Undo Log 中）\n\n2. Undo Log（回滚日志）\n\n存储数据行的历史版本（链式结构，称为 版本链）。\n每次 UPDATE&#x2F;DELETE 都会将旧行写入 Undo Log，并通过 DB_ROLL_PTR 链接。\n\n3. Read View（读视图）\n\n每个事务在第一次执行 SELECT 时创建（严格来说是在第一次“一致性读”时）。\n决定当前事务能看到哪些版本的数据。\n包含关键信息：\nm_ids：当前活跃（未提交）的事务 ID 列表\nmin_trx_id：m_ids 中最小的事务 ID\nmax_trx_id：下一个将要分配的事务 ID（即当前最大事务 ID + 1）\ncreator_trx_id：创建该 Read View 的事务 ID（若为只读事务则为 0）\n\n\n\n可见性规则当一个事务执行 快照读（普通 SELECT） 时，InnoDB 会沿着版本链向上查找，直到找到一个对该事务可见的版本。对某一行的某个版本DB_TRX_ID，判断是否可见的规则如下：\n\n如果 DB_TRX_ID &#x3D;&#x3D; creator_trx_id → 是自己修改的，可见。\n如果 DB_TRX_ID &lt; min_trx_id → 该版本由已提交的事务生成（因为所有活跃事务 ID ≥ min_trx_id），可见。\n如果 DB_TRX_ID &gt;&#x3D; max_trx_id → 该版本由未来事务生成（不可能发生），不可见。\n如果 min_trx_id ≤ DB_TRX_ID &lt; max_trx_id → 需检查 DB_TRX_ID 是否在 m_ids（活跃事务列表）中：\n若 在：该事务未提交，不可见\n若 不在：该事务已提交，可见\n\n\n\n\nUPDATE &#x2F; DELETE &#x2F; SELECT … FOR UPDATE 属于“当前读”，不走 MVCC 快照，而是读取最新数据并加锁。\n\n锁1. 记录锁（Record Lock）\n\n锁定索引记录本身。若 id 是主键或唯一索引，则对 id&#x3D;10 的记录加记录锁。\n\nSELECT * FROM t WHERE id = 10 FOR UPDATE;\n\n若不使用索引更新，有概率锁全表\n\n2. 间隙锁（Gap Lock）锁定索引记录之间的“间隙”，防止其他事务在该范围内插入新记录。目的：解决幻读问题。\n-- 假设 id 有值：1, 5, 10。会加间隙锁 (5, 10)，阻止插入 id=6,7,8,9SELECT * FROM t WHERE id &gt; 5 AND id &lt; 10 FOR UPDATE;\n\n间隙锁是开区间，不包含端点记录。\n\n3.临键锁（Next-Key Lock）\n\n记录锁 + 间隙锁 的组合。锁定 “记录 + 前面的间隙”，即左开右闭区间 (前一个值, 当前值]。\nInnoDB 默认的行锁算法（在 REPEATABLE READ 隔离级别下）。\n示例：\n索引值：… 5, 10, 15 …\n对 id&#x3D;10 加 Next-Key Lock → 锁住区间 (5, 10]\n效果：既锁住 id&#x3D;10 这行，也阻止插入 6～9 的值。\n\n\n\n\nNext-Key Lock 是 InnoDB 防止幻读的关键机制。\n\n兼容性\n\n\n锁类型 &#x2F; 操作\nSELECT（快照读）\nSELECT … FOR UPDATE\nUPDATE &#x2F; DELETE\nINSERT\n\n\n\n记录锁\n✅ 不冲突\n❌ 冲突\n❌ 冲突\n✅（除非唯一键冲突）\n\n\n间隙锁\n✅\n✅（同间隙可多个）\n✅\n❌（被间隙锁阻塞）\n\n\n临键锁\n✅\n❌\n❌\n❌\n\n\n","categories":["db"]},{"title":"sql","url":"/2026/02/28/db-sql/","content":"语句SELECTSELECT u.name, o.amountFROM users uINNER JOIN orders o ON u.id = o.user_idWHERE u.age &gt; 18 AND u.id in (1)ORDER BY price DESCLIMIT 10UNIONSELECT name FROM teachers;\nINSERTINSERT INTO users (name, email) VALUES (&#x27;Alice&#x27;, &#x27;alice@example.com&#x27;);\nUPDATEUPDATE users SET email = &#x27;new@example.com&#x27; WHERE id = 1;\nDELETEDELETE FROM users WHERE id = 1;\nCREATECREATE TABLE users (    id INT PRIMARY KEY,    name VARCHAR(100),    email VARCHAR(100));\nALTER（修改表结构）ALTER TABLE users ADD COLUMN phone VARCHAR(20);ALTER TABLE users DROP COLUMN phone;\nDROPDROP TABLE users;DROP DATABASE mydb;\nTRUNCATE（清空表中所有数据（比 DELETE 快，不可回滚））TRUNCATE TABLE logs;\nGRANTGRANT SELECT, INSERT ON users TO &#x27;user1&#x27;@&#x27;localhost&#x27;;\nREVOKEREVOKE INSERT ON users FROM &#x27;user1&#x27;@&#x27;localhost&#x27;;\n\n\n执行流程1. 连接与认证（Connection &amp; Authentication）\n客户端通过网络（如 TCP&#x2F;IP）连接到数据库服务器。\n数据库验证用户名、密码、权限等。\n建立会话（Session），分配资源（如内存、线程）。\n\n\n✅ 此阶段发生在 SQL 执行前，属于连接层。\n\n\n2. SQL 解析（Parsing）数据库接收到 SELECT 语句后，首先进行语法和语义分析：\n词法分析（Lexical Analysis）\n\n将 SQL 字符串拆分为“词元”（tokens），如 SELECT、*、FROM、users 等。\n\n语法分析（Syntax Parsing）\n\n检查 SQL 是否符合语法规则（如是否缺少 FROM）。\n构建抽象语法树（AST, Abstract Syntax Tree）。\n\n语义分析（Semantic Analysis）\n\n检查表是否存在、列是否合法、用户是否有权限访问。\n解析别名、函数、表达式等。\n\n\n❌ 如果出错（如表不存在），直接返回错误，不继续执行。\n\n\n3. 查询重写与优化（Query Rewriting &amp; Optimization）查询重写（Optional）\n\n数据库可能对 SQL 进行等价变换，例如：\n视图展开\n子查询转 JOIN\n常量折叠（如 WHERE 1=1 AND name=&#39;Alice&#39; → WHERE name=&#39;Alice&#39;）\n\n\n\n查询优化（Query Optimization）\n\n生成多个可能的执行计划（Execution Plan）。\n基于统计信息（如表行数、索引分布、数据选择性）估算每个计划的成本（Cost）。\n选择成本最低的执行计划。\n\n\n📌 优化器决定：是否用索引？用哪个索引？JOIN 顺序？是否全表扫描？\n\n\n4. 执行计划生成（Plan Generation）\n将选中的执行计划转换为可执行的操作符树（Operator Tree），例如：\n\nProject (name, email)  └── Filter (age &gt; 18)  └── Index Scan (users, idx_age)\n\n5. 执行引擎运行（Execution）数据库的执行引擎按计划一步步操作：\n\n访问存储引擎（如 InnoDB、MyISAM、Heap）\n读取数据页（从磁盘或缓冲池&#x2F;Buffer Pool）\n应用过滤条件（WHERE）\n排序（ORDER BY） → 可能用内存或临时文件\n分组聚合（GROUP BY + 聚合函数）\n去重（DISTINCT）\n连接（JOIN） → 嵌套循环、哈希连接、排序合并等\n限制结果数量（LIMIT）\n\n\n💡 数据通常以“行流”（row-by-row 或 chunk）方式处理，而非一次性加载全部。\n\n\n6. 结果返回（Result Delivery）\n将最终结果集格式化（如二进制协议、JSON、表格）。\n通过网络发送给客户端。\n释放本次查询占用的临时资源（如排序内存、临时表）。\n\n\n📌 举个简单例子SELECT name FROM users WHERE age &gt; 25 ORDER BY name LIMIT 10;\n后台发生了：\n\n检查 users 表是否存在，用户是否有 SELECT 权限。\n解析 AST，确认 age 和 name 是合法列。\n优化器决定：是否有 age 索引？是否值得用？\n若有 (age, name) 覆盖索引 → 直接索引扫描 + 排序。\n若无索引 → 全表扫描 → 过滤 → 排序 → 取前 10。\n\n\n执行引擎读取数据，过滤、排序、截断。\n返回最多 10 行 name 给客户端。\n\n","categories":["db"]},{"title":"事务","url":"/2026/02/28/db-%E4%BA%8B%E5%8A%A1/","content":"特性原子性（Atomicity）\n含义：事务是一个不可分割的工作单元，其中的所有操作要么全部成功执行，要么全部不执行。\n举例：转账操作中，从 A 账户扣款和向 B 账户加款必须同时成功或同时失败，不能只完成其中一步。\n实现机制：MySQL 通过 Undo Log（回滚日志） 实现原子性。如果事务中途失败，系统会利用 Undo Log 回滚已执行的操作。\n\n一致性（Consistency）\n含义：事务执行前后，数据库必须保持一致性状态，即满足所有预定义的约束（如主键、外键、唯一性、检查约束等）。\n注意：一致性不是由数据库单独保证的，而是由 原子性、隔离性和持久性共同保障，再加上应用程序逻辑的正确性。\n举例：账户总金额在转账前后应保持不变。\n\n隔离性（Isolation）\n含义：多个事务并发执行时，一个事务的操作不应影响其他事务，每个事务都像是独立执行一样。\n隔离级别（MySQL InnoDB 引擎支持四种）：\n读未提交：可能读到其他事务未提交的数据（脏读）。\n读已提交：只能读到已提交的数据，避免脏读，但可能出现不可重复读。\n可重复读：MySQL 默认隔离级别。保证在同一事务中多次读取结果一致，避免脏读和不可重复读，但可能出现幻读（InnoDB 通过 MVCC + 间隙锁解决大部分幻读问题）。\n串行化：最高隔离级别，强制事务串行执行，完全避免并发问题，但性能最差。\n\n\n实现机制：主要依靠 MVCC（多版本并发控制） 和 锁机制（如行锁、间隙锁、临键锁）\n\n持久性（Durability）\n含义：一旦事务提交，其对数据库的修改就是永久性的，即使系统崩溃也不会丢失。\n实现机制：MySQL 使用 Redo Log（重做日志）。事务提交时，先将变更写入 Redo Log 并持久化到磁盘，再异步写入数据页。即使宕机，重启后可通过 Redo Log 恢复已提交的事务。\n\n并发问题脏读一个事务读取了另一个尚未提交的事务所修改的数据。\n\n\n\n时间\n事务 A（读）\n事务 B（写）\n\n\n\nT1\n\nUPDATE account SET balance &#x3D; 100 WHERE id &#x3D; 1;\n\n\nT2\nSELECT balance FROM account WHERE id &#x3D; 1; → 得到 100\n\n\n\nT3\n\nROLLBACK; （撤销修改，余额恢复为原值，比如 50）\n\n\n不可重复读在同一个事务中，多次读取同一行数据，但结果不一致，因为其他事务在中间提交了对该行的修改或删除。\n\n\n\n时间\n事务 A\n事务 B\n\n\n\nT1\nSELECT balance FROM account WHERE id &#x3D; 1; → 50\n\n\n\nT2\n\nUPDATE account SET balance &#x3D; 100 WHERE id &#x3D; 1;\n\n\nT3\n\nCOMMIT;\n\n\nT4\nSELECT balance FROM account WHERE id &#x3D; 1; → 100\n\n\n\n幻读在同一个事务中，两次执行相同的范围查询（如 WHERE 条件），但返回的行数不同，因为其他事务在中间插入或删除了满足条件的新记录。\n\n\n\n时间\n事务 A\n事务 B\n\n\n\nT1\nSELECT COUNT(*) FROM orders WHERE status &#x3D; ‘pending’; → 2\n\n\n\nT2\n\nINSERT INTO orders (status) VALUES (‘pending’);\n\n\nT3\n\nCOMMIT;\n\n\nT4\nSELECT COUNT(*) FROM orders WHERE status &#x3D; ‘pending’; → 3\n\n\n\n","categories":["db"]},{"title":"日志","url":"/2026/02/28/db-%E6%97%A5%E5%BF%97/","content":"Redo Log（重做日志）作用\n保证事务的持久性（Durability）\n崩溃恢复（Crash Recovery）：数据库异常宕机后，通过 Redo Log 重做已提交但未写入磁盘的数据页。\n\n实现机制\n物理日志：记录的是“在某个数据页上做了什么修改”（如：page_id&#x3D;100, offset&#x3D;200, value&#x3D;xxx）。\n循环写入：Redo Log 是固定大小的文件（默认 48MB），写满后会覆盖旧日志（前提是 checkpoint 已推进）。\nWAL（Write-Ahead Logging）机制：\n先写 Redo Log 到内存（Log Buffer）\n再根据策略刷盘（innodb_flush_log_at_trx_commit 控制）\n最后异步刷新脏页到磁盘\n\n\n持久化策略\n每秒刷盘（性能高，可能丢 1 秒数据）\n每次 COMMIT 都刷盘（默认，最安全）\n每次 COMMIT 写 OS 缓存，每秒刷盘（折中）\n\n\n\nUndo Log（回滚日志）作用\n保证事务的原子性（Atomicity）：事务回滚时恢复原始数据\n支持 MVCC（多版本并发控制）：为其他事务提供一致性读（快照读）\n\n实现机制\n逻辑日志：记录的是“如何回滚”（如：将 name 从 ‘Bob’ 改回 ‘Alice’）\n存储在 Undo Tablespace（MySQL 8.0+ 中独立的 undo 表空间文件\n与数据行通过 DB_ROLL_PTR 指针形成版本链\n\n\nUndo Log 本身也需要 Redo Log 保护！即 Undo Log 的写入也会记录到 Redo Log 中。\n\nBinlog（Binary Log，二进制日志）作用\n主从复制（Replication）：主库将 Binlog 发送给从库重放\n数据恢复（Point-in-Time Recovery）：结合全量备份 + Binlog 可恢复到任意时间点\n审计与变更追踪\n\n机制（Server 层，所有引擎可用）\n逻辑日志：记录的是 SQL 语句 或 行变更内容\n追加写入：按事务顺序写入，文件不循环（可设置自动清理）\n事务提交时（在 InnoDB commit 之后），由 Server 层写入 Binlog\n\n\n\n\n格式\n说明\n优缺点\n\n\n\nSTATEMENT\n记录原始 SQL\n日志小，但存在函数（如 NOW()）不确定性问题\n\n\nROW\n记录每行变更前后的值\n安全、精确，日志较大（推荐）\n\n\nMIXED\n混合模式，自动选择\n兼顾两者\n\n\n两阶段提交（2PC）—— 保证 Redo Log 与 Binlog 一致性由于 InnoDB 使用 Redo Log，而主从复制依赖 Binlog，MySQL 采用 两阶段提交 来保证两者的一致性：\n\nPrepare 阶段：InnoDB 将 Redo Log 写入并标记为 PREPARE 状态\nCommit 阶段：\nServer 层写 Binlog\nInnoDB 将 Redo Log 标记为 COMMIT 状态\n\n\n如果在中间崩溃，恢复时：\n\n\n若 Redo Log 是 PREPARE 但 Binlog 存在 → 补提交\n若 Redo Log 是 PREPARE 但 Binlog 不存在 → 回滚\n\n这样确保了 主从数据一致 和 崩溃恢复正确性。\n","categories":["db"]},{"title":"索引","url":"/2026/02/28/db-%E7%B4%A2%E5%BC%95/","content":"种类存储分类1. 聚簇索引\n\n聚簇索引决定了表中数据行在磁盘上的物理存储顺序\n一张表只能有一个聚簇索引\n聚簇索引的叶子节点直接存储整行数据（即数据行本身）\n主键查询非常快，因为可以直接定位到数据\n范围查询效率高，因为数据物理上连续存储\n\n2. 二级索引\n\n除聚簇索引之外的所有索引都称为二级索引\n二级索引的叶子节点不存储完整的行数据，而是存储对应行的主键值\n当通过二级索引查找非主键字段时，MySQL 需要进行“回表”操作：先查二级索引得到主键，再用主键去聚簇索引中查找完整行数据。\n\n使用覆盖索引：如果查询的所有字段都包含在二级索引中（例如 SELECT id, name FROM users WHERE name &#x3D; ‘Alice’，且 (name, id) 是索引），则无需回表，提升性能。 \n\n字段分类\n\n\n类型\n是否唯一\n是否允许 NULL\n数量限制\n是否聚簇\n典型用途\n\n\n\n主键索引\n✅ 是\n❌ 否\n每表仅 1 个\n✅ 是（InnoDB）\n唯一标识行，高效查询\n\n\n唯一索引\n✅ 是\n✅ 是（多个 NULL）\n多个\n❌ 否\n保证业务字段唯一（如邮箱）\n\n\n普通索引\n❌ 否\n✅ 是\n多个\n❌ 否\n加速常规查询\n\n\n前缀索引\n❌ 否\n✅ 是\n多个（仅字符串）\n❌ 否\n优化长文本字段索引大小\n\n\n联合索引联合索引依然基于 B+ 树 实现，但排序规则是多级递进排序：以 (name, age, city) 为例：\n\n首先按 name 升序排列；\n当 name 相同时，按 age 升序排列；\n当 (name, age) 都相同时，按 city 升序排列。\n\n叶子节点存储内容（InnoDB）：\n\n所有索引列的值（name, age, city）\n对应的主键值（用于回表）\n\n索引下推：可以对联合索引包含的字段进行判断，直接过滤掉不满足条件的记录，减少回表次数 \n\n查询规则\n\n最左前缀原则：查询条件必须从联合索引的最左侧列开始，并且连续使用，才能有效利用索引\nMySQL 优化器会自动重排 WHERE 条件顺序：例如：WHERE age &#x3D; 25 AND name &#x3D; ‘Alice’ 会被优化为 name &#x3D; ‘Alice’ AND age &#x3D; 25，仍然可以走索引。\n一旦遇到范围条件（如 &gt;, &lt;, BETWEEN, LIKE ‘abc%’），后续列将无法使用索引\n\n索引跳跃扫描\n如果前导列的取值非常少（低基数），那么即使不知道它的具体值，也可以“跳着”遍历所有可能的前导列值，分别对每个值执行一次范围扫描。\n假设有一个复合索引：(gender, age)，gender 只有两个值：’M’ 和 ‘F’（低基数）\n\n查询：SELECT * FROM users WHERE age &#x3D; 30\n虽然查询没指定 gender，但数据库可以：\n枚举 gender 的所有不同值（’M’, ‘F’）\n对每个值，执行一次索引范围扫描：\n扫描 (gender&#x3D;’M’, age&#x3D;30)\n扫描 (gender&#x3D;’F’, age&#x3D;30)\n合并结果\n\nB+树优势1. 高度平衡，保证稳定查询性能\n\nB+ 树是一棵多路平衡查找树，所有叶子节点都在同一层。\n无论插入、删除还是查询，时间复杂度稳定为 O(logₙN)（n 为每个节点的子节点数，通常几百到上千）。\n\n\n例如：10 亿条记录，若每个节点存 1000 个键，则树高仅约 3～4 层 → 最多 3～4 次磁盘 I&#x2F;O 即可定位数据。\n\n2. 专为磁盘 I&#x2F;O 优化：大节点 + 顺序读取磁盘访问代价远高于内存\n\n磁盘一次 I&#x2F;O 通常读取一个页（Page）（InnoDB 默认 16KB）。\nB+ 树的每个节点大小 ≈ 一个磁盘页，一次性加载整个节点到内存，减少 I&#x2F;O 次数。节点存储大量键值\n由于内部节点只存索引键 + 指针（不存实际数据），一个 16KB 节点可容纳数百甚至上千个键。\n相比二叉树（每个节点仅 1 个键），树高大幅降低 → 减少磁盘访问次数。\n\n3. 叶子节点通过指针连接，支持高效范围查询\n\nB+ 树的所有数据都存储在叶子节点\n叶子节点之间用双向链表连接，形成有序序列\n\n4. 内部节点不存数据，提升空间利用率\n\nB+ 树的非叶子节点仅存储索引键和子节点指针，不存储实际数据行。\n相比 B 树（每个节点都存数据），B+ 树的内部节点能容纳更多键 → 树更“胖”、更“矮”。\n\n如果索引字段更短，可以实现更低的树高 + 更高的扇出（fan-out） &#x3D; 更少 I&#x2F;O \n\n对比\n\n\n索引结构\n是否支持范围查询\n是否支持排序\n查找速度\n写入开销\n适用场景\n\n\n\nB+ 树\n✅ 高效\n✅\nO(log N)\n中等\n通用（OLTP&#x2F;OLAP）\n\n\n哈希索引\n❌（仅等值）\n❌\nO(1) 平均\n低\n等值查询（如 Memory 引擎）\n\n\n二叉搜索树\n✅\n✅\nO(N) 最坏\n高\n内存小数据\n\n\n","categories":["db"]},{"title":"缓冲池","url":"/2026/02/28/db-%E7%BC%93%E5%86%B2%E6%B1%A0/","content":"核心作用\n减少磁盘读写：将频繁访问的数据页缓存在内存中，避免每次查询都读磁盘。\n加速 DML 操作：更新操作先在 Buffer Pool 中修改（称为“脏页”），再异步刷盘。\n支持事务与崩溃恢复：配合 Redo Log 实现 WAL（Write-Ahead Logging）机制。\n\nBuffer Pool &#x3D; InnoDB 的“高速缓存”，是 MySQL 性能的“心脏”。 \n\n内部结构缓存页（Page）\n默认大小为 16KB（与 InnoDB 数据页大小一致）\n存储表数据、索引、undo 页等\n\n页描述信息（Page Metadata）\n每个缓存页对应一个 控制块（Control Block），记录：\n表空间 ID、页号\n是否为脏页（is_dirty）\n最近访问时间（用于 LRU）\n锁信息、LSN（Log Sequence Number）等\n控制块本身也占用内存（约 800 字节&#x2F;页），但不计入 innodb_buffer_pool_size\n\n\n\n\n\nLRU 链表（Least Recently Used）\n管理缓存页的淘汰策略\n传统 LRU 问题：一次全表扫描会把热点数据全部挤出缓存\nInnoDB 优化：改进的 LRU 算法将 LRU 链表分为两部分：\nYoung Sublist（热区）：经常访问的页\nOld Sublist（冷区）：新读入或很少访问的页\n新读入的页先放入 Old 区头部\n只有在 Old 区停留超过 innodb_old_blocks_time（默认 1000ms）且被再次访问，才晋升到 Young 区\n\n\n\n避免全表扫描污染缓存 \n\nFree List（空闲页链表）\n记录当前未使用的缓存页，供新数据页加载时使用\n\nFlush List（脏页链表）\n按 LSN（Redo Log 序号） 顺序组织所有脏页\n用于后台线程（如 Master Thread、Page Cleaner）按顺序刷脏页到磁盘\n\nPage Hash Index（页哈希索引）\n快速查找某（表空间ID, 页号）是否已在 Buffer Pool 中\n哈希表结构，O(1) 查找效率\n\n工作流程1. 读取数据\n\nInnoDB 先通过页哈希索引检查 (space_id, page_no) 是否在 Buffer Pool\n若在 → 直接返回（逻辑读）\n若不在 → 从磁盘读入，放入 Free List → Old Sublist 头部\n\n\n\n2. 修改数据\n\n在 Buffer Pool 中找到对应页\n修改内存中的数据（标记为 脏页）\n写入 Redo Log（WAL 机制）\n脏页加入 Flush List\n\n3. 刷脏页（Checkpoint）\n\n后台线程定期将 Flush List 中的脏页写入磁盘\n触发条件：\n脏页比例过高（innodb_max_dirty_pages_pct）\nRedo Log 空间不足（需推进 Checkpoint 释放日志空间）\n系统空闲时\n\n\n\n","categories":["db"]},{"title":"test","url":"/2026/02/28/go-test/","content":"测试规范测试文件命名规则\n\n测试文件必须以 _test.go 结尾，例如：math_test.go\n测试文件与被测代码通常放在同一个包中（即 package 名相同）\n\n测试函数签名\n\n函数名以 Test 开头（大写 T）\n接收一个 *testing.T 类型的参数\n位于 _test.go 文件中\n\n// math_test.gopackage mainimport &quot;testing&quot;func TestAdd(t *testing.T) &#123;    result := Add(2, 3)    if result != 5 &#123;        t.Errorf(&quot;Add(2, 3) = %d; want 5&quot;, result)    &#125;&#125;\n子测试（Subtests）func TestMultiply(t *testing.T) &#123;    tests := []struct &#123;        a, b, want int    &#125;&#123;        &#123;2, 3, 6&#125;,        &#123;0, 5, 0&#125;,        &#123;-1, 4, -4&#125;,    &#125;    for _, tt := range tests &#123;        t.Run(fmt.Sprintf(&quot;%d*%d&quot;, tt.a, tt.b), func(t *testing.T) &#123;            if got := Multiply(tt.a, tt.b); got != tt.want &#123;                t.Errorf(&quot;Multiply(%d, %d) = %d; want %d&quot;, tt.a, tt.b, got, tt.want)            &#125;        &#125;)    &#125;&#125;\n基准测试（Benchmark）func BenchmarkAdd(b *testing.B) &#123;    for i := 0; i &lt; b.N; i++ &#123;        Add(2, 3)    &#125;&#125;\n","categories":["go"]},{"title":"gmp","url":"/2026/02/28/go-gmp/","content":"\n\n\n组件\n全称\n作用\n\n\n\nG\nGoroutine\n用户级轻量级线程，代表一个并发任务\n\n\nM\nMachine\n操作系统线程（OS Thread），真正执行代码的实体\n\n\nP\nProcessor\n逻辑处理器，持有 G 的队列，M 必须绑定 P 才能运行 G\n\n\n\nP 的数量 ≈ 并行能力（默认 &#x3D; CPU 核数）\n\n各组件详解\nG（Goroutine）\n\n\n由 go func() 创建，初始栈大小仅 2KB（可动态增长&#x2F;收缩）\n状态：_Gidle, _Grunnable, _Grunning, _Gwaiting, _Gdead 等\n每个 G 有自己的栈、程序计数器（PC）、寄存器等上下文\n\n\nM（Machine &#x2F; OS Thread）\n\n\n对应一个真实的操作系统线程（通过 clone 或 pthread_create 创建）\n数量通常 ≥ P 的数量（因为系统调用可能阻塞 M）\n启动时会绑定一个 P，然后从 P 的本地队列取 G 执行\n当 M 执行系统调用（如文件 I&#x2F;O、网络 read）时：\n若调用会阻塞 → M 与 P 解绑，P 可被其他 M 抢占\n系统调用返回后 → M 尝试重新绑定 P，否则进入休眠\n\n\n\n\nP（Processor）\n\n\n数量由 GOMAXPROCS 决定（默认 &#x3D; CPU 核数）\n每个 P 拥有：本地 G 队列、内存分配缓存、defer 链表、定时器堆等\nP 是调度的基本单位，M 必须持有 P 才能执行用户代码\n\nGMP 调度流程\n创建 G\n创建 G，放入 P 的本地队列\n如果 P 的本地队列已满（≥256）→ 放入 全局 G 队列\n全局队列由 runtime 管理，所有 P 共享\n\n\nM 执行 G\nM 绑定 P → 从 P 的本地队列取 G 执行\n执行过程中：\n若 G 阻塞（如 channel send&#x2F;receive、mutex lock）→ G 进入等待状态，M 继续执行下一个 G\n若 G 主动让出（如 time.Sleep, runtime.Gosched()）→ G 回到 runnable 状态，放回队列\n\n\n\n\n工作窃取（Work Stealing）为避免某些 P 空闲而其他 P 忙碌，Go 调度器实现负载均衡：\nM 尝试从 自己的 P 本地队列 取 G\n若本地为空 → 从 全局队列 偷一半 G\n若全局也空 → 从 其他 P 的本地队列 偷一半 G\n若所有队列都空 → M 释放 P 并进入休眠（直到有新 G 创建）\n\n\n\n系统调用（Syscall）处理传统方案（如早期 Go）：\n\n整个 M（OS 线程）被阻塞 → 无法执行其他 G → 浪费 CPU\n\nGo 的优化（Syscall 非阻塞化 + M&#x2F;P 解绑）：\n\nG 进入系统调用前，M 与 P 解绑\nP 可立即被其他空闲 M 抢占，继续执行其他 G\n系统调用完成后：\n若有空闲 M → 直接使用\n否则创建新 M（或唤醒休眠 M）\nM 重新绑定 P（或新 P）继续执行 G\n\n\n\n","categories":["go"]},{"title":"内存管理","url":"/2026/02/28/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","content":"垃圾回收三色标记清除1. 三色抽象\n\n白色：尚未被访问的对象，可能可回收。\n灰色：已被访问，但其引用的对象尚未扫描。\n黑色：已被完全扫描，确定存活。\n\n\n初始：所有对象为白色。结束：白色对象为垃圾，可回收；黑色为存活对象。\n\n2. 并发标记的安全性问题：写屏障在并发标记过程中，用户程序仍在运行，可能修改对象图，导致：\n\n漏标：一个本应存活的对象未被标记 → 被错误回收（严重 bug！）。\n\n解决方案：写屏障\n\n在 GC 标记阶段，每当程序执行 *ptr &#x3D; value（写指针）时，插入一段代码（写屏障）。\n写屏障确保：如果 value 是白色对象，且被黑色对象引用，则将 value 标记为灰色，防止漏标。\n\n黑色对象引用白色对象，立即将白色对象标灰，防止漏标 \n\nGC 的完整生命周期\n\n\n阶段\n是否 STW\n说明\n\n\n\nSweep Termination\n✅ 是\n清理上一轮未完成的清扫工作，STW（通常 &lt; 10μs）\n\n\nMark\n❌ 否\n并发标记：扫描根对象（全局变量、栈、寄存器等），传播灰色队列\n\n\nMark Termination\n✅ 是\n完成标记、关闭写屏障、计算下一次 GC 触发阈值，STW（通常 &lt; 100μs）\n\n\nSweep\n❌ 否\n并发清扫：遍历 mspan，释放白色对象占用的内存（惰性回收）\n\n\nSweep 阶段是惰性的，实际内存释放发生在下次分配时或后台 goroutine 中。 \n\nGC 触发机制1. 默认触发条件\n\n堆内存增长达到阈值：当前堆大小 ≥ 上次 GC 后堆大小 × (1 + GOGC&#x2F;100)。GOGC&#x3D;100（默认）\n手动触发：runtime.GC()\n强制触发：内存不足（OOM 前最后尝试）\n\n2. 根对象（Roots）\nGC 从以下位置开始标记：\n\n所有 goroutine 的栈（活跃和非活跃）\n全局变量\n寄存器中的指针\nruntime 内部结构（如 finalizer 队列）\n\n逃逸分析栈堆\n\n\n特性\n栈（Stack）\n堆（Heap）\n\n\n\n分配速度\n极快（仅移动栈指针）\n较慢（需调用内存分配器）\n\n\n回收方式\n函数返回时自动释放\n依赖垃圾回收（GC）\n\n\n生命周期\n限于当前函数作用域\n可跨函数、跨 goroutine\n\n\n并发安全\n每个 goroutine 独占栈\n全局共享，需 GC 管理\n\n\n\n理想情况：所有变量都分配在栈上。现实限制：如果变量的生命周期超出当前函数，就必须分配在堆上——这就是“逃逸”。\n\n建议\n\n\n场景\n优化方式\n\n\n\n不需要指针时，返回值而非指针\nfunc f() int &#123; x:=1; return x &#125;\n\n\n避免在热路径中创建闭包\n改用参数传递或预分配\n\n\n使用 sync.Pool 复用堆对象\n减少频繁分配（适用于大对象）\n\n\n避免将栈变量地址传入接口\nvar i interface&#123;&#125; = &amp;x 会强制逃逸\n\n\n原则：只在真正需要“延长生命周期”时才使用指针。 \n\n内存分配整体架构用户代码   ↓runtime.mallocgc()（内存分配入口）   ├── mcache（Per-P 缓存，无锁）   ├── mcentral（全局中心，按 size class 分组，有锁）   └── mheap（堆管理器，向 OS 申请内存，有锁）         ↓操作系统（mmap / sysAlloc）\n1. mcache（Processor 级缓存）\n\n每个 P（Processor） 拥有一个 mcache。\n它包含多个 mspan，每个对应一个 size class（对象大小类别）。\n分配小对象时无需加锁，因为 P 是 Goroutine 调度的基本单元，天然隔离。\n是 90%+ 小对象分配的主路径，性能极高。\n\n2. mcentral（全局中心）\n\n全局共享，按 size class 组织成多个链表。\n当 mcache 中某个 size class 的 mspan 用完时，会向 mcentral 申请新的 mspan。\n操作需要加锁，但由于频率较低，对性能影响较小。\n\n3. mheap（堆管理器）\n\n管理整个 Go 堆内存，默认最大支持 512GB。\n包含：\narenas：实际内存区域（通过 mmap 向 OS 申请）；\nbitmap：记录哪些字是指针（用于 GC 精确扫描）；\nspans：所有 mspan 的元数据索引。\n\n\n大对象（&gt;32KB）直接从 mheap 分配，绕过 mcache 和 mcentral\n\nmspan是内存分配的基本单位，由连续的 page（每页 8KB） 组成。每个 mspan 只服务一种 size class。内部划分为多个等长 slot，供对象使用。\n\n对象大小分类（Size Class）\n\n\n类型\n大小范围\n分配方式\n\n\n\n微小对象\n≤ 16 字节\n通过 tiny allocator 分配\n\n\n小对象\n16B ～ 32KB\n按 67 个 size class 分类，从 mspan 分配\n\n\n大对象\n&gt; 32KB\n直接从 mheap 分配 span\n\n\n\n✅ 关键设计：小对象被对齐到固定规格（如 16B、24B、32B…），避免内部碎片。\n\n分配流程\n逃逸分析（编译期）\n判断对象是否“逃逸”出当前函数作用域。\n若未逃逸 → 分配在 栈上（高效，无 GC 压力）。\n若逃逸 → 分配在 堆上。\n\n\n堆上分配路径（运行时）\n计算对象大小 → 确定 size class。\n从当前 P 的 mcache 中找到对应 mspan。\n若 mspan 有空闲 slot → 直接分配（无锁！）。\n若 mspan 已满 → 向 mcentral 申请新 mspan。\n若 mcentral 无可用 mspan → 向 mheap 申请内存。\n若 mheap 不足 → 向 操作系统 申请（mmap）。\n\n\n\n\n✅ 优势：绝大多数小对象分配在 mcache 完成，无系统调用、无锁、极快。\n\n大对象分配（&gt;32KB）直接从 mheap 分配一个或多个连续 span，不经过 mcache&#x2F;mcentral缺点：需要全局锁、容易产生外部碎片、分配开销较大\n","categories":["go"]},{"title":"其它","url":"/2026/02/28/go-%E5%85%B6%E5%AE%83/","content":"反射概念反射是指在程序运行时检查变量的类型和值、以及操作其内部结构的能力Go 反射基于两个核心接口：\n\nreflect.Type：表示 Go 中任意类型的元信息（如名称、方法、字段等）。\nreflect.Value：表示一个具体值，可以读取或修改其内容。\n\n用法示例func main() &#123;    var x int = 42    t := reflect.TypeOf(x)    v := reflect.ValueOf(x)    fmt.Println(&quot;Type:&quot;, t)           // Type: int    fmt.Println(&quot;Kind:&quot;, v.Kind())    // Kind: int    fmt.Println(&quot;Value:&quot;, v.Int())    // Value: 42&#125;\n限制\n性能开销大：反射比直接代码慢很多，应避免在性能敏感路径中使用。\n类型安全弱：编译器无法检查反射操作的正确性，容易在运行时报错。\n不能访问未导出（小写开头）字段&#x2F;方法：出于封装原则，反射无法修改或调用非导出成员。\n可修改性（CanSet）：只有“可寻址”且“导出”的字段才能被修改。可通过 v.CanSet() 判断。\n\n反序列一、静态类型 + 编译期优化\n\nGo 是静态类型语言，变量类型在编译时就已确定。\n反序列化时（如 json.Unmarshal），目标结构体的字段类型是已知的，无需运行时动态推断类型。\n编译器可以对字段访问、内存布局等进行优化，减少间接寻址和类型检查开销。\n对比：像 Python&#x2F;JavaScript 这类动态语言，在反序列化 JSON 时需动态创建对象、猜测类型，效率更低。\n\n\n\n二、高效的内存管理与分配\n\nGo 的 内存分配器 针对小对象做了高度优化，反序列化过程中频繁创建字符串、切片等操作非常高效。\n标准库（如 encoding&#x2F;json）会尽量复用缓冲区、避免不必要的内存拷贝。\n逃逸分析 能将临时对象分配在栈上，减少 GC 压力。\n\n三、反射虽慢，但被最小化使用\n虽然 encoding&#x2F;json 等包底层确实使用了 reflect 包，但 Go 团队做了大量优化：\n\n缓存反射元数据:第一次反序列化某个类型时，会缓存其 Type 和字段信息（如 tag、偏移量）。后续反序列化直接使用缓存，避免重复反射开销。避免深度反射循环\n对常见类型（如 string, int, bool）有快速路径（fast path），绕过通用反射逻辑。\n结构体字段按偏移量直接写入:一旦知道字段在内存中的偏移，可直接通过指针写入值，而非通过 Value.Set() 等通用接口。\n\n","categories":["go"]},{"title":"控制","url":"/2026/02/28/go-%E6%8E%A7%E5%88%B6/","content":"sync.Once底层结构type Once struct &#123;    done uint32       // 标记是否已执行（0=未执行，1=已执行）    m    Mutex         // 互斥锁，用于保护并发执行&#125;\n执行流程func (o *Once) Do(f func()) &#123;    if atomic.LoadUint32(&amp;o.done) == 0 &#123;        o.doSlow(f)    &#125;&#125;func (o *Once) doSlow(f func()) &#123;    o.m.Lock()    defer o.m.Unlock()    if o.done == 0 &#123;        defer atomic.StoreUint32(&amp;o.done, 1)        f()    &#125;&#125;\n原子变量可以避免指令重排造成的错误 \n\nsync.WaitGroup底层结构type WaitGroup struct &#123;    noCopy noCopy       // 静态检查：防止复制    sema   uint32    state1 [3]uint32     // 存储 counter（高 32 位）和 waiter（低 32 位）&#125;\n\nWaitGroup 内部使用一个 64 位状态字（state） 和一个 信号量（sema） 实现：\ncounter：未完成的任务数（即 Add 的总和减去 Done 的次数）\nwaiter：正在调用 Wait() 的 goroutine 数量\nsema：用于阻塞&#x2F;唤醒 goroutine 的信号量（由 runtime 管理）\n\n执行流程\nAdd &#x2F; Done 使用 原子操作 修改 counter。\nWait()：\n如果 counter &#x3D;&#x3D; 0，立即返回。\n否则，将 waiter +1，并通过 runtime_Semacquire 阻塞自己。\n\n\n当 Done() 使 counter 归零时，会通过 runtime_Semrelease 唤醒所有等待者。\n\n无锁设计：仅依赖原子操作和信号量，性能极高 \n\ncontext用途在并发程序中，常遇到以下问题：\n\n如何取消一个正在运行的 goroutine？\n如何为操作设置超时？\n如何在调用链中传递请求 ID、用户信息等元数据？\n如何避免因某个子任务卡住导致整个服务阻塞？\n\n传统的 channel 或 sync.WaitGroup 难以统一解决这些问题，而 context 提供了标准化、可组合、可传播的解决方案。\n接口type Context interface &#123;    Deadline() (deadline time.Time, ok bool)        // 获取截止时间    Done() &lt;-chan struct&#123;&#125;                          // 返回一个 channel，用于接收取消信号    Err() error                                     // 返回 context 被取消的原因    Value(key any) any                              // 获取与 key 关联的值&#125;\n\n不可变性：所有 context 操作都返回新 context，原 context 不变。\n树形结构：context 形成父子关系，父 context 取消会级联取消所有子 context。\n\nWithCancel：手动取消ctx, cancel := context.WithCancel(context.Background())go func() &#123;    // 模拟工作    select &#123;    case &lt;-time.After(5 * time.Second):        fmt.Println(&quot;Work done&quot;)    case &lt;-ctx.Done():        fmt.Println(&quot;Cancelled:&quot;, ctx.Err())        return    &#125;&#125;()time.Sleep(2 * time.Second)cancel() // 发出取消信号\n\nWithTimeout：超时自动取消ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)defer cancel() // 推荐：即使超时也会释放资源// 模拟耗时操作select &#123;case &lt;-time.After(3 * time.Second):    fmt.Println(&quot;Finished&quot;)case &lt;-ctx.Done():    fmt.Println(&quot;Timeout:&quot;, ctx.Err()) // context deadline exceeded&#125;\n\nWithCancel：手动取消ctx, cancel := context.WithCancel(context.Background())go func() &#123;    // 模拟工作    select &#123;    case &lt;-time.After(5 * time.Second):        fmt.Println(&quot;Work done&quot;)    case &lt;-ctx.Done():        fmt.Println(&quot;Cancelled:&quot;, ctx.Err())        return    &#125;&#125;()time.Sleep(2 * time.Second)cancel() // 发出取消信号\n\nWithValue：传递请求作用域的值type key stringconst RequestIDKey key = &quot;requestID&quot;ctx := context.WithValue(context.Background(), RequestIDKey, &quot;12345&quot;)// 在处理函数中获取func handle(ctx context.Context) &#123;    if id, ok := ctx.Value(RequestIDKey).(string); ok &#123;        log.Printf(&quot;Request ID: %s&quot;, id)    &#125;&#125;\n","categories":["go"]},{"title":"数据结构","url":"/2026/02/28/go-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"slice底层实现type SliceHeader struct &#123;    Data uintptr // 指向底层数组的指针    Len  int     // 切片当前长度    Cap  cap     // 切片容量（从 Data 指针开始到底层数组末尾的元素个数）&#125;\n所以多个切片可以共享同一个数组\n扩容机制\n如果当前容量 &lt; 1024：新容量 ≈ 旧容量 * 2\n如果 ≥ 1024：新容量 ≈ 旧容量 * 1.25\n最终容量会向上对齐到满足内存对齐要求的值\n\nstring底层实现type StringHeader struct &#123;    Data uintptr // 指向底层字节数组的指针    Len  int     // 字符串长度（以字节为单位）&#125;\nstring的内容创建之后就无法修改\nrune\nstring 是 UTF-8 编码的字节序列\n[]rune 是 Unicode 码点序列\n转换开销为O(n)\nfor i 遍历string的字节\nfor range 遍历string的rune\n\n处理string\ns +&#x3D; “x” 在循环中会多次内存分配（O(n²)）\n使用 strings.Builder 预分配缓冲区\nGo 的 &#x3D;&#x3D; 对 string 做了优化（先比长度，再比内容）\n使用 strings.Clone 避免内存泄漏\n\nmap底层结构type hmap struct &#123;    count     int      // 元素个数    flags     uint8    // 状态标志（如是否正在写入）    B         uint8    // 桶的数量为 2^B（即 bucket 数量）    noverflow uint16   // 溢出桶的近似数量    hash0     uint32   // 哈希种子（用于防碰撞攻击）    buckets    unsafe.Pointer // 指向 bucket 数组（长度 = 2^B）    oldbuckets unsafe.Pointer // 扩容时指向旧 bucket 数组（用于渐进式迁移）    nevacuate  uintptr        // 已迁移的 bucket 数量（扩容中使用）    extra      *mapextra      // 溢出桶等额外信息&#125;\n// 逻辑结构（实际在编译时被展开以避免内存对齐浪费）type bmap struct &#123;    topbits [8]uint8   // 高 8 位哈希值（用于快速比较）    keys    [8]KeyType // 存储 key    values  [8]ValueType // 存储 value    overflow unsafe.Pointer // 指向下一个溢出桶（*bmap）&#125;\n工作流程\n对 key 计算哈希值（使用 hash0 种子 + 哈希函数）\n取低 B 位 → 定位到主 bucket（bucketIndex &#x3D; hash &amp; (2^B - 1)）\n取高 8 位 → 存入&#x2F;比对 topbits\n在 bucket 的 8 个槽位中线性查找：\n若 topbits 匹配 → 再比较完整 key（避免哈希冲突误判）\n若找到空槽 → 插入\n若 bucket 满了 → 追加到 溢出桶（overflow bucket）\n\n\n\nhmap └── buckets (array of bmap, size = 2^B)      ├── bmap[0]: [k0,v0], [k1,v1], ..., [k7,v7] → overflow → bmap&#x27;      ├── bmap[1]: ...      └── ...\n扩容和迁移\n触发条件：当 count &gt;&#x3D; 6.5 * 2^B（即平均每个 bucket 超过 6.5 个元素）\n扩容方式：\n翻倍扩容：B +&#x3D; 1，bucket 数量 ×2\n等量扩容：当 overflow 桶过多但元素不多时（解决 key 分布不均）\n\n\n扩容时不一次性迁移所有数据，而是在 每次读写操作时迁移 1～2 个 bucket，避免大 map 扩容时长时间阻塞\noldbuckets 保留旧数据，直到全部迁移完成\n\n随机遍历\n程序启动时，runtime 生成一个全局随机种子（基于 ASLR、时间等）\n每次 for range map 时，mapiterinit 使用该种子计算：\n起始桶索引\n桶内起始位置\n\n\n遍历按“从该点开始，循环 wrap around”的方式扫描所有桶\n\n好处\n\n防止开发者依赖非契约行为（避免脆弱代码）\n暴露数据结构的本质（哈希表本就不保证顺序）\n防御哈希碰撞 DoS 攻击\n鼓励显式排序，提升代码可读性与正确性\n\nsync.map底层实现type Map struct &#123;    mu      Mutex          // 保护 dirty    read    atomic.Value   // readOnly 结构（包含 map[interface&#123;&#125;]*entry）    dirty   map[interface&#123;&#125;]*entry    misses  int            // 记录 read 未命中次数（触发 dirty 提升）&#125;type readOnly struct &#123;    m       map[interface&#123;&#125;]*entry    amended bool // true 表示 dirty 包含 read 没有的 key&#125;\n操作流程\n读写分离，维护两个 map：\nread map：只读（实际是 atomic 指针），无锁读\ndirty map：包含新写入的 key，受 mutex 保护\n大部分读操作直接走 read，完全无锁！\n\n\n懒删除 &amp; 懒提升\n删除只是标记（read 中 entry 置为 expunged）\n新 key 先写入 dirty，当 dirty 被访问足够多次后，才将 read 中未删除的 key 批量复制到 dirty\n\n\n缺点\n需要遍历所有 key（Range 性能差）\n需要知道 map 长度（无 Len() 方法）\n\n\n\n堆package mainimport (    &quot;container/heap&quot;    &quot;fmt&quot;)// 1. 定义堆类型（底层是 []int）type IntHeap []int// 2. 实现 sort.Interfacefunc (h IntHeap) Len() int           &#123; return len(h) &#125;func (h IntHeap) Less(i, j int) bool &#123; return h[i] &lt; h[j] &#125; // 最小堆func (h IntHeap) Swap(i, j int)      &#123; h[i], h[j] = h[j], h[i] &#125;// 3. 实现 heap.Interface 的 Push 和 Pop（必须是指针接收者！）func (h *IntHeap) Push(x any) &#123;    *h = append(*h, x.(int))&#125;func (h *IntHeap) Pop() any &#123;    old := *h    n := len(old)    x := old[n-1]       // 取最后一个元素    *h = old[0 : n-1]   // 缩短切片    return x&#125;func main() &#123;    // 4. 创建并初始化堆    h := &amp;IntHeap&#123;4, 2, 5, 1, 3&#125;    heap.Init(h) // O(n) 建堆    // 5. 使用堆    heap.Push(h, 0)    fmt.Println(&quot;堆顶:&quot;, (*h)[0]) // 输出: 0    // 弹出所有元素（从小到大）    for h.Len() &gt; 0 &#123;        fmt.Printf(&quot;%d &quot;, heap.Pop(h)) // 输出: 0 1 2 3 4 5    &#125;&#125;\n","categories":["go"]},{"title":"迭代器","url":"/2026/02/28/go-%E8%BF%AD%E4%BB%A3%E5%99%A8/","content":"基本介绍\niter.Seq2 是 Go 1.23 中引入的标准库新特性，用于支持泛型迭代器（iterator）模式。\n它是对 Go 中“生成器”或“惰性序列”模式的一种官方、高效且内存友好的实现方式，可以看作是对传统 channel + goroutine 模拟 yield 的一种更轻量、无并发开销的替代方案。\n\n背景在 Go 1.23 之前，若想实现类似 Python yield 的惰性序列，通常使用：\nfunc numbers() &lt;-chan int &#123; ... &#125;\n但这种方式有缺点：\n\n启动 goroutine 有调度开销；\n要管理 channel 生命周期；\n\n功能type Seq2[T any, U any] func(yield func(T, U) bool)// type ChatResponseIterator iter.Seq2[ChatResponse, error]\n\n它表示一个双值迭代器（例如 map 的 key-value 对、索引-元素对等）。\n迭代器内部在每次“产出”一对值时，调用 yield(k, v)；\n如果 yield 返回 false，迭代器应提前终止（支持 break 语义）。\n\n例子package mainimport (    &quot;fmt&quot;    &quot;iter&quot;)func mapSeq[K comparable, V any](m map[K]V) iter.Seq2[K, V] &#123;    return func(yield func(K, V) bool) &#123;        for k, v := range m &#123;            if !yield(k, v) &#123;                return // 提前退出            &#125;        &#125;    &#125;&#125;func main() &#123;    m := map[string]int&#123;&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3&#125;    for k, v := range mapSeq(m) &#123;        fmt.Println(k, v)    &#125;&#125;\n对比\n\n\n特性\niter.Seq2\nchannel + goroutine\n\n\n\n是否启动 goroutine\n❌ 否\n✅ 是\n\n\n内存分配\n通常为 0（可内联）\n至少分配 channel 和 goroutine 栈\n\n\n支持 break &#x2F; return\n✅ 原生支持\n需额外信号（如 context）\n\n\n并发安全\n❌ 单线程执行\n✅ 天然跨 goroutine\n\n\n适用场景\n本地惰性序列、转换、过滤\n跨 goroutine 流式处理\n\n\n\n💡 建议：如果不需要并发，优先使用 iter.Seq2；如果涉及 I&#x2F;O、并行生产消费，则仍用 channel。\n\n核心思想由调用者提供一个 yield(k, v) 回调，迭代器在需要“产出”一对值时调用它；若 yield 返回 false，则提前终止。\n","categories":["go"]},{"title":"通信","url":"/2026/02/28/go-%E9%80%9A%E4%BF%A1/","content":"channel底层实现type hchan struct &#123;    qcount   uint      // 当前队列中元素个数    dataqsiz uint      // 缓冲区大小（cap）    buf      unsafe.Pointer // 环形缓冲区    sendx    uint      // 发送索引    recvx    uint      // 接收索引    recvq    waitq     // 等待接收的 goroutine 队列    sendq    waitq     // 等待发送的 goroutine 队列    lock     mutex     // 保护所有字段&#125;\n性能：channel 操作比 Mutex + 共享内存稍慢，但语义清晰、不易出错 \n\n基本语法与操作声明与创建// 无缓冲 channel（同步）ch := make(chan int)// 有缓冲 channel（异步），缓冲区大小为 3ch := make(chan int, 3)\n发送与接收ch &lt;- 42        // 发送 42 到 chx := &lt;-ch       // 从 ch 接收值，赋给 x&lt;-ch            // 接收并丢弃值\n关闭 channelclose(ch)       // 关闭 channel（只有发送方应关闭）\n\n关闭后，不能再发送数据（会 panic）。\n接收方仍可读取剩余数据，读完后返回零值且 ok &#x3D; false。\n\n单向 channelfunc producer(out chan&lt;- int) &#123; // 只能发送    out &lt;- 42&#125;func consumer(in &lt;-chan int) &#123;  // 只能接收    fmt.Println(&lt;-in)&#125;func main() &#123;    ch := make(chan int)    go producer(ch)    go consumer(ch)    time.Sleep(time.Second)&#125;\n\n多路复用 selectselect &#123;case msg := &lt;-ch1:    fmt.Println(&quot;Received from ch1:&quot;, msg)case ch2 &lt;- data:    fmt.Println(&quot;Sent to ch2&quot;)case &lt;-time.After(1 * time.Second):    fmt.Println(&quot;Timeout!&quot;)default:    fmt.Println(&quot;No channel ready&quot;) // 非阻塞&#125;for v := range ch &#123; ... &#125;\n\n随机选择一个就绪的 case 执行（避免饥饿）。\ndefault 子句使 select 非阻塞。\n可用于实现超时、心跳、退出信号等。\n\ncond底层结构type Cond struct &#123;    noCopy noCopy   // 防止复制    L      Locker    // 关联的锁（*Mutex 或 *RWMutex）    notify uint32    // 用于 Signal/Broadcast 的通知机制（实际是信号量）    checker copyChecker&#125;\n\n调用者必须已持有 L 锁。\nWait() 内部：将当前 goroutine 加入等待队列，原子地释放 L 锁，挂起当前 goroutine\n当被 Signal()&#x2F;Broadcast() 唤醒后：自动重新获取 L 锁，返回（此时已持有锁）\n\n使用场景当条件不满足时，goroutine 挂起等待；当其他 goroutine 修改了共享状态并认为条件可能满足时，唤醒等待者去重新检查。 \nfunc main() &#123;    var mu sync.Mutex    cond := sync.NewCond(&amp;mu)    queue := make([]int, 0)    // 消费者    go func() &#123;        for &#123;            mu.Lock()            // 关键：必须在循环中检查条件！            for len(queue) == 0 &#123;                cond.Wait() // 释放锁，等待；唤醒后重新持有锁            &#125;            item := queue[0]            queue = queue[1:]            fmt.Println(&quot;Consumed:&quot;, item)            mu.Unlock()            time.Sleep(time.Second)        &#125;    &#125;()    // 生产者    for i := 0; i &lt; 10; i++ &#123;        mu.Lock()        item := rand.Intn(100)        queue = append(queue, item)        fmt.Println(&quot;Produced:&quot;, item)        mu.Unlock()        cond.Signal() // 通知一个消费者        time.Sleep(2 * time.Second)    &#125;    time.Sleep(30 * time.Second)&#125;\n\n","categories":["go"]},{"title":"错误处理","url":"/2026/02/28/go-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","content":"defer核心特性\n延迟执行：defer 语句会将函数调用“压入”一个后进先出（LIFO）的栈中。\n在函数返回前执行：无论函数是正常返回、panic 还是通过 return 退出，所有已注册的 defer 都会执行。\n参数在 defer 时求值：函数的参数在 defer 语句执行时就已确定，而不是在实际调用时。\n\n性能考量编译器对大多数 defer 场景做了内联优化（open-coded defer）：\n\n如果 defer 位于函数末尾且无复杂控制流，编译器会将其直接展开为普通代码，几乎无额外开销。\n仅在复杂场景（如循环中 defer、大量 defer）才使用运行时机制。\n\n但不要在循环中使用 defer（会导致 defer 栈无限增长） \n\n底层机制Go 运行时维护一个 defer 链表（每个 goroutine 有自己的 defer 栈）：\n\n执行 defer 时，将函数指针、参数、PC 信息打包成 defer 结构体，压入栈。\n函数返回前（包括 panic），运行时依次弹出并执行这些 defer。\n执行完所有 defer 后，才真正返回或终止 goroutine。\n\npanic和recover概念\npanic\n\n调用后立即停止当前 goroutine 的正常执行流程。\n开始栈展开（stack unwinding）：依次执行所有已注册的 defer 函数。\n如果在整个调用栈中都没有被 recover 捕获，程序将打印 panic 信息并退出（exit code 2）。\n\n\nrecover\n\n只能在 defer 函数中调用才有效。\n如果当前 goroutine 正在 panic 状态，recover() 会：\n停止 panic 的传播\n返回传给 panic() 的值\n恢复正常的执行流程（从 defer 返回后继续）\n\n\n如果没有 panic 发生，recover() 返回 nil。\n\n\n\n核心规则：recover 必须在 defer 中调用，且与 panic 在同一个 goroutine 中。 \n特性与细节1. panic 只影响当前 goroutine\n\n每个 goroutine 有独立的调用栈和 panic 状态。\n一个 goroutine panic 不会直接影响其他 goroutine（但主 goroutine 崩溃会导致整个程序退出）。\n\nfunc main() &#123;    go func() &#123;        panic(&quot;goroutine panic&quot;) // 这个 panic 不会影响 main    &#125;()    time.Sleep(time.Second) // 主 goroutine 等待，但子 goroutine 崩溃后消失    fmt.Println(&quot;Main continues&quot;)&#125;\n\n如果程序退出时存在未处理的 panic（即 panic 后 goroutine 终止但未 recover），即使 main 正常结束，Go 也会强制让整个进程以失败状态退出，并可能抑制部分 stdout 输出\n\n2. 多个 panic：只有第一个生效\n3. recover 只能捕获当前 goroutine 的 panic\nerror本质type error interface &#123;    Error() string&#125;\n\n任何实现了 Error() string 方法的类型都实现了 error 接口。\n调用 err.Error() 可获得错误的字符串描述（通常用于日志或调试）。\n\n自定义type PathError struct &#123;    Path string    Err  error&#125;func (e *PathError) Error() string &#123;    return fmt.Sprintf(&quot;access denied to %s: %v&quot;, e.Path, e.Err)&#125;// 使用err := &amp;PathError&#123;Path: &quot;/etc/passwd&quot;, Err: os.ErrPermission&#125;\n对比\n\n\n特性\nerror\npanic\n\n\n\n用途\n预期的、可恢复的错误（如文件未找到）\n不可恢复的严重错误（如数组越界）\n\n\n处理方式\n显式检查 if err != nil\n通过 defer + recover 捕获\n\n\n性能\n零成本（只是指针比较）\n较高（栈展开）\n\n\nGo 推荐\n✅ 首选\n❌ 仅限极端情况\n\n\n","categories":["go"]},{"title":"锁","url":"/2026/02/28/go-%E9%94%81/","content":"互斥锁底层结构type Mutex struct &#123;    state int32    sema  uint32&#125;\n\nstate：一个 32 位整数，复合状态字段，包含多个标志位。\nsema：信号量，用于阻塞和唤醒 goroutine（由 runtime 管理）\n\n\n\n\n位（从低到高）\n含义\n\n\n\n第 0 位（最低位）\nlocked：1 表示已被加锁，0 表示未加锁\n\n\n第 1 位\nwoken：1 表示有 goroutine 被唤醒，正准备获取锁\n\n\n第 2 位\nstarving：1 表示当前处于“饥饿模式”\n\n\n第 3～31 位\nwaiters count：等待队列中 goroutine 的数量\n\n\n工作模式\n正常模式（Normal Mode）\n\n\n新来的 goroutine 可以与刚被唤醒的 goroutine 竞争锁。\n如果竞争失败，进入等待队列。\n不公平：可能导致新来的 goroutine “插队”成功，而老的等待者一直拿不到锁（但能提升吞吐）。\n\n\n饥饿模式（Starvation Mode）\n\n\n当一个 goroutine 等待锁超过 1ms，Mutex 会切换到饥饿模式。\n在此模式下：\n新来的 goroutine 不能直接抢锁，必须进入等待队列尾部。\n锁总是交给等待队列头部的 goroutine（FIFO），保证公平性。\n\n\n一旦某个 goroutine 获取锁后发现：没有等待者，或它的等待时间 &lt; 1ms → 切回正常模式。\n\n设计哲学：在高吞吐和低延迟之间权衡。短时间用正常模式提高性能，长时间等待则切到饥饿模式避免饿死。 \n\n场景\n\n\n场景\n性能\n\n\n\n无竞争\n极快（一次 CAS）\n\n\n轻度竞争\n自旋减少上下文切换\n\n\n高竞争 + 短临界区\n正常模式高吞吐\n\n\n高竞争 + 长等待\n饥饿模式保公平\n\n\n读写锁底层结构type RWMutex struct &#123;    w           Mutex      // 用于保护内部状态和写操作    writerSem   uint32     // 写者信号量（用于阻塞/唤醒写者）    readerSem   uint32     // 读者信号量（用于阻塞/唤醒读者）    readerCount int32      // 当前活跃读者数量（含等待中的？见下文）    readerWait  int32      // 写者正在等待时，还需完成的读者数量&#125;\n\n读共享、写互斥：多个 goroutine 可同时持有读锁（RLock）。\n写锁（Lock）必须独占：不能有其他读或写。0\n写优先：一旦有写者在等待，新的读者将被阻塞，防止写者“饿死”。\n\nGo 使用一个巧妙的 trick：当有写者在等待时，readerCount 会被减去一个大常数（1 &lt;&lt; 30），使其变为负数。\nconst rwmutexMaxReaders = 1 &lt;&lt; 30\n\n正常情况（无写者等待）：readerCount &#x3D; N（N ≥ 0）→ 表示当前有 N 个活跃读者。\n有写者等待时：readerCount &#x3D; N - rwmutexMaxReaders（结果为负）→ 表示：\n实际活跃读者数 &#x3D; readerCount + rwmutexMaxReaders\n且存在至少一个等待的写者。\n\n\n\n优势：用单个 int32 同时表示“读者数量”和“是否有写者等待”，避免额外锁保护。 \n\n读锁加锁\n原子增加 readerCount。\n如果结果 &lt; 0 → 说明有写者在等 → 当前读者需阻塞（通过 readerSem）。\n否则 → 直接成功，无需阻塞。\n\n解锁\n减少 readerCount。\n如果结果仍 &lt; 0，说明有写者在等。\n如果减完后 readerCount &#x3D;&#x3D; -rwmutexMaxReaders，说明所有读者都释放了 → 唤醒写者。\n\n写锁加锁\n先抢 w 锁：确保只有一个写者能执行后续逻辑。\nreaderCount -&#x3D; rwmutexMaxReaders：使 readerCount 变负，阻止新读者进入。\n如果 readerCount !&#x3D; -rwmutexMaxReaders：说明还有读者在临界区。\n将原读者人数加到 readerWait。\n阻塞自己（writerSem）。\n\n\n如果 r &#x3D;&#x3D; -rwmutexMaxReaders：无读者，直接获得写锁。\n\n解锁\nreaderCount +&#x3D; rwmutexMaxReaders → 恢复为正数，允许新读者进入。\n唤醒所有因写者等待而被阻塞的读者。\n最后释放 w 锁，允许下一个写者进入。\n\n","categories":["go"]},{"title":"基本介绍","url":"/2026/02/28/kafka-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/","content":"消息队列核心概念\n生产者（Producer）：发送消息到队列的应用程序。\n消费者（Consumer）：从队列中接收并处理消息的应用程序。\n消息（Message）：传递的数据单元，通常包含有效载荷（payload）和元数据（如路由信息、优先级等）。\n队列（Queue）：存储消息的缓冲区，遵循先进先出（FIFO）原则（部分系统支持优先级队列等变体）。\nBroker：消息队列服务器，负责接收、存储和转发消息。\n\n主要作用\n系统解耦：各个服务之间不直接依赖，通过消息队列间接通信，降低耦合度。\n异步处理：生产者发送消息后可立即返回，无需等待消费者处理完成，提升响应速度。\n流量削峰：在高并发场景下，将突发请求暂存于队列中，由消费者按能力逐步处理，避免系统崩溃。\n可靠传递与重试机制：支持消息持久化、确认机制（ACK）、失败重试等，提高系统可靠性。：顺序保证（部分场景）：某些消息队列（如 Kafka、RocketMQ）支持消息的有序消费。\n\nkafka的优势核心特点\n高吞吐量：每秒可处理数百万条消息。\n持久化存储：消息写入磁盘并支持多副本，保证可靠性。\n水平扩展：支持集群部署，轻松扩展到数百个节点。\n低延迟：端到端延迟通常在毫秒级。\n容错性：通过副本机制实现故障自动恢复。\n\n结构\n\n\n概念\n说明\n\n\n\nTopic（主题）\n消息的逻辑分类，生产者将消息发布到特定 Topic，消费者订阅 Topic 来消费消息。\n\n\nPartition（分区）\n每个 Topic 可分为多个 Partition，用于并行处理和水平扩展。每个 Partition 是一个有序、不可变的消息序列。\n\n\nProducer（生产者）\n向 Kafka Topic 发送消息的应用程序。可指定分区策略（如轮询、哈希等）。\n\n\nConsumer（消费者）\n从 Kafka Topic 读取消息的应用程序。消费者以 Consumer Group（消费者组） 的形式组织。\n\n\nConsumer Group\n多个消费者组成一个组，共同消费一个 Topic。Kafka 保证每条消息只被组内的一个消费者处理（实现负载均衡）。\n\n\nBroker\nKafka 集群中的一个服务器节点，负责存储和转发消息。\n\n\nOffset（偏移量）\n每条消息在 Partition 中的唯一位置标识。消费者通过维护 offset 实现“从哪开始消费”。\n\n\nReplica（副本）\n每个 Partition 可有多个副本，分布在不同 Broker 上，用于容灾。其中一个是 Leader（读写），其余是 Follower（同步）。\n\n\n示例生产者// producer.gopackage mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;time&quot;    &quot;github.com/segmentio/kafka-go&quot;)func main() &#123;    // Kafka broker 地址    brokers := []string&#123;&quot;localhost:9092&quot;&#125;    topic := &quot;test-topic&quot;    // 创建 writer（生产者）    w := kafka.NewWriter(kafka.WriterConfig&#123;        Brokers:   brokers,        Topic:     topic,        Balancer:  &amp;kafka.LeastBytes&#123;&#125;, // 负载均衡策略    &#125;)    defer w.Close()    ctx := context.Background()    for i := 0; i &lt; 5; i++ &#123;        msg := kafka.Message&#123;            Key:   []byte(fmt.Sprintf(&quot;key-%d&quot;, i)),            Value: []byte(fmt.Sprintf(&quot;Hello from kafka-go %d&quot;, i)),        &#125;        err := w.WriteMessages(ctx, msg)        if err != nil &#123;            log.Printf(&quot;Failed to write message: %v&quot;, err)            continue        &#125;        fmt.Printf(&quot;Sent: key=%s, value=%s\\n&quot;, string(msg.Key), string(msg.Value))        time.Sleep(500 * time.Millisecond)    &#125;    fmt.Println(&quot;All messages sent.&quot;)&#125;\n消费者// consumer.gopackage mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;time&quot;    &quot;github.com/segmentio/kafka-go&quot;)func main() &#123;    brokers := []string&#123;&quot;localhost:9092&quot;&#125;    topic := &quot;test-topic&quot;    groupID := &quot;my-group&quot;    // 创建 reader（消费者）    r := kafka.NewReader(kafka.ReaderConfig&#123;        Brokers:       brokers,        GroupID:       groupID,        Topic:         topic,        MinBytes:      1,    // 最小拉取字节数        MaxBytes:      1e6,  // 最大拉取字节数（1MB）        CommitInterval: 1 * time.Second, // 自动提交 offset 间隔    &#125;)    defer r.Close()    ctx := context.Background()    fmt.Println(&quot;Starting consumer...&quot;)    for &#123;        msg, err := r.ReadMessage(ctx)        if err != nil &#123;            log.Printf(&quot;Error reading message: %v&quot;, err)            break        &#125;        // 模拟处理消息（可能失败）        fmt.Printf(&quot;Received: key=%s, value=%s, offset=%d\\n&quot;,            string(msg.Key), string(msg.Value), msg.Offset)        // 【关键】确保处理成功后再继续（kafka-go 默认自动提交 offset）        // 如果需要更可靠：关闭自动提交，手动控制        time.Sleep(200 * time.Millisecond) // 模拟业务处理    &#125;&#125;\n","categories":["kafka"]},{"title":"常见问题","url":"/2026/02/28/kafka-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","content":"消息丢失丢失原因\n生产者发送失败：网络抖动、Broker 宕机、未确认发送成功 \n消息队列存储失败：Broker 崩溃且未持久化、副本同步未完成就确认 \n消费者处理失败：消费后未提交 offset 就崩溃、处理逻辑出错但已“假装消费”\n\n解决方案生产者监听发送结果，检查每条消息是否真正成功\nBroker\n开启消息持久化（默认已开启，写入磁盘）\n合理配置副本数（replication.factor ≥ 3）\n设置 min.insync.replicas ≥ 2，与 acks&#x3D;all 配合，确保至少有 2 个副本写入才认为成功。\n\n消费者只有消息被成功处理后，才提交 ack\n消费重复原因\n\n\n原因\n说明\n\n\n\n消费者处理完但未提交 offset 就崩溃\n下次重启后会重新消费同一批消息\n\n\n自动提交 offset 早于业务处理完成\n处理中宕机 → offset 已提交 → 消息丢失？不，其实是“看似消费了”，但若处理失败，实际需要重试，而 offset 提前提交导致跳过 → 但更常见的是：手动提交延迟 + rebalance 导致重复\n\n\nConsumer Group 发生 Rebalance\n分区重新分配，新消费者可能从上次提交的 offset 开始拉取，而旧消费者可能已处理但未提交\n\n\n生产者重试导致重复发送（未开启幂等）\n虽然这是生产端问题，但最终表现为消费者看到重复消息\n\n\n解决方案✅ 正确思路：不试图彻底避免重复，而是让消费逻辑具备“幂等性”\n\n每条消息有唯一业务 ID（如订单号、事件 ID），缓存中“检查是否处理过” + “标记为已处理” 要原子化\n数据库根据id建立唯一索引\n把业务改成状态机\n\n消息挤压原因\n消费者处理太慢：单条消息处理耗时过长（如调用外部 API、复杂计算）\n消费者数量不足：分区数 &gt; 消费者实例数，无法并行消费\n消费者故障或卡住：死循环、OOM、线程阻塞等\n突发流量高峰：大促、爬虫、异常重试风暴等\n资源瓶颈：CPU、内存、数据库连接、网络带宽不足\n\n解决方案\n紧急扩容（快速止血）\n先增加分区，然后启动更多消费者进程\n\n\n优化消费逻辑（治本）：\n一次拉取多条消息，批量处理（如批量写 DB）\n在单个消费者内启动多个 goroutine 并行处理 （⚠️ 必须保证顺序性不受影响）\n优化业务逻辑\n\n\n架构级应对（长期策略）\n限流 &amp; 降级\n死信队列 + 人工干预\n分优先级队列\n预扩容 &amp; 压测\n\n\n极端情况处理（积压百万+）\n创建新 Consumer Group（避免影响线上）\n编写极简消费者：只做必要处理（甚至只打印日志）\n快速消费掉历史积压消息\n切回正常消费者\n\n\n\n","categories":["kafka"]},{"title":"底层原理","url":"/2026/02/28/kafka-%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","content":"顺序读\n每个 Partition 是一个不可变的、有序的日志。消息按追加顺序分配递增的 offset（偏移量）。Consumer 从低 offset 向高 offset 顺序拉取，不会跳过或乱序读取。\nKafka Consumer 是单线程拉取 + 单线程处理模型（除非你手动并发）。\n只要业务上需要保序的事件共享同一个 key（如用户 ID、订单 ID），它们就会进入同一 Partition，从而被顺序消费。\n\n\n在单个 Partition（分区）内保证消息的顺序性，但不保证跨 Partition 或跨 Topic 的全局顺序。\n\n快顺序写磁盘\nKafka 将消息追加写入（append-only） 到日志文件末尾，不进行随机写或更新。\n\n零拷贝\n消费者拉取消息时，Kafka 使用 sendfile() 系统调用（Linux）。数据从磁盘文件 → 内核 Page Cache → 网卡缓冲区，全程无需拷贝到用户空间，也无需 CPU 参与数据搬运。\n\n批量处理\n多条消息攒成一个批次（batch）再发送和拉取，减少网络请求次数，网络 RTT（往返延迟）被摊薄。\n\n高效压缩\nProducer 可对整个 batch 进行压缩，Consumer 拉取 Broker 后解压。减少磁盘 I&#x2F;O 和存储成本，减少网络带宽占用（压缩比可达 3:1～5:1）。\n\n日志基本单位：Partition\n每个 Topic 被划分为多个 Partition，每个 Partition 是一个独立的、有序的、不可变的消息日志。\n日志中的每条消息按写入顺序分配一个 64 位单调递增的 offset（偏移量），作为唯一标识。\nPartition 是 Kafka 并行处理和水平扩展的基本单元。\n\n物理存储：Segment（段文件）\n为了避免单个日志文件无限增长，Kafka 将每个 Partition 的日志切分为多个 Segment\n文件采用紧凑的二进制格式，支持批量（Record Batch）存储：\n\n\n\n\n文件类型\n命名示例\n作用\n\n\n\n.log\n00000000000000000000.log\n存储实际消息数据（二进制格式）\n\n\n.index\n00000000000000000000.index\n偏移量索引：映射 offset → 物理位置（position in .log）\n\n\n.timeindex\n00000000000000000000.timeindex\n时间戳索引：映射 timestamp → offset\n\n\n\n✅ 文件名以该 Segment 的起始 offset 命名（如 00000000000000000000 表示从 offset 0 开始）。\n\n当满足以下任一条件时，创建新 Segment：\n\n大小达到阈值：默认 log.segment.bytes &#x3D; 1GB\n时间超过阈值：默认 log.roll.ms &#x3D; 7 天（即使未满 1GB 也会滚动）\n\n索引1. Offset 索引\n\n默认每写入 4KB 数据，建立一条索引项\n通过二分查找 + 顺序扫描找到目标2. 时间戳索引\n用于支持 offsetsForTimes() API（根据时间查 offset）。\n同样采用稀疏索引，默认每 4KB 建立一条 (timestamp, relativeOffset) 索引。\n\n清理策略1. 基于时间&#x2F;大小的删除（Delete Policy） ← 默认\n\n超过保留期限或大小的 整个 Segment 文件被删除。\nlog.retention.hours（默认 168 小时 &#x3D; 7 天）\nlog.retention.bytes（单个 Partition 最大字节数）2. 日志压缩（Log Compaction） ← 适用于状态变更场景\n目标：为每个 key 只保留最新 value，类似“快照”。\n适用场景：用户资料更新、配置变更等。\n清理过程：\n后台 Cleaner 线程扫描日志。\n对每个 key，保留最后一个出现的 record。\n旧版本 record 被逻辑删除（Segment 重写后丢弃）。\n\n\n\n消费者平衡原因\n消费者加入组\t新消费者启动并订阅相同 Topic\n消费者离开组\t主动关闭（consumer.close()）、进程崩溃、网络断开\n消费者心跳超时\t未在 session.timeout.ms 内发送心跳（默认 45s）\n消费处理超时\t单次 poll() 处理时间超过 max.poll.interval.ms（默认 5 分钟）\nTopic 元数据变更\t分区数增加、订阅的 Topic 列表变化\n\n流程1：选举 Group Coordinator\n\n每个 Consumer Group 由集群中一个 Broker 担任 Coordinator。\n消费者通过 findCoordinator 请求定位它。\n\n2：JoinGroup 阶段\n\n所有消费者向 Coordinator 发送 JoinGroup 请求。\nCoordinator 选择一个消费者作为 Leader（通常是第一个加入的）。\n返回成员列表给 Leader，其他成员收到空响应。\n\n3：分区分配（由 Leader 执行）\n\nLeader 根据配置的分区分配策略计算分区分配方案。\n将分配结果通过 SyncGroup 请求发送给 Coordinator。\n\n4：SyncGroup 阶段\n\nCoordinator 将分配结果广播给所有消费者。\n每个消费者得知自己负责哪些 Partition。\n\n5：开始消费\n\n消费者根据新分配的分区，从上次提交的 offset 开始拉取消息。\n🔄 整个过程是同步阻塞的：Rebalance 期间，整个消费者组停止消费。\n\n\n\n为什么不让 broker 负责分配Broker 轻量化，不承担业务逻辑，专注 I&#x2F;O策略灵活可扩展，客户端可以自主实现或决定使用何种策略\n\n分配策略\n\n\n策略\n特点\n适用场景\n\n\n\nRangeAssignor（默认）\n按 Partition 范围分配，可能不均\n分区数少、消费者数整除分区数\n\n\nRoundRobinAssignor\n轮询分配，更均匀\n通用场景\n\n\nStickyAssignor（推荐）\n尽量保持原有分配，最小化变动\n减少 Rebalance 开销\n\n\nCooperativeStickyAssignor（Kafka 2.3+）\n增量式 Rebalance，支持部分消费者继续消费\n高可用要求高的场景\n\n\n再平衡的代价\n消费停顿：整个组在 Rebalance 期间无法消费消息\n重复消费：分区被重新分配后，可能从上次提交的 offset 重读\n状态丢失：若消费者维护本地状态（如缓存），迁移后需重建\n资源浪费：频繁 Rebalance 导致大量网络、CPU 开销\n“惊群效应”：多个消费者同时失效 → 同时触发 Rebalance → Coordinator 过载\n\n集群核心角色1. Broker（普通节点）\n\n存储分配给它的 Partition 副本（Leader 或 Follower）。\n处理 Producer 的写入请求（仅 Leader 分区）。\n处理 Consumer 的拉取请求（仅 Leader 分区）。\n与其他 Broker 同步 Partition 的副本数据（Follower 主动 Fetch）。2. Controller（控制器，KRaft 引入）\n集群中唯一的 Active Controller（由所有 Broker 选举产生）。\n职责包括：\n监听 Broker 上下线（通过 ZK watch 或 KRaft Raft log）\n管理 Partition Leader 选举\n触发首选副本选择\n处理 Topic 创建&#x2F;删除&#x2F;分区扩容等元数据变更\n\n\n\n最小集群规模生产环境建议 ≥ 3 个 Broker（保证副本容错）KRaft 模式下 Controller Quorum 也需 ≥ 3 节点（奇数）\n\n数据分布与副本机制（Replication）1. Partition 分布策略\n\n创建 Topic 时，Kafka 尽量将 Partition 均匀分布到各 Broker。\n副本分配遵循：\n同一分区的多个副本不在同一 Broker\n尽量跨机架（通过 broker.rack 配置）\n\n\n\n2. ISR（In-Sync Replicas）机制\n\nLeader：处理读写\nFollower：从 Leader 拉取数据\nISR 列表：与 Leader 保持同步的副本集合（由 replica.lag.time.max.ms 控制）\n只有 ISR 中的副本才有资格被选为新 Leader\n\n3. 故障恢复流程\n\nBroker 宕机 → Controller 检测到失联\nController 从 ISR 中选举新 Leader\nProducer&#x2F;Consumer 自动重试，透明切换\n\n","categories":["kafka"]},{"title":"http","url":"/2026/02/28/network-http/","content":"http常见状态码1xx：信息性状态码表示请求已被接收，需要继续处理。\n\n100 Continue：客户端应继续发送请求体。\n101 Switching Protocols：服务器同意切换协议（如从 HTTP 切换到 WebSocket）。\n\n2xx：成功状态码表示请求已成功被服务器接收、理解并接受。\n\n200 OK：请求成功，响应中包含所请求的数据。\n201 Created：请求成功并且服务器创建了新的资源（常用于 POST 请求）。\n204 No Content：请求成功，但响应中不包含内容（常用于 DELETE 或 PUT 请求）。\n\n3xx：重定向状态码表示客户端需要采取进一步操作才能完成请求。\n\n301 Moved Permanently：请求的资源已永久移动到新位置。\n302 Found（临时重定向）：资源临时位于另一个 URI 下。\n304 Not Modified：资源未修改，可使用缓存（用于条件请求，如带 If-Modified-Since 头）。\n\n4xx：客户端错误状态码表示请求包含语法错误或无法完成。\n\n400 Bad Request：请求格式有误，服务器无法理解。\n403 Forbidden：服务器理解请求，但拒绝执行（权限不足）。\n404 Not Found：请求的资源在服务器上未找到。\n429 Too Many Requests：客户端在给定时间内发送了太多请求（限流）。\n\n5xx：服务器错误状态码表示服务器在处理请求时发生错误。\n\n500 Internal Server Error：服务器遇到未知错误，无法完成请求。\n502 Bad Gateway：作为网关或代理的服务器从上游服务器收到无效响应。\n503 Service Unavailable：服务器暂时不可用（如维护或过载）。\n504 Gateway Timeout：网关或代理服务器未及时从上游服务器收到响应。\n\n版本对比\n\n\n特性\nHTTP&#x2F;1.0\nHTTP&#x2F;1.1\nHTTP&#x2F;2\nHTTP&#x2F;3\n\n\n\n连接模型\n短连接\n长连接（默认）\n多路复用（单连接）\nQUIC 流（独立可靠）\n\n\n传输格式\n纯文本\n纯文本\n二进制帧\n二进制帧（QUIC 封装）\n\n\n队头阻塞\n有（每次请求）\n有（应用层）\n无（多路复用）\n无（流级独立）\n\n\n头部压缩\n❌\n❌\n✅（HPACK）\n✅（QPACK）\n\n\n服务器推送\n❌\n❌\n✅\n✅（受限于浏览器支持）\n\n\n传输层\nTCP\nTCP\nTCP\nUDP + QUIC\n\n\n加密\n可选（HTTPS）\n可选（HTTPS）\n通常加密（TLS）\n强制加密（TLS 1.3）\n\n\nhttps核心目标机密性： 防止第三方窃听通信内容完整性： 确保数据在传输过程中未被篡改身份认证： 验证服务器（有时也包括客户端）的真实身份，防止“钓鱼网站”。\n技术组件\n数字证书（Digital Certificate）\n\n\n遵循 X.509 标准，由 CA（如 Let’s Encrypt、DigiCert、Sectigo）签发。\n包含公钥、域名、组织信息、有效期、CA 签名。\n浏览器内置信任根证书列表，用于验证证书链。\n\n\n非对称加密 vs 对称加密\n\n\n非对称加密（如 RSA、ECC）：用于身份认证和密钥交换，速度慢。\n对称加密（如 AES、ChaCha20）：用于实际数据传输，速度快。\nHTTPS 结合两者优势：非对称加密协商密钥，对称加密传输数据。\n\n\n前向保密（Forward Secrecy）\n\n\n即使服务器私钥未来泄露，也无法解密过去的通信记录。\n通过 ECDHE 等临时密钥交换算法实现，现代 HTTPS 强烈推荐启用。\n\n工作原理（基于 TLS）步骤 1：TCP 连接建立客户端与服务器先通过三次握手建立 TCP 连接（默认端口 443）。\n步骤 2：TLS 握手（关键阶段）这是 HTTPS 安全性的核心，主要完成：协商加密算法套件（Cipher Suite）验证服务器身份（通过数字证书）生成并交换会话密钥（用于后续对称加密）典型 TLS 1.2&#x2F;1.3 握手流程（简化版）：\n\nClient Hello： 客户端发送支持的 TLS 版本、加密套件列表、随机数（Client Random）。\nServer Hello：服务器选择 TLS 版本、加密套件，并返回自己的随机数（Server Random）。\n服务器证书：服务器发送由受信任 CA（证书颁发机构）签发的数字证书，其中包含：域名、服务器公钥、CA 的数字签名、\n客户端验证证书，浏览器检查：证书是否由可信 CA 签发、域名是否匹配、是否在有效期内、是否被吊销（通过 CRL 或 OCSP）\n密钥交换\n在 RSA 密钥交换中：客户端生成预主密钥（Pre-Master Secret），用服务器公钥加密后发送。\n在 ECDHE（前向保密） 中：双方通过椭圆曲线 Diffie-Hellman 交换参数，各自独立计算出相同的会话密钥。\n\n\n生成会话密钥：双方使用 Client Random + Server Random + Pre-Master Secret（或 DH 共享密钥）派生出对称会话密钥（如 AES-256）。\n切换至加密通信：双方发送 Change Cipher Spec 消息，之后所有 HTTP 数据均用会话密钥对称加密传输。\n\n\n✅ TLS 1.3 优化：将握手压缩至 1-RTT（甚至 0-RTT），移除了不安全的加密套件，强制前向保密。\n\nrpc理念\n\n\n维度\nHTTP（尤其是 RESTful）\nRPC\n\n\n\n抽象层级\n面向资源（Resource）\n面向方法&#x2F;函数调用（Procedure）\n\n\n语义重心\n“对某个资源执行什么操作”（GET &#x2F;users, POST &#x2F;orders）\n“调用某个服务的某个方法”（userService.getUser(id)）\n\n\n耦合性\n强调无状态、统一接口（GET&#x2F;POST&#x2F;PUT&#x2F;DELETE）\n允许强契约，客户端和服务端共享接口定义\n\n\n适用场景\n对外开放 API、Web 前端交互、跨组织集成\n内部微服务通信、高性能系统、企业内网服务调用\n\n\n优势1. 性能更高\n\nHTTP&#x2F;1.1 文本协议冗余大（Header 重复、无压缩）；\nRPC 框架常使用二进制协议（如 Protocol Buffers、Thrift），体积小、解析快；\ngRPC 基于 HTTP&#x2F;2，支持多路复用、头部压缩，比传统 REST over HTTP&#x2F;1.1 快数倍。\n\n2. 强类型与接口契约\n\n客户端和服务端自动生成代码，调用就像本地函数，编译期即可检查参数类型、方法是否存在。\n\n3. 更丰富的调用模式\n\nHTTP 主要是 请求-响应（Request-Reply）；\nRPC 支持：单向调用、流式调用、异步回调\n\n4. 内置服务治理能力\n\n服务注册与发现、负载均衡、熔断降级、链路追踪、超时重试、认证鉴权\n\nwebsocket核心特点\n全双工通信：客户端和服务器可同时独立发送数据，互不阻塞。\n单 TCP 连接：握手后复用同一个 TCP 连接，避免重复握手开销。\n低延迟 &amp; 低开销：数据帧头部仅 2～14 字节（HTTP 请求头通常几百字节）。\n跨域支持：通过 Origin 头实现安全校验，天然支持跨域（需服务器允许）。\n基于 TCP：可靠、有序、不丢包（与 UDP 协议不同）。\n\n\nhttp 主要还是用来处理请求访问的\n\n长轮询使用长轮询：\nClient: GET /messages?lastId=100 → (等待 30 秒)Server: （30 秒内无消息）→ 返回空 []Client: 立即再发 GET /messages?lastId=100 →（又等 30 秒）（若第 15 秒有新消息，需等到下次请求才能收到 → 最多延迟 30 秒）\n\n消息延迟 &#x3D; 0 ～ 轮询间隔\n每次请求都有几百字节的 HTTP 头冗余\n\n使用 WebSocket：\nClient ↔ Server: 已建立连接Server: 直接 send(&quot;新消息&quot;) → Client 立即收到Client: send(&quot;回复&quot;) → Server 立即处理\n\n消息延迟 ≈ 网络 RTT（通常 &lt; 100ms）\n数据直接以二进制或文本帧传输，无 HTTP 头\n\n\nwebsocket需要现代浏览器支持\n\n","categories":["network"]},{"title":"tcp","url":"/2026/02/28/network-tcp/","content":"TCP主要特点\n面向连接: 在数据传输前，通信双方必须先建立连接（三次握手），传输结束后需释放连接（四次挥手）。\n可靠传输：TCP 通过以下机制确保数据正确、完整、按序到达：序列号与确认应答、超时重传、校验和、流量控制、拥塞控制\n全双工通信：连接两端可同时发送和接收数据。\n基于字节流：应用层交给 TCP 的数据被视为无结构的字节流，TCP 不保留消息边界（与 UDP 的“报文”不同）。\n点对点：一个 TCP 连接只能在两个端点之间建立（不支持多播或广播）。\n\n连接管理三次握手\nClient                          Server  |        SYN (seq=x)           |  | ---------------------------&gt; |  |      SYN-ACK (seq=y, ack=x+1)|  | &lt;--------------------------- |  |      ACK (seq=x+1, ack=y+1)  |  | ---------------------------&gt; |\n主动关闭方                     被动关闭方  |        FIN (seq=u)           |  | ---------------------------&gt; |  |        ACK (ack=u+1)         |  | &lt;--------------------------- |  |        FIN (seq=v)           |  | &lt;--------------------------- |  |        ACK (ack=v+1)         |  | ---------------------------&gt; |\n\n可靠性机制\n序列号与确认机制\n每个字节都有唯一序列号。\n接收方返回 ACK 表示“已成功接收序号 &lt; N 的所有字节”，期望收到第 N 字节。\n\n\n超时重传\n发送方启动定时器，若未在超时时间内收到 ACK，则重传数据。\n超时时间动态调整，基于 RTT（往返时间）估算。\n\n\n快速重传\n若收到3 个重复 ACK（表示某段丢失），立即重传，无需等待超时。\n\n\n流量控制\n使用滑动窗口机制，接收方通过 窗口字段告知发送方可发送多少字节。\n防止发送方发送过快导致接收方缓冲区溢出。\n\n\n拥塞控制\n慢启动：指数增长 cwnd（拥塞窗口）\n拥塞避免：线性增长\n快速恢复：在快速重传后进入，避免回到慢启动\n拥塞发生时：若超时，cwnd 重置为 1；若快速重传，cwnd 减半\n\n\n\nUDP主要特点\n无连接：发送数据前无需建立连接，直接发送数据报。\n不可靠：不保证数据到达、不重传、不排序、不确认。\n低开销：头部仅 8 字节，处理简单，延迟低。\n面向报文：保留应用层消息边界，一个 send 对应一个 datagram。\n支持广播&#x2F;多播：可向多个主机同时发送（TCP 仅支持点对点）。\n\n工作方式\n发送方：\n应用程序调用 sendto()，指定目标 IP 和端口。\nUDP 封装数据为 datagram，交给 IP 层发送。\n不等待确认，不维护连接状态。\n\n\n接收方：\nIP 层将数据报交给对应端口的 UDP。\nUDP 检查校验和（若启用），若正确则交付给上层应用。\n若校验失败或缓冲区满，直接丢弃，不通知发送方。\n\n\n\nQUIC对比\n\n\n特性\nTCP + TLS\nQUIC\n\n\n\n连接建立\n2–3 RTT\n0–1 RTT\n\n\n多路复用\nHTTP&#x2F;2 仍受 TCP 队头阻塞影响\n流级独立，无队头阻塞\n\n\n连接迁移\n断开重连\n无缝切换（Connection ID）\n\n\n协议演进\n依赖 OS 内核\n用户态实现，快速迭代\n\n\n中间设备干扰\n易被代理&#x2F;NAT 修改\n加密头部，抗干扰\n\n\n\n✅ QUIC &#x3D; UDP + 可靠传输 + 安全加密 + 多路复用 + 连接迁移\n\n可靠传输（核心机制）独立的流与帧应用数据   ↓[Stream] → 逻辑数据通道（如 HTTP/3 的请求/响应）   ↓[Frame] → 将 Stream 数据切片并打上元信息（如偏移量、流ID）   ↓[Packet] → 多个 Frame 打包成一个加密的 QUIC 包（UDP payload）   ↓UDP Datagram → 网络传输单元\n┌───────────────┐│   QUIC Packet │ ← 1 个 UDP datagram│ (Packet #105) │├───────────────┤│   Frames:     ││  ┌─────────┐  ││  │ STREAM  │  │ ← 承载 Stream 4, offset=0～1023│  │ Stream=4│  ││  └─────────┘  ││  ┌─────────┐  ││  │ STREAM  │  │ ← 承载 Stream 8, offset=0～512│  │ Stream=8│  ││  └─────────┘  ││  ┌─────────┐  ││  │   ACK   │  │ ← 确认收到包 #100～104│  └─────────┘  │└───────────────┘\n\nQUIC 将数据划分为多个逻辑流（Streams），每个流独立编号。\n每个流内的数据按偏移量（offset） 有序传输。\n数据以 帧（Frame） 形式封装在 QUIC 包中，常见帧类型：\nSTREAM 帧：携带应用数据\nACK 帧：确认收到的数据\nRESET_STREAM：异常终止流\nMAX_DATA &#x2F; MAX_STREAM_DATA：流量控制\n\n\n\n\n🔑 关键：每个流独立确认和重传，避免队头阻塞！\n\n基于包的确认\n每个 QUIC 包都有一个全局递增的包号（Packet Number）。\n接收方通过 ACK 帧告知发送方哪些包已收到（类似 TCP 的 SACK）。\n发送方根据 ACK 判断丢包，并仅重传丢失的 STREAM 帧（不是整个包）。\n\n\n📌 与 TCP 不同：QUIC 的包号永不重复、严格递增，即使重传也用新包号，避免“重传二义性”。\n\n选择性重传（Selective Retransmission）\n若某个 STREAM 帧丢失，QUIC 只重传该帧（可能放入新包中）。\n接收方可缓存乱序到达的数据，待缺失部分补齐后交付应用。\n\n\n✅ 结果：一个流丢包不影响其他流的数据交付 → 解决队头阻塞。\n\n前向纠错（可选）与快速恢复\n虽然 QUIC 主要依赖重传，但也可结合 FEC（前向纠错）用于实时场景。\n拥塞控制算法（如 Cubic、BBR）在 QUIC 中由用户态实现，可快速更新。\n\n连接 ID 实现连接迁移\nQUIC 使用 Connection ID（而非 IP+端口）标识连接。\n当客户端 IP 变更（如切换网络），只要 Connection ID 不变，连接可无缝继续，无需重新握手，保持会话状态。\n\n一次握手核心思想： 把 TCP 和 TLS 的功能合并到一个协议层中，在 UDP 上一次性完成\n客户端知道服务器支持 QUIC（通过 DNS 记录 HTTPS 或  SVCB，或之前访问过）\nClient → Server:  - 发送 Initial Packet（UDP 包）    • 包含 QUIC 版本号    • 客户端 Connection ID    • **TLS 1.3 ClientHello**（内嵌在 QUIC 帧中）    • 初始加密参数（用于推导密钥）Server → Client:  - 回复 Initial + Handshake Packets    • Server Connection ID    • **TLS 1.3 ServerHello, Certificate, ...**    • 服务端加密参数    • 应用数据（可选，如 HTTP/3 响应头）\n\n🔑 关键：所有这些都在一个 RTT 内完成，因为 QUIC 把 TLS 握手直接作为连接建立的一部分，而不是后续步骤。\n\n","categories":["network"]},{"title":"基础","url":"/2026/02/28/network-%E5%9F%BA%E7%A1%80/","content":"OSI 七层模型介绍\n\n\n层次\n名称\n主要功能\n数据单元\n\n\n\n7\n应用层（Application Layer）\n提供用户与网络的直接接口\n数据（Data）\n\n\n6\n表示层（Presentation Layer）\n数据格式转换、加密解密、压缩\n数据\n\n\n5\n会话层（Session Layer）\n建立、管理和终止会话\n数据\n\n\n4\n传输层（Transport Layer）\n端到端可靠&#x2F;不可靠传输\n段（Segment）或数据报\n\n\n3\n网络层（Network Layer）\n路由选择、逻辑寻址（IP）\n包（Packet）\n\n\n2\n数据链路层（Data Link Layer）\n帧传输、物理寻址（MAC）、差错检测\n帧（Frame）\n\n\n1\n物理层（Physical Layer）\n在物理介质上传输比特流\n比特（Bit）\n\n\n各层详细说明第7层：应用层（Application Layer）\n作用：用户直接交互的层面，为应用程序提供网络服务接口，是用户与网络的“窗口”。\n不是指“应用程序本身”，而是指支持应用通信的协议和服务。\n典型协议&#x2F;服务：HTTP&#x2F;HTTPS（网页）、FTP（文件传输）、SMTP&#x2F;POP3&#x2F;IMAP（邮件）、DNS（域名解析）、Telnet、SSH（远程登录）\n不关心底层如何传输，只关注“做什么”。\n\n第6层：表示层（Presentation Layer）\n作用：解决数据表示问题，确保发送方和接收方对数据的理解一致。\n核心功能：\n数据格式转换：如 ASCII ↔ EBCDIC，JSON ↔ XML。\n加密与解密：SSL&#x2F;TLS 的部分功能在此层实现（实际中常在应用层处理）。\n数据压缩：减少传输量（如GZIP）。\n字符编码转换：UTF-8、GBK 等。\n注意：在 TCP&#x2F;IP 模型中，表示层功能通常由应用层协议自行处理。\n\n\n\n\n\n第5层：会话层（Session Layer）\n作用：管理两个设备之间的通信会话（对话）。\n核心功能：\n建立、维护、同步、终止会话。\n会话检查点与恢复：如断点续传。\n控制对话方向：单工、半双工、全双工。\n\n\n现实情况：大多数现代应用（如Web）将此功能集成到应用层（如通过 cookies 或 tokens 管理会话）。\n\n第4层：传输层（Transport Layer）\n作用：提供端到端（进程到进程）的可靠或不可靠数据传输。\n核心功能：\n分段与重组：将上层数据分割成适合网络传输的单元。\n端口寻址：使用端口号区分同一主机上的不同应用（如80&#x3D;HTTP，443&#x3D;HTTPS）。\n流量控制：防止发送方压垮接收方。\n错误恢复（仅TCP）：确认、重传、排序。\n拥塞控制（TCP）。\n\n\n主要协议：TCP、UDP\n\n第3层：网络层（Network Layer）\n作用：实现主机到主机的逻辑通信，负责跨网络的数据包路由。\n核心功能：\n逻辑寻址：使用 IP 地址（如 192.168.1.10）标识设备。\n路由选择：通过路由器决定最佳路径（依赖路由协议如 OSPF、BGP）。\n分片与重组：当数据包超过 MTU 时进行分片。\n\n\n主要协议：IP、ICMP（ping 使用）、ARP（严格来说属于链路层，但服务于网络层）\n典型设备：路由器\n\n第2层：数据链路层（Data Link Layer）\n作用：在同一物理网络（如同一个局域网）内实现节点到节点的可靠帧传输。\n分为两个子层：\nLLC（Logical Link Control）：提供流量控制、差错控制（较少使用）。\nMAC（Media Access Control）：控制对物理介质的访问，处理 MAC 地址。\n\n\n核心功能：\n成帧（Framing）：将网络层包封装成帧。\n物理寻址：使用 MAC 地址（如 00:1A:2B:3C:4D:5E）。\n差错检测：通过 CRC（循环冗余校验）判断帧是否损坏。\n介质访问控制：如以太网的 CSMA&#x2F;CD（有线）、Wi-Fi 的 CSMA&#x2F;CA（无线）。\n\n\n典型设备：交换机、网桥\n\n第1层：物理层（Physical Layer）\n作用：在物理介质上传输原始比特流（0 和 1）。\n不关心数据含义，只负责“传比特”。\n典型设备：集线器（Hub）、中继器（Repeater）、网卡（NIC 的物理部分）\n\n对比\n\n\n功能\nOSI 七层\nTCP&#x2F;IP 四层\n\n\n\n用户接口、服务\n应用层\n应用层\n\n\n数据表示、加密\n表示层\n← 合并到应用层\n\n\n会话管理\n会话层\n← 合并到应用层\n\n\n端到端传输\n传输层\n传输层\n\n\n路由与IP\n网络层\n网际层（Internet）\n\n\n帧传输、MAC\n数据链路层\n网络接口层\n\n\n比特传输\n物理层\n网络接口层\n\n\n\n✅ OSI 是理论模型，TCP&#x2F;IP 是实践标准。\n\n网址到网页整体流程概览（10个关键步骤）\nURL 解析\nDNS 查询（域名 → IP 地址）\n建立 TCP 连接（三次握手）\n建立 TLS 安全连接（HTTPS 特有，四次握手）\n发送 HTTP 请求\n服务器处理请求并返回 HTTP 响应\n浏览器接收 HTML 内容\n解析 HTML 并构建 DOM 树\n加载并解析 CSS、JavaScript、图片等资源\n页面渲染（Painting）并呈现给用户\n\n具体分析1️. URL 解析浏览器首先解析你输入的字符串：\n\n协议：https\n主机名：www.example.com\n路径：&#x2F;（默认根路径）\n端口：默认 443（HTTPS），若为 HTTP 则是 80\n\n2. DNS 查询（Domain Name → IP Address）\n因为网络通信依赖 IP 地址，而非域名，所以必须将 www.example.com 转换为对应的 IP（如 93.184.216.34）。\nDNS 查询过程：\n浏览器缓存：先查是否近期访问过该域名。\n操作系统缓存（如 Windows 的 DNS Client 服务）。\n本地 hosts 文件（如 &#x2F;etc&#x2F;hosts 或 C:\\Windows\\System32\\drivers\\etc\\hosts）。\n向本地 DNS 服务器（通常是 ISP 提供的，如 8.8.8.8）。\n若本地 DNS 无缓存，则递归查询：根域名服务器（.）、顶级域服务器（.com）权威域名服务器（example.com 的 NS 记录）\n最终获得目标 IP 地址，并缓存结果（TTL 时间内有效）。\n⚠️ DNS 使用 UDP 协议（端口 53），除非响应过大才用 TCP。\n\n\n\n\n\n3. 建立 TCP 连接（三次握手）\n浏览器通过 Socket API 向目标 IP 的 443 端口发起 TCP 连接。\nTCP 三次握手：\nSYN：客户端 → 服务器（“我想连你”）\nSYN-ACK：服务器 → 客户端（“我同意，你也准备好了吗？”）\nACK：客户端 → 服务器（“我准备好了！”）\n\n\n\n4. TLS 握手（仅 HTTPS）\n由于使用的是 https:&#x2F;&#x2F;，需在 TCP 之上建立加密安全通道。\nTLS 1.2&#x2F;1.3 握手主要步骤（简化）：\nClient Hello：浏览器发送支持的加密套件、随机数。\nServer Hello：服务器选择加密算法，返回证书、随机数。\n证书验证：浏览器验证证书是否由可信 CA 签发、是否过期、域名是否匹配。\n密钥交换：双方协商出会话密钥（用于后续对称加密）。\nFinished：双方用密钥加密消息确认握手成功。\n🔒 之后所有 HTTP 数据都通过 AES 等对称加密传输，保证机密性与完整性。\n\n\n\n\n\n","categories":["network"]},{"title":"应用","url":"/2026/02/28/redis-%E5%BA%94%E7%94%A8/","content":"缓存三问缓存雪崩（Cache Avalanche）✅ 定义\n\n大量缓存数据在同一时间失效（或缓存服务整体宕机），导致大量请求同时穿透缓存，直接访问数据库。\n\n🔍 成因\n\n所有缓存 key 设置了相同的过期时间（如统一30分钟），到期后集中失效。\nRedis 实例宕机或网络故障，整个缓存层不可用。\n\n⚠️ 危害\n\n数据库瞬间承受极高并发压力，可能直接崩溃。\n系统整体响应变慢甚至不可用。\n\n🛠 解决方案\n\n设置随机过期时间，在基础过期时间上加一个随机值（如 ±5 分钟），避免集中失效。\n多级缓存（本地缓存 + Redis）\n高可用架构：Redis 集群 + 主从复制 + 哨兵或 Redis Cluster，避免单点故障。\n熔断与降级：当数据库负载过高时，启用服务降级（如返回默认值、限流、拒绝请求）。\n热点数据永不过期（谨慎使用）：对极核心数据可设置“逻辑过期”，由后台异步更新，而非依赖 TTL。\n\n缓存击穿（Cache Breakdown）✅ 定义\n\n某个热点 key 在缓存中过期的瞬间，大量并发请求同时发现缓存未命中，全部打到数据库。\n注意：与雪崩不同，击穿只针对单个热点 key，但并发量极高。\n\n\n\n🔍 成因\n\n某个热门商品、用户信息等被高频访问。该 key 刚好过期，而此时有成千上万请求同时到达。\n\n⚠️ 危害\n\n数据库被同一查询瞬间打爆。虽然影响范围小，但破坏力强。\n\n🛠 解决方案\n\n互斥锁：只允许一个线程去查数据库并回填缓存，其他线程等待或重试。\n热点 key 预热：在高峰前主动加载热点数据到缓存。\n\n缓存穿透（Cache Penetration）✅ 定义\n\n请求的数据在缓存和数据库中都不存在，但每次请求都会穿透缓存直查数据库。\n\n🔍 成因\n\n用户输入非法 ID（如负数、超大 ID）。\n黑客恶意攻击，用大量不存在的 key 发起请求（如 &#x2F;user?id&#x3D;999999999999）。\n\n⚠️ 危害\n\n数据库被无效查询拖垮。\n缓存完全失效，无法起到保护作用。\n\n🛠 解决方案\n\n布隆过滤器（Bloom Filter），快速判断 key 是否一定不存在。\n\n优点：内存效率高，查询快；\n缺点：存在误判（可能把不存在的判为存在，但不会反向误判）。\n\n\n\n\n数据一致性不一致原因\n写操作涉及两个系统：更新 DB 后需同步更新&#x2F;删除缓存。\n操作非原子性：DB 和缓存无法在一个事务中提交。\n并发竞争：多个线程同时读写，顺序错乱导致脏数据。\n\n主流一致性策略Cache-Aside Pattern（旁路缓存，最常用）\n读：先查缓存，未命中则查 DB 并回填缓存\n写：先更新 DB，再删除缓存（不是更新缓存！）\n\n🔧 增强方案（保证强一致）：\n\n删除缓存重试机制：若删缓存失败，将 key 加入消息队列异步重试。\n双删策略（Double Delete）：\n\nupdate_db(data)          # 1. 更新数据库delete_cache(key)        # 2. 立即删除缓存sleep(500ms)             # 3. 等待可能的并发读完成delete_cache(key)        # 4. 再次删除（防击穿回种旧数据）\n\n适用于对一致性要求极高的场景，但增加延迟。\n\nRead&#x2F;Write Through（读写穿透）✅ 特点：\n\n由缓存层代理所有读写操作，应用只与缓存交互。\nWrite Through：写请求先更新缓存，缓存同步写 DB。\nRead Through：读请求由缓存自动加载 DB 数据（类似 Cache-Aside 的读逻辑）。\n\n⚠️ 问题：\n\n需要自定义缓存中间件（如 Redis 本身不支持 Write Through）。\n写性能受 DB 同步写影响。\n\nWrite Behind（写回模式）✅ 特点：\n\n先更新缓存，异步批量写入 DB（如 Kafka + 批处理）。\n写性能极高（无需等待 DB）。\n适合写多读少场景（如日志、监控数据）。\n\n⚠️ 问题：\n\n数据丢失风险：缓存宕机导致未持久化数据丢失。\n一致性最弱：DB 数据可能严重滞后。\n\n关键问题深度解析Q1：为什么“删除缓存”比“更新缓存”更合理？\n\n更新缓存成本高：若缓存数据需多表 JOIN 生成，每次写都要复杂计算。\n并发安全：删除后由下次读触发加载，天然避免并发写冲突。\n懒加载优势：冷数据无需维护缓存。\n\nQ2：最终一致性 vs 强一致性？\n\n最终一致性：允许短暂不一致（秒级），通过重试&#x2F;补偿达到一致（99% 场景足够）。\n强一致性：需分布式事务（如 2PC、TCC），但性能极低，一般不用在缓存场景。\n\n没有银弹！一致性方案需根据业务容忍度权衡80% 场景：Cache-Aside + 删除缓存 + 重试机制15% 高要求场景：双删 + Binlog 监听5% 极端场景：放弃缓存或使用分布式事务\n","categories":["redis"]},{"title":"持久化","url":"/2026/02/28/redis-%E6%8C%81%E4%B9%85%E5%8C%96/","content":"AOF核心思想\n主线程记录每一条修改数据的命令，并在重启时重新执行这些命令，重建原始数据集。\n\n同步策略\n\n\n配置项\n说明\n安全性\n性能\n最多丢失数据\n\n\n\nalways\n每次写命令都调用 fsync 同步到磁盘\n⭐⭐⭐ 极高\n❌ 最差（受磁盘 I&#x2F;O 限制）\n0 字节\n\n\neverysec\n每秒调用一次 fsync（默认推荐）\n⭐⭐ 高\n✅ 良好\n最多 1 秒\n\n\nno\n由操作系统决定何时同步（通常 30 秒）\n⭐ 低\n⚡ 最佳\n可能丢失 30 秒+\n\n\n重写\nAOF 文件会不断增长，即使对同一个 key 多次操作（如 INCR 100 次），也会记录 100 条命令。但最终状态只需一条 SET key 100 即可表示。\n不是读取旧 AOF 文件，而是基于当前内存数据生成最小命令集！\n\nRedis 使用 子进程（fork） 执行重写，避免阻塞：\n\n主进程继续处理请求，并同时将新命令写入 AOF 缓冲区和 AOF 重写缓冲区。\n子进程遍历内存中的所有键值对，生成新的 AOF 文件（临时文件）。\n子进程完成后，主进程：\n将 AOF 重写缓冲区 中的新命令追加到临时文件；\n原子地将临时文件 rename 为正式 AOF 文件；\n后续写入新 AOF 文件。\n✅ 整个过程主进程不阻塞，保证服务可用性。\n\n\n\n\n\nRDB核心思想\n在某个时间点对内存数据做一次完整快照，并保存为文件\n文件是经过压缩的二进制格式，体积小、加载快\n\n实现原理Redis 利用操作系统的 Copy-On-Write（写时复制，COW） 机制高效生成快照：\n\n主进程 fork 出一个子进程。fork 瞬间，子进程与父进程共享内存页。\n子进程遍历内存中的所有键值对，将其序列化并写入临时 RDB 文件。\n主进程继续处理客户端请求：若有写操作修改了某内存页，操作系统会为该页创建副本（COW），子进程仍读取 fork 时的原始数据。\n子进程完成写入后，原子地将临时文件 rename 为 dump.rdb。\n旧 RDB 文件被覆盖，新快照生效。\n\n\n✅ 整个过程主进程几乎不阻塞（仅 fork 瞬间有短暂停顿，数据量大时需注意）。\n\n对比\n\n\n特性\nRDB\nAOF\n\n\n\n持久化方式\n快照（全量）\n日志（增量）\n\n\n数据安全性\n较低（可能丢数分钟数据）\n高（最多丢 1 秒）\n\n\n恢复速度\n⚡ 极快\n较慢（需重放命令）\n\n\n文件大小\n小\n通常较大（即使重写后）\n\n\n对性能影响\nfork 时有短暂延迟\nfsync 可能造成 I&#x2F;O 压力\n\n\n适用场景\n备份、主从同步、快速恢复\n高可靠性要求、金融类应用\n\n\n\n🔁 最佳实践：同时开启 RDB + AOF\n\n用 RDB 做定期备份和快速启动；\n用 AOF 保证最小数据丢失。\n\n\n","categories":["redis"]},{"title":"数据结构","url":"/2026/02/28/redis-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"常用数据类型1. String（字符串）\n\n描述：最基本的数据类型，可以存储字符串、整数或浮点数。\n最大容量：512 MB。\n常用命令：\nSET key value\nGET key\n\n\n应用场景：\n缓存简单对象（如用户信息）。\n计数器（如文章阅读量）。\n分布式锁（配合 SETNX）。\n\n\n\n2. Hash（哈希）\n\n描述：一个键值对集合，适合存储对象。\n结构：key → { field1: value1, field2: value2, … }\n常用命令：\nHSET key field value\nHGET key field\n\n\n应用场景：\n存储用户资料（如 ID、姓名、邮箱等字段）。\n避免序列化整个对象，可单独更新某个字段。\n\n\n\n3. List（列表）\n\n描述：有序、可重复的字符串列表，底层使用双向链表实现。\n常用命令：\nLPUSH key value / RPUSH key value\nLPOP key / RPOP key\nLRANGE key start stop\n\n\n应用场景：\n消息队列（简易版）。\n最新动态&#x2F;评论列表（如微博时间线）。\n栈或队列结构。\n\n\n\n4. Set（集合）\n\n描述：无序、不重复的字符串集合，基于哈希表实现。\n常用命令：\nSADD key member\nSMEMBERS key\nSISMEMBER key member\n\n\n\n5. Sorted Set（有序集合，ZSet）\n\n描述：类似 Set，但每个成员关联一个分数（score），用于排序。\n特点：元素唯一，按 score 升序排列。同时含有哈希表和跳表\n常用命令：\nZADD key score member\nZREM key member\nZRANK key member\n\n\n\n数据结构SDS1. O(1) 获取字符串长度\n\n直接读取 len 字段，无需遍历。\n\n2. 杜绝缓冲区溢出\n\n所有修改操作（如 sdscat）会先检查空间，不足则自动扩容。\n\n3. 二进制安全\n\n字符串内容可以包含任意字节（包括 \\0），因为长度由 len 决定，而非 \\0。因此 Redis 可以安全地存储图片、序列化对象等二进制数据。\n\n4. 空间预分配（减少 realloc 次数）\n\n增长策略：若新长度 &lt; 1MB：分配 2 × 新长度 的空间（即预留同样大小的空闲空间）。若 ≥ 1MB：额外分配 1MB 空闲空间。\n\n5. 惰性空间释放\n\n缩短字符串时，不立即释放内存，而是增加 free 值。后续增长可复用这些空间。\n\n哈希表基本结构typedef struct dictEntry &#123;    void *key;    union &#123;        void *val;        uint64_t u64;        int64_t s64;        double d;    &#125; v;    struct dictEntry *next;  // 链地址法解决冲突&#125; dictEntry;\n\n使用 链地址法（拉链法） 处理哈希冲突。\nvalue 是一个联合体，可高效存储指针、整数或浮点数。\n\ntypedef struct dictht &#123;    dictEntry **table;     // 哈希数组，每个元素是指向 dictEntry 链表的指针    unsigned long size;    // 哈希数组大小（总是 2^n）    unsigned long sizemask; // size - 1，用于计算索引（h &amp; sizemask）    unsigned long used;    // 当前已有键值对数量&#125; dictht;\n\n渐进式 Rehash（核心优化！）Redis 不一次性 rehash 所有键（会阻塞主线程），而是分多次完成：\n\n分配新哈希表 ht[1]，大小为第一个 ≥ used * 2 的 2^n。\n设置 rehashidx &#x3D; 0，表示开始 rehash。\n后续操作（增删改查）同时操作两个表：\n先在 ht[0] 查找&#x2F;删除，若找不到再查 ht[1]。\n新增一律写入 ht[1]。\n每次操作后，顺带迁移 ht[0] 中一个 bucket 到 ht[1]。\n\n\n同时，定时任务（serverCron）也会主动迁移部分 bucket。\n当 ht[0] 所有键都迁完，释放 ht[0]，将 ht[1] 设为 ht[0]，rehashidx &#x3D; -1。\n\n\n✅ 优点：避免长时间阻塞，保证 Redis 单线程高响应性。\n\n跳表基本原理\n最底层（Level 0）是包含所有元素的有序链表。\n每一层都是下一层的“快速通道”，跳过部分节点。\n通过随机层数维持结构平衡（类似“概率平衡”）。\n使用幂律分布生成节点层数，确保高层节点稀疏，最多 32 层，足够支持 2^32 个元素\n\nLevel 3:  Head ----------------&gt; D ------&gt; NILLevel 2:  Head ----------------&gt; D ------&gt; NILLevel 1:  Head ------&gt; B ------&gt; D ------&gt; NILLevel 0:  Head -&gt; A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; NIL                  A &lt;- B &lt;- C &lt;- D &lt;- E\n\n实现细节typedef struct zskiplistNode &#123;    sds ele;                    // 成员对象（字符串，即 ZSet 的 member）    double score;               // 分值（用于排序）    struct zskiplistNode *backward; // 后退指针（仅指向前一个节点，用于反向遍历）    struct zskiplistLevel &#123;        struct zskiplistNode *forward; // 前进指针        unsigned long span;            // 到 forward 节点跨越的节点数（用于 rank 计算）    &#125; level[];                  // 柔性数组，表示各层（实际层数在创建时确定）&#125; zskiplistNode;\ntypedef struct zskiplist &#123;    struct zskiplistNode *header, *tail; // 头尾指针    unsigned long length;                // 节点总数    int level;                           // 当前最大层数（不包括 header）&#125; zskiplist;\n\n对比\n\n\n特性\n跳表\n红黑树\nB+树\n\n\n\n实现复杂度\n低\n高\n中高\n\n\n范围查询\n优秀（链表遍历）\n一般（需中序遍历）\n优秀\n\n\n内存开销\n较高（多层指针）\n低\n中\n\n\n并发修改\n容易（局部锁）\n困难\n中等\n\n\nRedis 适用性\n✅ 最佳\n❌\n❌（适合磁盘）\n\n\n\n✅ 所有操作平均时间复杂度均为 O(log N)，最坏 O(N)（极低概率）。\n\n","categories":["redis"]},{"title":"高可用","url":"/2026/02/28/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/","content":"主从复制优势\n\n\n作用\n说明\n\n\n\n数据冗余\n主节点数据自动备份到从节点，防止单点故障导致数据丢失\n\n\n读写分离\n主节点处理写请求，从节点处理读请求，提升系统吞吐量\n\n\n故障恢复\n主节点宕机后，可手动或通过 Sentinel 自动将从节点提升为主节点\n\n\n负载均衡\n多个从节点可分担大量读请求压力\n\n\n高可用基础\n是 Redis Sentinel 和 Redis Cluster 架构的前提\n\n\n\n主节点可以采用树形拓扑（级联复制 Master → Slave1 → Slave2），减少主节点出口带宽压力\n\n基本原理1. 建立连接\n\n从节点通过 REPLICAOF   命令向主节点发起长连接。\n主从建立 TCP 连接后，从节点会发送 PSYNC 命令（Partial Sync），尝试进行增量同步。\n\n2. 数据同步根据是否首次连接或断线时间长短，分为：全量同步、增量同步\n3. 命令传播（实时同步）\n\n主节点每执行一条写命令（如 SET, DEL），就将该命令以 RESP 协议 格式转发给所有从节点。\n从节点接收到命令后立即执行，保持数据一致。\n\n全量同步（Full Sync）触发条件\n\n从节点首次连接主节点\n从节点断线太久，无法进行增量同步\n主节点 runid 发生变化（如主节点重启）\n\n流程详解\n\n从节点发送 PSYNC ? -1（表示未知 runid 和偏移量）\n主节点回复 FULLRESYNC  \n主节点 fork 子进程生成 RDB 快照，同时，主节点将新写入的命令缓存在复制缓冲区\nRDB 文件生成后，主节点将其发送给从节点\n从节点清空自身数据，加载 RDB 文件\n主节点将复制缓冲区中的增量命令发送给从节点\n从节点执行这些命令，完成全量同步\n\n\n⚠️ 注意：在 Redis 8.0 之前，RDB 和增量命令都通过同一个连接传输；Redis 8.0 引入 RDB Channel，使用两条独立通道分别传输 RDB 和命令，提升效率并降低主节点压力。\n\n增量同步（Partial Sync &#x2F; PSYNC）全量同步代价高（需传输整个 RDB）。为优化短暂断连后的重连，Redis 2.8 引入 PSYNC 机制。\n核心组件\n\nrunid：主节点的唯一标识（每次启动生成）\nreplication offset：复制偏移量，表示已同步的数据位置\nrepl_backlog_buffer（复制积压缓冲区）：\n主节点维护的一个环形缓冲区\n持续记录最近的写命令（即使从节点离线）\n默认大小 repl-backlog-size &#x3D; 1MB（可配置）\n\n\n\n增量同步流程\n\n从节点断线后重连，发送 PSYNC  \n主节点检查：\nold-runid 是否等于当前 runid？\nold-offset 是否仍在 repl_backlog_buffer 范围内？\n\n\n若满足，则主节点从 backlog 中提取缺失命令，发送给从节点 → 增量同步成功\n否则，退化为全量同步\n\n\n从节点将 runid 和 offset 持久化到 RDB 文件中，重启后仍可增量同步主从切换时，新主节点保留旧主的 master_replid2，支持旧从节点增量同步\n\n哨兵功能\n哨兵节点为独立进程，通常为奇数个（如 3、5），避免脑裂\n应用客户端连接哨兵获取主节点信息，而非直接连接主节点\n\n\n\n\n功能\n说明\n\n\n\n监控\n哨兵定期向主节点、从节点及其他哨兵发送 PING，检测其是否存活\n\n\n通知\n当被监控的 Redis 实例状态异常时，可通过 API 或脚本通知管理员\n\n\n自动故障转移\n主节点宕机后，自动选举新主节点，并重配从节点和客户端\n\n\n配置中心\n客户端通过哨兵查询当前主节点地址，无需硬编码\n\n\n工作流程步骤 1：持续监控\n每个哨兵每秒向所有 Redis 节点（主、从）和其他哨兵发送 PING\n检查响应是否超时\n\n步骤 2：主观下线（Subjective Down, SDOWN）某个哨兵自身认为主节点不可达\n\n仅代表单个哨兵的判断\n可能是网络抖动或哨兵自身问题导致的误判\n不会触发故障转移\n\n步骤 3：客观下线（Objective Down, ODOWN）当认为主节点下线的哨兵数量 ≥ quorum（法定人数）\n\n发现 SDOWN 的哨兵会向其他哨兵发送 is-master-down-by-addr 请求\n统计同意“主节点已下线”的哨兵数量\n若 ≥ quorum，则标记为 ODOWN\n\n\n⚠️ quorum 不是“总哨兵数的一半+1”，而是判定 ODOWN 所需的最小同意数。但选举 Leader 仍需多数派（&gt; N&#x2F;2）。\n\n步骤 4：选举 Leader 哨兵基于 Raft 算法 的简化版, 选出一个哨兵来执行故障转移（避免多个哨兵同时操作）\n\n每个哨兵只能投一票\n获得 超过半数 哨兵投票的节点成为 Leader\n先发起选举者优先（先到先得）\n✅ 例如：3 个哨兵，需至少 2 票；5 个哨兵，需至少 3 票。\n\n\n\n步骤 5：执行故障转移（Failover）Leader 哨兵执行以下操作：\n\n选择新主节点：从所有从节点中按优先级筛选：健康 &gt; 优先级 &gt; 复制偏移量 &gt; Run ID 字典序\n提升新主节点：向选中的从节点发送 REPLICAOF NO ONE，使其成为主节点\n重配其他从节点： 向其余从节点发送 REPLICAOF  ，使其同步新主\n更新旧主节点（若恢复）：当原主节点恢复上线，哨兵会自动将其降级为从节点，同步新主数据\n发布事件通知：向所有哨兵和客户端发布 +switch-master 事件，客户端可通过订阅该频道或主动查询哨兵获取新主地址\n\n集群特点单机redis瓶颈\n\n\n\n限制\n说明\n\n\n\n内存容量\n受限于单机物理内存（如 64GB）\n\n\n吞吐能力\n所有读写请求由单线程处理（Redis 6&#x2F;7 引入多线程 I&#x2F;O，但命令执行仍单线程）\n\n\n高可用依赖哨兵\n哨兵只解决故障转移，不解决容量和写扩展问题\n\n\n集群优势\n\n\n\n特性\n说明\n\n\n\n✅ 数据分片（Sharding）\n将 key 空间划分为 16384 个槽（slot），分散到多个主节点\n\n\n✅ 去中心化\n无中心节点，所有节点对等通信（Gossip 协议）\n\n\n✅ 高可用\n每个主节点可配置一个或多个从节点，主挂后自动切换\n\n\n✅ 客户端直连\n客户端直接连接任一节点，通过重定向（MOVED&#x2F;ASK）找到正确节点\n\n\n✅ 在线扩容&#x2F;缩容\n支持动态添加&#x2F;移除节点，自动迁移 slot\n\n\n❌ 不支持多 key 事务跨 slot\nMULTI/EXEC、MSET、SUNION 等要求所有 key 在同一 slot\n\n\n哈希槽（Hash Slot）\n固定 16384 个槽（0 ～ 16383）\n\n足够大：支持最多 1000+ 节点（官方建议不超过 1000）\n足够小：心跳包中携带的槽信息不会过大（每个槽用 2 字节表示，16384 × 2 &#x3D; 32KB）\n\n\n使用 CRC16 算法计算 key 的 hash 值，再对 16384 取模：\n\n\nslot = CRC16(key) % 16384\n\n集群架构\n主节点（Master）：负责一部分 slot 的读写，存储数据\n从节点（Replica&#x2F;Slave）：复制主节点数据，不服务读写（默认），仅用于故障转移\n📌 生产环境建议：3 主 3 从（6 节点）起步。\n\n\n\n故障转移Redis Cluster 使用 Gossip 协议 实现节点间信息交换，无需中心协调。\n\n主观下线（PFAIL）：某节点认为另一个主节点不可达\n客观下线（FAIL）：通过 Gossip 传播，当集群中多数主节点都认为该节点下线，则标记为 FAIL\n从节点发起选举：该主节点的从节点尝试升级为主\nRaft-like 投票：其他主节点投票，获得多数票者胜出\n更新集群状态：新主接管 slot，广播新配置\n⚠️ 注意：只有主节点参与投票，从节点不参与决策。\n\n\n\n对比\n\n\n维度\nRedis Cluster\nRedis Sentinel\n\n\n\n目标\n分布式存储 + 高可用\n高可用（单主架构）\n\n\n数据模型\n分片（Sharding）\n全量复制\n\n\n写扩展\n✅ 支持\n❌ 不支持\n\n\n容量扩展\n✅ 支持\n❌ 不支持\n\n\n多 key 操作\n有限制（需同 slot）\n无限制\n\n\n运维复杂度\n较高\n较低\n\n\n适用场景\n大数据量、高并发写\n中小规模、强一致性\n\n\n","categories":["redis"]},{"title":"mode","url":"/2026/02/28/system-mode/","content":"mode基本概念用户态（User Mode）\n\n用户程序运行在用户态。\n权限较低：不能直接访问硬件资源（如内存、磁盘、网卡等），也不能执行某些敏感的CPU指令（如修改页表、关闭中断等）。\n目的是防止应用程序误操作或恶意破坏系统。\n\n内核态（Kernel Mode）\n\n操作系统内核运行在内核态。\n权限最高：可以执行任何CPU指令，访问所有内存地址和硬件设备。\n负责管理硬件资源、调度进程、处理中断、提供系统调用接口等核心功能。\n\n优点\n安全性：如果所有程序都能随意访问硬件或修改内核数据结构，一个出错或恶意的程序就可能崩溃整个系统。\n稳定性：将不可靠的用户程序隔离在受限环境中，即使程序崩溃也不会影响内核和其他进程。\n抽象与统一接口：内核通过系统调用向用户提供统一、安全的硬件访问接口，隐藏底层复杂性。\n\n对比\n\n\n特性\n用户态\n内核态\n\n\n\n权限级别\n低（如 x86 的 Ring 3）\n高（如 x86 的 Ring 0）\n\n\n可执行指令\n普通指令\n所有指令（包括特权指令）\n\n\n内存访问\n受限（只能访问自己的虚拟地址空间）\n可访问全部物理&#x2F;虚拟内存\n\n\n硬件访问\n不可直接访问\n可直接控制硬件\n\n\n出错影响\n通常只影响当前进程\n可能导致系统崩溃（Kernel Panic）\n\n\n运行代码\n应用程序\n操作系统内核\n\n\n虚拟地址基本概念虚拟地址（Virtual Address, VA）\n\n进程看到的地址空间（如 0x400000 ～ 0x7fffffffffff 在 x86-64 上）。\n对进程透明，由编译器&#x2F;链接器生成。\n\n物理地址（Physical Address, PA）\n\n实际 RAM 芯片上的地址。\n只有内核和硬件能直接操作。\n\n地址转换\n\nCPU 生成虚拟地址 → MMU（Memory Management Unit） 查页表 → 转换为物理地址。\n若映射不存在或权限错误 → 触发缺页异常（Page Fault），由内核处理。\n\n优点与代价✅ 优点\n\n安全隔离：进程互不干扰\n简化编程：程序员无需管理物理地址\n高效共享：代码段、共享库只需一份物理内存\n支持大程序：即使物理内存小，也能运行大型应用（靠 swap）\n内存保护：防止缓冲区溢出等攻击（配合 NX bit）\n\n⚠️ 代价\n\n地址转换开销：虽有 TLB 缓解，但仍存在\n内存碎片：虚拟地址连续 ≠ 物理连续\n复杂性：页表管理、缺页处理、TLB 刷新等增加内核复杂度\n性能陷阱：频繁缺页、TLB miss、swap I&#x2F;O 会显著降低性能\n\n关键功能实现缺页异常（Page Fault）处理\n当访问未映射或未加载的虚拟页时\nCPU 触发 page fault 异常（向量 14）\n内核 do_page_fault() 分析原因：\n合法访问但未加载 → 分配物理页，从磁盘&#x2F;文件&#x2F;零页加载内容（按需分页）\n写时复制（COW） → 父子进程 fork 后共享页，首次写触发 COW\n非法访问（如空指针） → 发送 SIGSEGV\n\n\n建立页表映射，恢复执行\n\n内存映射（mmap）\n将文件或设备映射到进程地址空间（如共享库 .so 文件）。\n访问映射区域 → 触发缺页 → 从文件读取内容到内存。\n支持匿名映射（无文件 backing，用于堆、栈）。\n\n交换（Swapping）\n当物理内存不足时，将不活跃的页写入swap 分区&#x2F;文件。\n下次访问时再换入（Swap-in）。\nLinux 使用LRU（最近最少使用） 等算法选择换出页。\n\n写时复制（Copy-on-Write, COW）\nfork() 时子进程不复制父进程内存，而是共享页，并标记为只读。\n任一方尝试写 → 触发 page fault → 内核分配新页并复制内容。\n极大优化了进程创建性能。\n\n","categories":["system"]},{"title":"网络","url":"/2026/02/28/system-%E7%BD%91%E7%BB%9C/","content":"零拷贝一种优化数据传输性能的技术，其核心思想是：在数据从磁盘或网络传送到应用程序（或反之）的过程中，尽可能减少甚至避免 CPU 对数据的复制操作和上下文切换，从而降低 CPU 开销、内存带宽消耗，并提升 I&#x2F;O 吞吐量。\n传统拷贝假设一个典型场景：Web 服务器读取一个文件并通过网络发送给客户端。\n\nread() 系统调用：\n用户进程调用 read(fd, buffer, size)；\nCPU 从用户态切换到内核态；\n内核通过 DMA（直接内存访问）从磁盘读取数据到内核缓冲区（Page Cache）；\n内核再将数据从内核缓冲区复制到用户空间缓冲区；\nCPU 切回用户态。\n\n\nsend() 或 write() 系统调用：\n用户进程调用 send(socket, buffer, size)；\nCPU 再次陷入内核态；\n内核将用户缓冲区的数据复制到 socket 的内核发送缓冲区；\n网卡通过 DMA 将数据从内核缓冲区发送到网络；\nCPU 切回用户态。\n\n\n\n✅ 问题总结：\n\n4 次上下文切换（用户 ↔ 内核 ×2）；\n2 次不必要的 CPU 数据拷贝（内核 → 用户 → 内核）；\nCPU 被大量占用在“搬运数据”上，而非真正处理业务。\n\n典型实现：sendfile()\n应用程序调用 sendfile(file_fd, socket_fd, …)；\nCPU 进入内核态一次；\n内核通过 DMA 从磁盘读取数据到 Page Cache（若未缓存）；\n内核直接将 Page Cache 中的数据传递给 socket 缓冲区（现代 Linux 使用 “zero-copy sendfile” + TCP send buffer 引用机制）；\n网卡通过 DMA 将数据发送到网络；\nCPU 返回用户态。\n\n✅ 优势：\n\n2 次上下文切换（减少一半）；\n0 次 CPU 数据拷贝（仅 DMA 操作）；\n极大提升吞吐量，降低延迟和 CPU 占用。\n\n的零拷贝\n打开文件（获取 fd）；\n获取 TCP 连接的原始 fd；\n调用 unix.Sendfile(out_fd, in_fd, offset, count)。\n\n\nI&#x2F;O 多路复用高性能网络编程的核心技术之一，它允许单个线程同时监控多个文件描述符（FD），在其中任意一个 FD 就绪（可读、可写或异常）时通知程序进行处理。这种机制避免了为每个连接创建线程&#x2F;进程的开销，是构建高并发服务器（如 Nginx、Redis）的基础。\nselect基本原理\n\n使用位图（bitmask）表示文件描述符集合；\n内核遍历所有被监控的 FD，检查是否就绪；\n调用返回后，用户需遍历 FD 集合判断哪些就绪。\n\n缺点\n\nFD 数量限制：默认最多 1024（FD_SETSIZE），可通过编译调整但不灵活\n每次调用需重传全部 FD 集合：用户态 ↔ 内核态拷贝开销大\n内核线性扫描所有 FD：时间复杂度 O(n)，n 为最大 FD 值（非活跃连接也扫描）\n不可重用：每次调用后 fd_set 被修改，需重新初始化\n\npoll(select 的改进版)基本原理\n\n使用 数组（struct pollfd）代替位图；\n不再有 FD 数量硬限制（仅受系统资源限制）；\n仍需每次传递整个数组，内核仍线性扫描。\n\n缺点\n\n仍需每次传递整个 fds 数组（用户态 ↔ 内核拷贝）；\n内核仍需 O(n) 扫描所有 FD；\n大量空闲连接时性能差。\n\nepoll核心思想\n将“监控 FD 集合”与“等待事件”分离；\n内核维护一个红黑树 + 就绪链表；\n只返回就绪的 FD，无需遍历全部。\n\n工作流程\n调用 epoll_create() 创建 epoll 实例；\n对每个要监控的 socket，调用 epoll_ctl(EPOLL_CTL_ADD, …) 注册；\n循环调用 epoll_wait() 获取就绪事件；\n直接处理 events 数组中的 FD（无需遍历）；\n连接关闭时，调用 epoll_ctl(EPOLL_CTL_DEL, …) 移除。\n\n工作模式LT（Level Triggered，默认）\n\n只要 FD 处于就绪状态，每次 epoll_wait 都会返回（可以后续再处理）；\n类似 select&#x2F;poll，编程简单，不易遗漏事件。\n\nET（Edge Triggered）\n\n仅在 FD 状态发生变化时通知一次；\n必须一次性读完所有数据（非阻塞 I&#x2F;O + 循环 read），否则会丢失事件；\n性能更高，减少 epoll_wait 调用次数。\n\nReactor一种事件驱动的并发编程模型，通过一个或多个线程监听多个 I&#x2F;O 事件，当事件就绪时，分发给对应的处理器进行同步处理，从而避免为每个连接创建线程，实现高并发、低资源消耗。\n核心思想\n使用 I&#x2F;O 多路复用（如 epoll、kqueue、select）监听多个文件描述符（FD）；\n当某个 FD 上发生就绪事件（如可读、可写），Reactor 将事件分发给预注册的事件处理器（Handler）；\nHandler 同步执行 I&#x2F;O 操作（如 read()&#x2F;write()），因为已知 FD 就绪，不会阻塞。\n\n组件\n\n\n组件\n职责\n说明\n\n\n\nReactor\n事件分发器\n核心调度器，负责监听事件并分发给 Handler\n\n\nDemultiplexer\n多路分解器\n底层 I&#x2F;O 多路复用机制（如 epoll_wait）\n\n\nEvent Handler\n事件处理器接口\n定义处理事件的方法（如 handle_event()）\n\n\nConcrete Event Handler\n具体处理器\n实现业务逻辑（如读请求、写响应）\n\n\nAcceptor\n连接接收器\n特殊的 Handler，用于 accept 新连接\n\n\n\n📌 在代码中，Reactor 通常是一个循环（event loop），不断调用 epoll_wait() 等待事件。\n\n基本工作流程\n初始化：Reactor 创建 epoll 实例，注册 listen socket 的可读事件；\n等待事件：Reactor 调用 epoll_wait() 阻塞等待；\n新连接到达：\nlisten socket 可读；\nReactor 调用 Acceptor 的 handle_event()；\nAcceptor 调用 accept() 获取新连接；\n将新 socket 注册到 Reactor，关联一个 ConnectionHandler；\n\n\n数据到达：\n客户端发送数据 → socket 可读；\nReactor 调用对应 ConnectionHandler 的 handle_event()；\nHandler 同步调用 recv() 读取数据（不阻塞）；\n处理业务逻辑，可能调用 send() 发送响应；\n\n\n循环继续。\n\n常见变体单 Reactor 单线程\n\n所有操作（accept、read、业务处理、write）都在一个线程完成；\n✅ 简单、无锁；\n❌ 业务逻辑不能耗时（否则阻塞整个事件循环）；\n\n单 Reactor 多线程（Worker Pool）\n\nReactor 线程只负责 I&#x2F;O 事件分发；\n业务处理交给线程池；\n✅ 解耦 I&#x2F;O 与计算；\n❌ 线程切换开销，需注意线程安全；\n\n多 Reactor 多线程（主从 Reactor）\n\nMain Reactor：仅处理 accept，将新连接分发给 Sub Reactor；\nSub Reactor：每个运行在独立线程，管理一组连接的 I&#x2F;O；\n✅ 充分利用多核 CPU；\n✅ 无锁（每个 Sub Reactor 独占连接）；\n典型应用：Nginx、Netty。\n\n","categories":["system"]},{"title":"进程","url":"/2026/02/28/system-%E8%BF%9B%E7%A8%8B/","content":"进程操作系统中的进程（Process）是程序的一次执行实例，是系统进行资源分配和调度的基本单位。\n定义\n进程 &#x3D; 程序代码 + 执行上下文（如寄存器状态、程序计数器、堆栈、打开的文件等）\n每个进程拥有独立的地址空间（在支持虚拟内存的系统中）\n进程是动态的，具有生命周期：创建 → 就绪 → 运行 → 阻塞&#x2F;等待 → 终止\n\n调度调度类型\n\n长期调度（作业调度）：决定哪些进程从外存进入内存（控制并发度）\n中期调度（交换调度）：将进程换出&#x2F;换入内存（用于虚拟内存管理）\n短期调度（CPU 调度）：从就绪队列中选择一个进程分配 CPU（最频繁）\n\n常见调度算法\n\n\n\n算法\n特点\n优缺点\n\n\n\n先来先服务（FCFS）\n按到达顺序执行\n简单；但平均等待时间长，有“护航效应”\n\n\n短作业优先（SJF）\n优先执行预计运行时间短的进程\n最优平均等待时间；但难以预测，可能饥饿\n\n\n优先级调度\n每个进程有优先级，高者先执行\n灵活；但低优先级进程可能饿死\n\n\n时间片轮转（RR）\n每个进程轮流执行固定时间片\n公平、响应快；时间片太小导致上下文切换开销大\n\n\n多级反馈队列（MLFQ）\n多个优先级队列，动态调整优先级\n兼顾响应与吞吐；实现复杂（如 Windows、Unix 使用类似机制）\n\n\n调度目标\n\n公平性：每个进程合理获得 CPU，避免饥饿\n高效性：CPU 利用率高\n响应时间短（交互式系统）\n吞吐量大（批处理系统）\n\n进程间通信（IPC, Inter-Process Communication）\n\n\nIPC 方法\n速度\n数据量\n跨进程关系\n跨机器\n同步&#x2F;异步\n复杂度\n典型用途\n\n\n\n管道（Pipe）\n中等\n小～中\n仅父子&#x2F;亲缘\n❌\n阻塞式（同步）\n低\nShell 命令链、简单父子通信\n\n\n命名管道（FIFO）\n中等\n小～中\n任意本地进程\n❌\n阻塞式\n低\n本地无亲缘进程通信\n\n\n消息队列\n中\n中\n任意\n❌（除非自建网络层）\n可配置\n中\n解耦生产者-消费者、可靠消息传递\n\n\n共享内存\n⭐最快\n大\n任意\n❌\n需配合同步机制\n高\n高性能数据交换（如图像、实时数据）\n\n\n信号（Signal）\n快\n极小（仅通知）\n任意\n❌\n异步\n低\n进程中断、终止通知\n\n\n套接字（Socket）\n较慢\n任意\n任意\n✅\n可同步&#x2F;异步\n中～高\n网络通信、本地高性能服务（Unix Domain Socket）\n\n\n\n通用首选：Unix Domain Socket（平衡性能、灵活性、易用性）\n极致性能：共享内存（但需谨慎处理同步）\n快速原型：管道或消息队列\n\n线程和协程定义\n线程（Thread）： 操作系统内核调度的最小执行单元，由 OS 管理，支持抢占式多任务。\n协程（Coroutine）：用户态的轻量级执行单元，由程序或运行时调度，采用协作式（非抢占）切换。\n协程是用户级抽象，线程是内核级实体。\n\n\n\n调度方式\n\n\n特性\n线程\n协程\n\n\n\n调度者\n操作系统内核\n用户程序 &#x2F; 运行时（如 Go runtime、JVM）\n\n\n切换方式\n抢占式（时间片用完或被更高优先级中断）\n协作式（主动 yield 或在 I&#x2F;O 点挂起）\n\n\n上下文切换开销\n高（需陷入内核、刷新 TLB、缓存污染）\n极低（仅保存寄存器和栈指针）\n\n\n并行能力\n可真正并行（多核 CPU）\n单个协程不并行，但多个协程可映射到多线程实现并行\n\n\n资源与性能\n\n\n维度\n线程\n协程\n\n\n\n内存占用\n每个线程默认栈 1–8 MB（Linux&#x2F;Windows）\n栈初始仅 2–8 KB，按需增长（Go）或固定小栈（Java Virtual Thread）\n\n\n创建速度\n慢（系统调用 + 内存分配）\n快（纯用户态操作）\n\n\n最大数量\n通常几千（受限于虚拟内存）\n可达百万级\n\n\n适用场景\nCPU 密集型、需要真并行\nI&#x2F;O 密集型、高并发请求（如 Web 服务）\n\n\nJava 协程 vs Go 协程\n\n\n特性\nGo 协程（Goroutine）\nJava 协程（Virtual Thread）\n\n\n\n调度器\nGo Runtime 自带 M:N 调度器（M 个 OS 线程调度 N 个 Goroutine）\nJVM 调度 Virtual Thread 到 Carrier Thread（OS 线程）上\n\n\n切换点\n函数调用、channel 操作、系统调用等自动挂起\n在 blocking I&#x2F;O 或 Thread.yield() 时挂起（需 JVM 支持）\n\n\n栈管理\n分段栈（早期）→ 连续栈（当前），动态伸缩\n固定小栈（～1KB），但可扩展；比 Platform Thread（1MB）小得多\n\n\nAPI 风格\n显式 go func() 启动；channel 通信\n仍用 Thread.start()，但用 Thread.ofVirtual().start()；兼容现有线程 API\n\n\n阻塞处理\nGo runtime 将阻塞 syscall 转为异步（如网络 I&#x2F;O）\nJVM 重写 JDK 阻塞方法（如 Socket.read()）以支持挂起\n\n\n错误处理\npanic 在 Goroutine 中传播，需 recover\n异常在线程中正常抛出，与普通线程一致\n\n\n\n功能上大同小异\n\n","categories":["system"]},{"title":"分布式","url":"/2026/02/28/%E5%9C%BA%E6%99%AF-%E5%88%86%E5%B8%83%E5%BC%8F/","content":"CAP 理论C – Consistency（一致性）\n所有节点在同一时间看到的数据是完全相同的，任何读操作都能读到最近一次成功写入的值。\n示例：用户 A 写入数据后，用户 B 立即读取，必须得到最新值。\n\nA – Availability（可用性）\n每个请求（读或写）都能在有限时间内收到非错误的响应（不保证是最新的数据）。\n系统即使部分节点故障，仍能继续提供服务。\n示例：即使某个数据库节点宕机，用户仍能访问网站并获得响应（可能是旧数据）。\n\nP – Partition Tolerance（分区容错性）\n系统在网络发生分区（网络断开、延迟、丢包等）的情况下，仍能继续运行。\n分区是指节点之间无法通信，但各自仍可独立处理请求。\n在现实的分布式系统中，网络分区是不可避免的（如机房断网、跨地域延迟），因此 P 是必须接受的前提。\n\n为什么不能三者兼得？假设系统发生网络分区（P 必须满足），此时系统被分成两部分：Group1 和 Group2。\n\n如果选择 CP（一致 + 分区容错）：\n为保证一致性，系统会在分区期间拒绝部分请求（比如只允许主分区写入，其他分区只读或拒绝服务）。\n结果：牺牲可用性（A）。\n典型系统：ZooKeeper、etcd、HBase。\n\n\n如果选择 AP（可用 + 分区容错）：\n系统在分区时仍接受读写请求，但不同分区可能返回不同版本的数据。\n结果：牺牲强一致性（C），通常采用最终一致性。\n典型系统：Cassandra、DynamoDB、Eureka。\n\n\nCA（一致 + 可用）：\n只能在无网络分区的理想环境下成立（如单机系统或局域网内高可靠系统）。\n一旦发生网络故障，系统就无法同时保证 C 和 A。\n因此，在真正的分布式场景中，CA 实际上不可行。\n\n\n\n\n✅ 结论：P 是必须的 → 实际系统只能在 CP 和 AP 之间权衡。\n\n分布式事务基于消息队列的最终一致性（可靠消息最终一致性）（简单）\n原理：通过消息中间件（如 Kafka、RocketMQ、RabbitMQ）保证本地事务与消息发送的一致性。\n典型流程：\n服务 A 执行本地事务，并将“待发送消息”写入本地消息表（与业务数据同库）。\n后台任务轮询本地消息表，将消息投递到 MQ。\n服务 B 消费消息，执行本地事务。\n若失败，MQ 支持重试 + 死信队列处理。\n\n\n优点：解耦、高可用、性能好。\n缺点：只能保证最终一致性；需处理消息重复消费（幂等性）。\n适用场景：订单创建后发通知、积分增加、库存扣减等异步操作。\n\n\n示例：电商下单 → 扣库存 → 发优惠券 → 发短信，各步骤通过 MQ 异步解耦。\n\n本地消息表（Local Message Table）（简单）\n原理：在业务数据库中建一张消息表，与业务操作在同一个本地事务中写入。\n流程：\n开启事务，更新业务表 + 插入消息表。\n提交事务。\n异步任务读取消息表，发送到 MQ 或直接调用下游服务。\n\n\n优点：简单、可靠，不依赖外部 MQ 的事务消息功能。\n缺点：侵入业务库；需维护消息表和重试机制。\n适用场景：中小型企业系统，或 MQ 不支持事务消息时。\n\nTCC（Try-Confirm-Cancel）（复杂）\n原理：业务层面实现三阶段补偿：\nTry：预留资源（如冻结库存、预扣余额）。\nConfirm：确认提交（真正扣减）。\nCancel：取消操作（释放预留资源）。\n\n\n要求：\n所有参与方必须实现 Try&#x2F;Confirm&#x2F;Cancel 接口。\nConfirm&#x2F;Cancel 必须幂等。\n\n\n优点：性能优于 2PC，可自定义业务逻辑。\n缺点：开发复杂度高，需处理悬挂、空回滚等问题。\n适用场景：金融转账、高并发资金交易等对一致性要求较高的场景。\n\n\n示例：支付系统中，Try 阶段冻结用户账户金额，Confirm 阶段真正划账。\n\nSeata 等开源框架（AT 模式）（复杂）\n自动记录业务 SQL 的前后镜像（undo log）。\n全局事务协调器（TC）管理分支事务。\n提交时异步删除 undo log；回滚时用 undo log 补偿。\n优点：对业务代码侵入小（只需加注解）。\n缺点：依赖数据库锁，高并发下性能受限；仅支持部分数据库。\n适用场景：微服务架构中需要快速落地分布式事务的场景。\n\n分布式锁基于 Redis 的分布式锁\n利用 Redis 的 单线程 + 原子操作 特性，通过 SET key value NX PX 命令实现。\n\nSET lock_key client_id NX PX 30000\n高级问题与解决方案锁过期时间如何设置？\n\n太短：业务未执行完锁就释放 → 多个客户端同时进入临界区。\n太长：故障时长时间阻塞其他客户端。\n\n✅ 解决方案：看门狗（Watchdog）机制\n\n加锁成功后启动后台线程，定期（如每 10 秒）延长锁的过期时间。\n业务执行完毕后主动释放锁，停止看门狗。\n\n可重入锁实现\n\n在锁的 value 中记录 加锁次数 和 client_id。\n同一 client 多次加锁时，递增计数；释放时递减，归零才真正删除 key。\n\n","categories":["场景"]},{"title":"分析指令","url":"/2026/02/28/%E5%9C%BA%E6%99%AF-%E5%88%86%E6%9E%90%E6%8C%87%E4%BB%A4/","content":"top系统统计信息区 (前 5 行)第 1 行：系统运行时间与负载 (top)top - 14:03:25 up 10 days,  2:30,  3 users,  load average: 0.05, 0.10, 0.05\n\n14:03:25: 当前系统时间。\nup 10 days, 2:30: 系统已运行的时间（ uptime）。\n3 users: 当前登录的用户数量。\nload average: 0.05, 0.10, 0.05: 系统负载平均值。\n分别代表 1分钟、5分钟、15分钟 内的平均负载。\n含义: 负载是指处于“可运行状态”（正在使用 CPU 或等待 CPU）和“不可中断睡眠状态”（通常是在等待磁盘 I&#x2F;O）的进程数平均值。\n判断标准: 如果数值超过 CPU 核心数，说明系统过载。例如，4 核 CPU，负载超过 4.0 就意味着有进程在排队等待。\n\n\n\n第 2 行：进程统计 (Tasks)Tasks: 230 total,   1 running, 229 sleeping,   0 stopped,   0 zombie\n\ntotal: 进程总数。\nrunning: 正在运行的进程数（正在占用 CPU）。\nsleeping: 处于睡眠状态的进程数（等待事件，如网络请求、用户输入）。\nstopped: 被停止的进程数（通常是被 Ctrl+Z 挂起或调试中）。\nzombie: 僵尸进程数。这是子进程已结束但父进程未回收其资源的状态。如果该值长期不为 0，可能意味着程序有 Bug。\n\n第 3 行：CPU 使用率 (%Cpu(s))%Cpu(s):  2.3 us,  1.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n\nus (user): 用户空间占用 CPU 百分比（应用程序代码，如 Java, Python, Nginx 等）。如果过高，说明应用层计算密集。\nsy (system): 内核空间占用 CPU 百分比（系统调用）。如果过高，可能是驱动问题或频繁的上下文切换。\nni (nice): 改变过优先级的用户进程占用 CPU 百分比。\nid (idle): 空闲 CPU 百分比。越高越好。\nwa (iowait): 等待 I&#x2F;O（磁盘读写）完成的 CPU 时间百分比。如果很高（如 &gt;30%），说明磁盘是瓶颈，CPU 在空转等待数据。\nhi (hardware interrupts): 处理硬件中断消耗的时间。\nsi (software interrupts): 处理软件中断消耗的时间。\nst (steal time): 虚拟化环境特有。表示虚拟机被宿主机（Hypervisor）强制调度出去，无法使用 CPU 的时间。如果很高，说明宿主机资源紧张或邻居虚拟机抢占资源。\n\n\n注意: 在多核系统中，默认显示的是所有核心的平均值。按键盘 1 可以展开显示每个逻辑 CPU 的详细情况。\n\n第 4 行：物理内存使用 (MiB Mem)MiB Mem :  16000.0 total,   2048.5 free,   8000.0 used,   5951.5 buff/cache\n\ntotal: 总物理内存。\nfree: 完全未被使用的内存。\nused: 进程实际使用的内存。\nbuff&#x2F;cache: 用作缓冲区（buffer）和缓存（cache）的内存。\n\n\n关键点: Linux 会尽可能利用空闲内存做文件缓存以提升性能。当应用程序需要内存时，系统会自动释放这部分内存。因此，判断内存是否不足主要看 available（如果有显示）或 free + buff&#x2F;cache，而不仅仅是看 free。\n\n第 5 行：交换分区使用 (MiB Swap)MiB Swap:   4096.0 total,   4096.0 free,      0.0 used.   7500.0 avail Mem\n\ntotal&#x2F;used&#x2F;free: 交换分区（磁盘虚拟内存）的总量、已用和空闲量。\navail Mem: 可用内存估算值。这是判断内存是否耗尽的最重要指标。它考虑了可回收的缓存和交换分区的潜力。如果此值很低，系统可能会开始频繁使用 Swap，导致性能急剧下降（OOM 风险）。\n\n进程列表区\n\n\nPID\nUSER\nPR\nNI\nVIRT\nRES\nSHR\nS\n%CPU\n%MEMTIME+\nCOMMAND\n\n\n\n3808015\nroot\n20\n0\n10748\n4192\n3316\nR\n0.7\n0.0\n0:15.17\n\n\n\nPID: Process ID。进程的唯一标识符。用于 kill 进程 (kill -9 PID)。\nUSER: 启动该进程的用户名。\nPR:\tPriority。进程的优先级。数值越小，优先级越高。由内核动态调整。\nNI:\tNice。用户可调整的优先级修正值。范围 -20 (最高) 到 19 (最低)。正值表示“谦让”，负值表示“霸道”。\nVIRT:\tVirtual Memory。进程使用的虚拟内存总量（单位通常是 KiB 或 MiB）。包含代码、数据、共享库以及已申请但未使用的内存。这个值通常会很大，不代表实际物理内存占用。\nRES:\tResident Memory。常驻内存。进程实际使用的物理内存大小（不包括 Swap）。这是判断进程真实内存占用的核心指标。\nSHR:\tShared Memory。共享内存大小。指该进程与其他进程共享的内存部分（如共享库）。\nS:\tStatus。进程状态：\nR: Running (运行中)\nS: Sleep (睡眠&#x2F;等待)\nD: Disk Sleep (不可中断睡眠，通常在等 IO)\nZ: Zombie (僵尸)\nT: Traced&#x2F;Stopped\n\n\n%CPU:\t上次更新到现在的 CPU 时间占用百分比。在多核环境下，该值可以超过 100%（例如 400% 表示占满了 4 个核）。\n%MEM:\t进程使用的物理内存占总物理内存的百分比。\nTIME+:\t进程自启动以来占用的 CPU 总时间（精确到百分之一秒）。\nCOMMAND:\t启动进程的命令名称或命令行参数。\n\nps解析\n\n\nUSER\nPID\n%CPU\n%MEM\nVSZ\nRSS\nTYY\nSTAT\nSTART\nTIHE\nCOMMAND\n\n\n\nroot\n1\n0.0\n0.0\n166700\n12200\n?\nSs\n2025\n6:15\n&#x2F;lib&#x2F;systemd&#x2F;systemd –system –deserialize 49 noibrs\n\n\n\nUSER:\t进程所有者。\nPID:\t进程 ID。\n%CPU:\tCPU 占用率（从进程启动到现在的平均值，不是瞬时值，这点与 top 不同）。\n%MEM:\t内存占用率。\nVSZ:\t虚拟内存大小 (Virtual Memory Size)，单位 KB。\nRSS:\t常驻内存大小 (Resident Set Size)，即实际物理内存，单位 KB。\nTTY:\t终端类型。? 表示无终端（后台服务），pts&#x2F;0 表示伪终端。\nSTAT:\t进程状态（R&#x3D;运行, S&#x3D;睡眠, D&#x3D;不可中断, Z&#x3D;僵尸, T&#x3D;停止）。后面可能跟附加标志（如 &lt; 高优先级, N 低优先级, s 会话首进程）。\nSTART:    进程启动时间。\nTIME:\t进程占用的 CPU 总时间。\nCOMMAND:\t启动进程的命令及参数。\n\n对比\n\n\n特性\nps\ntop\n\n\n\n显示模式\n静态快照（执行一次就结束）\n动态实时（持续刷新）\n\n\n主要用途\n获取精确的进程列表、PPID、启动时间；脚本自动化；配合 kill&#x2F;grep\n实时监控系统负载、发现瞬时资源飙高、交互式杀进程\n\n\nCPU 数据\n显示的是平均利用率\n显示的是瞬时利用率\n\n\n交互性\n无（需重新运行命令）\n强（支持按键排序、杀进程等）\n\n\n资源消耗\n极低\n较低（但持续占用）\n\n\nexplain基本用法在 SELECT 语句前加上 EXPLAIN（或 EXPLAIN EXTENDED &#x2F; EXPLAIN ANALYZE）：\nEXPLAIN SELECT * FROM users WHERE email = &#x27;test@example.com&#x27;;\n输出结果通常是一个表格，每一行代表查询执行中的一个步骤（如果是多表连接，会有多行）。\n核心字段详细解析 (MySQL 8.0+)以下是 EXPLAIN 输出中最重要的列及其含义：\n1. id (查询标识符)\n含义: 表示查询中执行 SELECT 子句或操作表的顺序。\n规则:\nid 相同: 执行顺序由上至下。\nid 不同: 如果是子查询，id 号越大，优先级越高，越先执行。\nid 为 NULL: 表示这是最终合并结果集的一步（如 UNION 的结果）。\n\n\n\n2. select_type (查询类型)表示查询的类型，主要用于区分普通查询、联合查询、子查询等。\n\nSIMPLE: 简单查询（不包含子查询或 UNION）。\nPRIMARY: 最外层的查询。\nSUBQUERY: 包含在 SELECT 列表中的子查询。\nDERIVED: 包含在 FROM 子句中的子查询（派生表）。MySQL 会将结果物化到临时表中。\nUNION: UNION 语句中第二个及以后的 SELECT 语句。\nDEPENDENT SUBQUERY: 依赖外部查询结果的子查询（效率通常较低，因为外部每行都要执行一次子查询）。\n\n3. table (表名)显示当前行正在访问的表名。\n4. type (访问类型) ⭐ 最重要指标\n\n\n类型\n含义\n性能评价\n\n\n\nsystem\n表只有一行记录（系统表），是 const 的特例。\n🏆 最快\n\n\nconst\n通过主键或唯一索引一次性找到一行。\n🏆 极快\n\n\neq_ref\n联表查询时，对于前表的每一行，在当前表中通过主键或唯一索引找到一行。\n✅ 很好 (常见于 Join)\n\n\nref\n通过非唯一索引查找，可能找到多行。\n✅ 好\n\n\nfulltext\n全文索引搜索。\n✅ 好\n\n\nref_or_null\n类似 ref，但额外包含对 NULL 值的搜索。\n⚠️ 一般\n\n\nindex_merge\n使用了索引合并优化方法。\n⚠️ 一般\n\n\nrange\n利用索引进行范围扫描（如 &gt;, &lt;, BETWEEN, IN）。\n⚠️ 一般 (优于全表扫描)\n\n\nindex\n全索引扫描。扫描了整棵索引树，比全表扫描快（因为索引通常比数据文件小），但仍需遍历所有索引节点。\n❌ 较差\n\n\nALL\n全表扫描。从头到尾扫描所有数据行。\n💀 最差 (必须优化)\n\n\n\n优化目标: 尽量保证查询达到 range 级别，最好达到 ref 或 const。避免出现 ALL 和 index。\n\n5. possible_keys (可能用到的索引)\n含义: 指出 MySQL 在查询时可能使用哪些索引。\n注意: 如果为 NULL，表示没有相关索引。但这不代表最终一定会用这些索引。\n\n6. key (实际使用的索引) ⭐ 关键指标\n含义: MySQL 实际决定使用的索引。\n分析:\n如果 key 为 NULL，说明没用到索引（除非是 ALL 类型的小表）。\n如果 possible_keys 有多个，但 key 只有一个，说明优化器选择了它认为最高效的那个。\n如果 key 显示的索引不是你预期的，可能需要使用 FORCE INDEX 强制测试，或者检查统计信息是否准确。\n\n\n\n7. key_len (索引长度)\n含义: 表示使用的索引长度（字节数）。\n作用: 用于判断联合索引被使用了多少列。\n例如：联合索引 (a, b, c)，如果 key_len 只覆盖了 a 的长度，说明只用到了第一列（遵循最左前缀原则）。\n越长越好（在联合索引场景下），意味着匹配了更多的列。\n\n8. ref (参考列)\n含义: 显示哪个字段或常数与 key 进行比较。\n常见值:\n常量值：const。\n字段名：db.table.column（常见于 JOIN 操作）。\nfunc：使用了函数。\n\n\n\n9. rows (预估扫描行数) ⭐ 重要指标\n含义: MySQL 估算的需要扫描的行数。\n作用: 这是一个估算值，基于统计信息。数值越小越好。\n注意: 在 InnoDB 中，由于 MVCC 机制，这个值往往不准确，但趋势具有参考意义。如果 type 是 ALL 且 rows 很大，必须优化。\n\n10. Extra (额外信息) ⭐ 包含大量诊断信息\n\n\n信息\n含义\n性能影响\n\n\n\nUsing index\n覆盖索引。查询的数据直接从索引中获取，无需回表查数据行。\n🏆 极好\n\n\nUsing where\n存储引擎返回数据后，Server 层进行了 WHERE 过滤。\n✅ 正常 (若配合索引则好)\n\n\nUsing temporary\nMySQL 需要创建临时表来处理查询（常见于 GROUP BY, ORDER BY 无法利用索引时）。\n❌ 差 (需优化)\n\n\nUsing filesort\nMySQL 无法利用索引完成排序，需要在内存或磁盘中进行文件排序。\n❌ 差 (需优化)\n\n\nUsing join buffer\n使用了连接缓冲区（通常意味着连接条件没有用到索引）。\n❌ 差\n\n\nImpossible WHERE\nWHERE 子句永远为假（如 id=1 AND id=2），查询结果为空。\n-\n\n\nSelect tables optimized away\n使用了聚合函数（如 COUNT(*)）且有索引，直接通过索引统计，无需查表。\n🏆 极好\n\n\npprofhttps://www.cnblogs.com/hobbybear/p/18059425\n","categories":["场景"]},{"title":"设计模式","url":"/2026/02/28/%E5%9C%BA%E6%99%AF-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","content":"函数工厂传入一个返回值的函数（即“函数工厂”或“延迟求值函数”）而不是直接传入值，主要有以下几个关键原因：\n支持动态&#x2F;运行时获取值（Dynamic &#x2F; Runtime Value）有些值不是启动时就确定的，而是在每次请求、每次调用时才需要获取。\n// ❌ 静态传入：启动时固定，无法更新client := NewClient(&quot;my-static-api-key&quot;)// ✅ 动态传入：每次调用时重新获取client := NewClient(func() string &#123;    return getLatestAPIKeyFromVault() // 从密钥管理服务实时拉取&#125;)\n\n比如：你的 API Key 每小时轮换一次（出于安全策略），如果直接传字符串，程序就得重启才能生效；而传函数，框架可以在每次发请求前调用它，自动使用最新密钥。\n\n避免初始化顺序问题（Avoid Initialization Order Issues）Go 中包的初始化顺序复杂，有时你无法在 init() 或全局变量中直接拿到某个值。\nvar apiKey stringfunc init() &#123;    // 假设 config.Load() 依赖网络或文件，可能失败或尚未完成    cfg := config.Load()    apiKey = cfg.APIKey&#125;// 如果框架要求直接传 apiKey，但此时它还是空字符串！svc := NewService(apiKey) // ❌ 可能为空！// 改为传函数：svc := NewService(func() string &#123; return config.Get().APIKey &#125;) // ✅ 安全\n\n\n函数延迟了值的读取，绕过了“先有鸡还是先有蛋”的初始化困境。\n\n\n支持上下文感知（Context-Aware Values）某些值依赖于当前请求的上下文（如用户身份、租户 ID、trace ID 等）。\n// 伪代码：多租户 SaaS 系统middleware := AuthMiddleware(func(ctx context.Context) (string, error) &#123;    tenantID := ctx.Value(&quot;tenant_id&quot;).(string)    return getAPIKeyForTenant(tenantID) // 不同租户不同 key&#125;)\n\n直接传一个字符串无法表达“这个值取决于当前请求是谁”。\n\n便于测试（Testability）在单元测试中，你可以轻松注入 mock 行为：\n// 生产环境svc := NewService(func() string &#123; return os.Getenv(&quot;API_KEY&quot;) &#125;)// 测试环境svc := NewService(func() string &#123; return &quot;test-mock-key&quot; &#125;)\n甚至可以模拟失败场景：\nsvc := NewService(func() string &#123;    if time.Now().After(expireTime) &#123;        return &quot;&quot; // 模拟密钥过期    &#125;    return &quot;valid-key&quot;&#125;)\n\n如果直接传值，你就失去了这种灵活性。\n\n总结：为什么传函数而不是值？\n\n\n原因\n说明\n\n\n\n🔁 动态性\n值可能随时间、上下文变化\n\n\n🧪 可测试性\n易于 mock 和注入行为\n\n\n⏳ 懒加载\n避免提前执行昂贵操作\n\n\n🧩 解耦初始化\n规避包初始化顺序问题\n\n\n🌐 上下文感知\n支持 per-request &#x2F; per-tenant 配置\n\n\n🧱 接口一致性\n框架设计更统一、扩展性强\n\n\n\n💡 核心思想：函数是“行为”的抽象，而不仅仅是“数据”。通过传递函数，你把“如何获取值”的控制权交给了调用方，而不是由框架硬编码。\n\n","categories":["场景"]},{"title":"限流","url":"/2026/02/28/%E5%9C%BA%E6%99%AF-%E9%99%90%E6%B5%81/","content":"漏桶算法漏桶算法（Leaky Bucket Algorithm） 是网络流量整形（Traffic Shaping）和速率限制（Rate Limiting）中最经典、最严格的算法之一。它的核心思想是将不可预测的突发流量转换为平滑的恒定速率流量，从而保护下游系统。\n\n一般用于调用第三方受限 API\n\n核心原理与比喻想象一个底部有一个小孔的桶：\n\n进水（请求流入）：用户请求像水一样以不可预测的速率流入桶中。有时水流很大（突发流量），有时很小。\n桶（缓冲区）：桶有一个固定的容量（Capacity）。如果流入的水超过了桶的容量，水就会溢出，这部分请求会被直接丢弃或拒绝。\n漏水（恒定处理）：桶底的小孔以恒定的速率向外漏水。无论进水多快，出水的速度永远保持不变。\n\n在计算机系统中的映射：\n\n桶 &#x3D; 消息队列或缓冲区。\n进水 &#x3D; 外部请求进入系统。\n漏水 &#x3D; 系统以固定速率处理请求（如每秒处理 10 个请求）。\n溢出 &#x3D; 当队列满时，新来的请求被限流（返回 429 Too Many Requests 或直接丢弃）。\n\n核心特性\n强制恒定速率（Constant Rate）：这是漏桶算法最显著的特征。无论上游流量如何波动，下游接收到的流量永远是平滑且恒定的。它强行将“突发”抹平。\n缓冲能力（Buffering）：桶的大小决定了系统能容忍多大的突发流量。如果桶很大，它可以暂时容纳大量突发请求，然后慢慢处理；如果桶很小，系统对突发的容忍度就很低。\n无“信用”积累：如果一段时间内没有请求（桶空了），漏桶不会积攒“处理能力”。当新请求到来时，它依然只能以固定的速率流出。这意味着它不支持利用空闲时间来处理后续的突发流量。\n\n令牌桶算法核心原理在分布式场景下，令牌桶的逻辑如下：\n\n令牌生成：系统以固定速率（rate，如 10个&#x2F;秒）向桶中添加令牌。\n桶容量：桶有一个最大容量（capacity），令牌数量不能超过此值。\n请求处理：\n当请求到来时，尝试从桶中获取 N 个令牌\n如果当前令牌数 ≥N ，则扣除令牌，请求通过。\n如果当前令牌数 &lt;N ，则请求被拒绝（或等待，但 Redis 实现通常直接返回拒绝）。\n\n\n\n\n关键点：由于是分布式系统，多个节点同时访问 Redis，“计算当前令牌数”和“扣减令牌”必须是原子操作，否则会出现超卖（令牌数为负）或限流失效。\n\nRedis 数据结构设计为了在 Redis 中高效实现，通常只需要一个 Key 来存储状态。这个 Key 的值需要包含两个核心信息：\n\n当前令牌数量 (tokens)：浮点数或整数。\n上次更新令牌的时间戳 (last_refill_time)：用于计算这段时间内产生了多少新令牌。\n\n存储方案：\n\n方案 A (推荐)：使用 Redis String 存储一个 JSON 字符串或自定义格式字符串，例如 “tokens,last_refill_time”。\n方案 B：使用 Redis Hash 结构，字段分别为 tokens 和 last_time。\n方案 C (高性能)：直接使用 String 存储 tokens，利用 Redis 的过期时间或单独 Key 存时间（不推荐，原子性难保证）。\n\n最常用方案 A：Key 为 rate_limit:{resource_id}，Value 为 {“tokens”: 9.5, “time”: 1709100000.123}。\nLua 脚本实现 (核心)为了保证原子性，必须将“读取状态 -&gt; 计算新增令牌 -&gt; 判断是否足够 -&gt; 扣减令牌 -&gt; 保存状态”这一系列操作封装在 Lua 脚本中，通过 EVAL 命令执行。\n脚本逻辑推导\n\ncapacity &#x3D; 桶最大容量\nrate &#x3D; 每秒生成令牌数\nnow &#x3D; 当前时间戳 (秒，带小数)\ntokens &#x3D; 当前令牌数\nlast_time &#x3D; 上次更新时间\n\n计算步骤：\n\n读取 Redis 中存储的 tokens 和 last_time。如果是第一次，初始化为 capacity 和 now。\n计算时间差：delta &#x3D; now - last_time。\n计算新增令牌：new_tokens &#x3D; delta * rate。\n更新当前令牌数：current_tokens &#x3D; min(capacity, tokens + new_tokens) (不能超过容量)。\n判断请求需要的令牌数 requested (通常为 1)：\n如果 current_tokens &gt;&#x3D; requested：\n扣减：current_tokens &#x3D; current_tokens - requested\n更新 Redis：保存新的 current_tokens 和 now。\n返回成功 (1)。\n\n\n如果 current_tokens &lt; requested：\n更新 Redis：保存当前的 current_tokens (虽然没扣减，但时间戳要更新，以便下次计算准确) 和 now。\n返回失败 (0)。\n\n\n\n\n\nbbrBBR (Bottleneck Bandwidth and RTT) 限流算法是一种自适应的流量控制策略。它不依赖人工配置的固定阈值（如 QPS），而是通过实时监控系统负载（CPU、响应时间、并发数），动态调整允许的请求量，使系统始终运行在“最大吞吐量”与“最低延迟”的平衡点上。\n一、核心模块详解CPU 监控模块：系统的“健康体温计”CPU 使用率是判断系统是否过载的最直接指标。Kratos 的 BBR 实现并不直接使用瞬时的 CPU 快照，而是采用 EWMA (Exponential Weighted Moving Average，指数加权移动平均) 算法来平滑处理 CPU 数据。\n\n工作原理：\n采样：每隔极短的时间（如 100ms）采集一次当前进程的 CPU 使用率。\n平滑计算：利用公式 $CPU_{avg} &#x3D; \\alpha \\times CPU_{current} + (1 - \\alpha) \\times CPU_{prev}$ 进行计算。其中 $\\alpha$ 是衰减因子（通常较小，如 0.1~0.3）。\n作用：消除因 GC（垃圾回收）或瞬时毛刺导致的 CPU 尖峰抖动，反映系统真实的负载趋势。\n\n\n决策逻辑：\n当 $CPU_{avg}$ 低于设定阈值（默认通常为 80% 或根据配置），认为系统资源充裕，允许增加并发请求。\n当 $CPU_{avg}$ 高于阈值，认为系统已过载，触发限流逻辑，强制减少通过的请求数。\n\n\n\n滑动窗口计数器：高精度的“历史记录仪”为了准确统计最近一段时间内的请求情况（RT、成功数、拒绝数），BBR 使用了滑动窗口技术，避免了固定窗口的“临界突刺”问题。\n\n结构设计：\n将一个大时间窗口（如 1 秒）划分为多个小的时间桶（Bucket，如 10 个桶，每个 100ms）。\n每个 Bucket 独立记录该时间段内的：请求总数、成功数、耗时总和（用于计算平均 RT）、拒绝数。\n\n\n滑动机制：\n随着时间推移，指针移动到新位置，最旧的 Bucket 数据被自动丢弃&#x2F;覆盖，新数据写入当前 Bucket。\n统计计算：当前的指标（如平均 RT、总 QPS） &#x3D; 窗口内所有有效 Bucket 数据的聚合值。\n\n\n优势：\n平滑性：即使某个瞬间流量激增，由于是多个桶的加权平均，指标变化是渐进的，避免限流策略剧烈震荡。\n实时性：能够精确感知到最近几百毫秒内的负载变化，比固定窗口（如“每秒限制”）反应更快。\n\n\n\n并发控制：动态调整的“水龙头”这是 BBR 算法的核心决策层，它模仿 TCP BBR 的拥塞控制思想，通过计算 Max In-flight (最大允许并发请求数) 来控制流量。\n核心公式（简化版）：\n$\\text{Limit} &#x3D; \\text{EstimatedBandwidth} \\times \\text{MinRTT}$\n$\\text{NextWindow} &#x3D; \\text{CurrentFlight} \\times \\frac{\\text{TargetCPU}}{\\text{CurrentCPU}} \\times \\frac{\\text{BaseRTT}}{\\text{CurrentRTT}} $\n工作流程：\n\n探测阶段：系统尝试逐步增加并发数，观察 RT 和 CPU 的变化。\n正常状态：如果 CPU 和 RT 稳定，缓慢增大 maxFlight（允许的最大并发数），以试探系统上限。\n拥塞状态：\n若 $CPU &gt; \\text{Threshold}$ 或 $RT$ 显著增加（超过基线 RT 的倍数）。\n算法判定发生拥塞，立即按系数缩小 maxFlight。\n新到达的请求如果超过 maxFlight，直接被拒绝（返回 429 Too Many Requests）。\n\n\n恢复阶段：当指标回落后，再次尝试缓慢增加并发数。\n\n缓存机制：高性能的“数据暂存区”在高并发场景下，频繁的内存分配和锁竞争会消耗大量 CPU。BBR 实现中大量使用了缓存和池化技术来保证自身低开销。\n\nRing Buffer (环形缓冲区)：滑动窗口的底层存储通常是一个固定大小的环形数组。这种结构避免了动态扩容带来的内存分配开销，且利用 CPU 缓存行（Cache Line）局部性原理，提高读取速度。\nSync.Pool：Go 语言中的对象池。用于复用统计数据结构（如 Bucket 对象、节点对象），减少 GC 压力。\n无锁&#x2F;少锁设计：在更新统计数据时，尽量使用原子操作（atomic）或将写操作限制在单个时间片内，减少全局锁的竞争。\n最大通过量和最少时间会缓存，避免大量的重复计算\n\n二、BBR 限流 vs 传统限流算法\n\n\n特性\nBBR 自适应限流\n令牌桶 &#x2F; 漏桶 (固定阈值)\n固定窗口计数器\n\n\n\n配置方式\n零配置&#x2F;自适应。无需预设 QPS，系统自动寻找最佳水位。\n人工配置。需预先设定 QPS 或速率，难以适应环境变化。\n人工配置。需设定时间窗口大小和计数阈值。\n\n\n应对突发流量\n智能削峰。根据实时负载动态调整，既不让系统崩，也不浪费资源。\n僵硬。要么直接拒绝（令牌不足），要么积压（漏桶），无法感知系统真实承载力。\n临界突刺。在窗口边界处可能允许 2 倍于阈值的流量瞬间进入。\n\n\n资源利用率\n高。始终尝试将系统推至 CPU&#x2F;RT 的极限边缘运行，最大化吞吐。\n低。为保安全通常设低阈值，导致大部分时间资源闲置。\n中。取决于阈值设定的保守程度。\n\n\n环境适应性\n强。适用于异构集群（不同配置的 Pod 自动适配不同流量）。\n弱。所有实例需统一配置，小规格实例易挂，大规格实例浪费。\n弱。同左。\n\n\n实现复杂度\n高。涉及 EWMA 计算、滑动窗口、动态反馈调节，逻辑复杂。\n低。数学模型简单，易于实现和理解。\n低。逻辑最简单。\n\n\n稳定性风险\n初期波动。在刚启动或负载剧烈变化时，可能需要几个周期来“学习”最佳值。\n稳定。行为完全可预测。\n稳定。但容易在边界时刻失效。\n\n\n典型应用场景\n云原生微服务、容器化部署、负载波动大的互联网核心链路。\n对外 API 网关（需严格承诺 SLA）、内部非核心服务。\n简单的防刷接口、低频访问控制。\n\n\n三、总结：为什么 Kratos 选择 BBR？Kratos 作为面向云原生的微服务框架，其核心设计理念是弹性和自愈。\n\n解决“配不准”的痛点：在 K8s 环境中，Pod 可能随时漂移、扩缩容，且不同节点的物理性能存在差异（噪声邻居）。固定阈值（如 1000 QPS）在 A 机器上可能刚好，在 B 机器上就直接打挂了。BBR 让每个实例**“各自为战”**，根据自己的 CPU 和 RT 决定能抗多少流量。\n防止雪崩效应：传统限流往往是在系统已经过载（线程池满、DB 连接耗尽）后才生效，或者为了防止过载而设置得过于保守。BBR 能在过载发生的萌芽阶段（RT 刚开始变长、CPU 刚开始升高）就提前介入，降低并发，从而保护下游依赖，避免级联故障。\n最大化资源价值：它允许系统在安全范围内尽可能多地处理请求，而不是为了安全而过度牺牲吞吐量。\n\n一句话总结：BBR 限流将流量控制的主动权从“运维人员的配置文件”交还给了“系统实时的运行状态”，是构建高可用、自适应云原生架构的关键组件。\n","categories":["场景"]}]